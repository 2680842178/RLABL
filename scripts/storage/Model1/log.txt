train.py --algo ppo --env MiniGrid-ConfigWorld-v0 --model Model1 --save-interval 10 --frames 100000

Namespace(algo='ppo', env='MiniGrid-ConfigWorld-v0', model='Model1', seed=1, log_interval=1, save_interval=10, procs=16, frames=100000, epochs=4, batch_size=256, frames_per_proc=None, discount=0.99, lr=0.001, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, mem=False)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): Tanh()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): Tanh()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): Tanh()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): Tanh()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

train.py --algo ppo --env MiniGrid-ConfigWorld-v0 --model Model1 --save-interval 10 --frames 100000

Namespace(algo='ppo', env='MiniGrid-ConfigWorld-v0', model='Model1', seed=1, log_interval=1, save_interval=10, procs=16, frames=100000, epochs=4, batch_size=256, frames_per_proc=None, discount=0.99, lr=0.001, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, mem=False)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): Tanh()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): Tanh()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): Tanh()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): Tanh()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

U 1 | F 002048 | FPS 0220 | D 9 | Reward:米考mM -1.00 0.00 -1.00 -1.00 |  entropy 1.650 | value -0.122 | policy_loss 0.213 | value_loss 0.090 | grad_norm 0.801
U 2 | F 004096 | FPS 0754 | D 12 | Reward:米考mM -0.69 0.46 -1.00 0.00 |  entropy 1.682 | value -0.198 | policy_loss -0.010 | value_loss 0.015 | grad_norm 0.081
U 3 | F 006144 | FPS 0776 | D 14 | Reward:米考mM -0.25 0.43 -1.00 0.00 |  entropy 1.753 | value -0.162 | policy_loss -0.033 | value_loss 0.004 | grad_norm 0.060
train.py --algo ppo --env MiniGrid-ConfigWorld-v0 --model Model1 --save-interval 10 --frames 100000

Namespace(algo='ppo', env='MiniGrid-ConfigWorld-v0', model='Model1', seed=1, log_interval=1, save_interval=10, procs=16, frames=100000, epochs=4, batch_size=256, frames_per_proc=None, discount=0.99, lr=0.001, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, mem=False)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): Tanh()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): Tanh()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): Tanh()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): Tanh()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

U 1 | F 002048 | FPS 0203 | D 10 | Reward:米考mM -1.00 0.00 -1.00 -1.00 |  entropy 1.650 | value -0.122 | policy_loss 0.213 | value_loss 0.090 | grad_norm 0.801
U 2 | F 004096 | FPS 0774 | D 12 | Reward:米考mM -0.69 0.46 -1.00 0.00 |  entropy 1.682 | value -0.198 | policy_loss -0.010 | value_loss 0.015 | grad_norm 0.081
U 3 | F 006144 | FPS 0783 | D 15 | Reward:米考mM -0.25 0.43 -1.00 0.00 |  entropy 1.753 | value -0.162 | policy_loss -0.033 | value_loss 0.004 | grad_norm 0.060
U 4 | F 008192 | FPS 0788 | D 17 | Reward:米考mM -0.29 0.46 -1.00 0.00 |  entropy 1.782 | value -0.150 | policy_loss -0.016 | value_loss 0.013 | grad_norm 0.094
U 5 | F 010240 | FPS 0783 | D 20 | Reward:米考mM -0.38 0.48 -1.00 0.00 |  entropy 1.756 | value -0.157 | policy_loss 0.009 | value_loss 0.018 | grad_norm 0.087
U 6 | F 012288 | FPS 0771 | D 23 | Reward:米考mM -0.50 0.50 -1.00 0.00 |  entropy 1.790 | value -0.152 | policy_loss 0.013 | value_loss 0.020 | grad_norm 0.136
U 7 | F 014336 | FPS 0789 | D 25 | Reward:米考mM -0.19 0.39 -1.00 0.00 |  entropy 1.798 | value -0.151 | policy_loss -0.032 | value_loss 0.004 | grad_norm 0.053
U 8 | F 016384 | FPS 0789 | D 28 | Reward:米考mM -0.12 0.33 -1.00 0.00 |  entropy 1.794 | value -0.125 | policy_loss -0.022 | value_loss 0.004 | grad_norm 0.047
U 9 | F 018432 | FPS 0791 | D 31 | Reward:米考mM -0.06 0.24 -1.00 0.00 |  entropy 1.793 | value -0.107 | policy_loss -0.030 | value_loss 0.001 | grad_norm 0.046
U 10 | F 020480 | FPS 0787 | D 33 | Reward:米考mM -0.06 0.24 -1.00 0.00 |  entropy 1.768 | value -0.095 | policy_loss -0.020 | value_loss 0.004 | grad_norm 0.087
Status saved
U 11 | F 022528 | FPS 0770 | D 36 | Reward:米考mM -0.25 0.43 -1.00 0.00 |  entropy 1.760 | value -0.093 | policy_loss 0.004 | value_loss 0.011 | grad_norm 0.152
train.py --algo ppo --env MiniGrid-ConfigWorld-v0 --model Model1 --save-interval 10 --frames 100000

Namespace(algo='ppo', env='MiniGrid-ConfigWorld-v0', model='Model1', seed=1, log_interval=1, save_interval=10, procs=16, frames=100000, epochs=4, batch_size=256, frames_per_proc=None, discount=0.99, lr=0.001, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, mem=False)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): Tanh()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): Tanh()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): Tanh()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): Tanh()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

U 11 | F 022528 | FPS 0268 | D 7 | Reward:米考mM -0.19 0.39 -1.00 0.00 |  entropy 1.787 | value -0.094 | policy_loss 0.005 | value_loss 0.009 | grad_norm 0.089
U 12 | F 024576 | FPS 0778 | D 10 | Reward:米考mM -0.12 0.33 -1.00 0.00 |  entropy 1.790 | value -0.060 | policy_loss -0.019 | value_loss 0.000 | grad_norm 0.032
U 13 | F 026624 | FPS 0725 | D 13 | Reward:米考mM -0.06 0.24 -1.00 0.00 |  entropy 1.767 | value -0.053 | policy_loss -0.007 | value_loss 0.004 | grad_norm 0.055
U 14 | F 028672 | FPS 0761 | D 15 | Reward:米考mM -0.06 0.24 -1.00 0.00 |  entropy 1.792 | value -0.035 | policy_loss -0.007 | value_loss 0.000 | grad_norm 0.018
U 15 | F 030720 | FPS 0726 | D 18 | Reward:米考mM -0.06 0.24 -1.00 0.00 |  entropy 1.787 | value -0.026 | policy_loss -0.006 | value_loss 0.002 | grad_norm 0.050
U 16 | F 032768 | FPS 0745 | D 21 | Reward:米考mM 0.00 0.00 0.00 0.00 |  entropy 1.791 | value -0.027 | policy_loss -0.009 | value_loss 0.000 | grad_norm 0.016
U 17 | F 034816 | FPS 0773 | D 24 | Reward:米考mM 0.00 0.00 0.00 0.00 |  entropy 1.795 | value -0.021 | policy_loss -0.004 | value_loss 0.000 | grad_norm 0.004
U 18 | F 036864 | FPS 0738 | D 26 | Reward:米考mM -0.06 0.24 -1.00 0.00 |  entropy 1.776 | value -0.019 | policy_loss -0.002 | value_loss 0.003 | grad_norm 0.044
U 19 | F 038912 | FPS 0776 | D 29 | Reward:米考mM 0.00 0.00 0.00 0.00 |  entropy 1.792 | value -0.018 | policy_loss -0.006 | value_loss 0.000 | grad_norm 0.013
U 20 | F 040960 | FPS 0784 | D 32 | Reward:米考mM 0.00 0.00 0.00 0.00 |  entropy 1.794 | value -0.014 | policy_loss -0.003 | value_loss 0.000 | grad_norm 0.006
Status saved
U 21 | F 043008 | FPS 0753 | D 34 | Reward:米考mM 0.00 0.00 0.00 0.00 |  entropy 1.794 | value -0.012 | policy_loss -0.003 | value_loss 0.000 | grad_norm 0.007
U 22 | F 045056 | FPS 0764 | D 37 | Reward:米考mM 0.00 0.00 0.00 0.00 |  entropy 1.794 | value -0.009 | policy_loss -0.003 | value_loss 0.000 | grad_norm 0.006
U 23 | F 047104 | FPS 0785 | D 40 | Reward:米考mM 0.00 0.00 0.00 0.00 |  entropy 1.795 | value -0.008 | policy_loss -0.002 | value_loss 0.000 | grad_norm 0.002
U 24 | F 049152 | FPS 0761 | D 42 | Reward:米考mM 0.00 0.00 0.00 0.00 |  entropy 1.795 | value -0.007 | policy_loss -0.002 | value_loss 0.000 | grad_norm 0.003
train.py --algo ppo --env MiniGrid-ConfigWorld-v0 --model Model1 --save-interval 10 --frames 100000

Namespace(algo='ppo', env='MiniGrid-ConfigWorld-v0', model='Model1', seed=1, log_interval=1, save_interval=10, procs=16, frames=100000, epochs=4, batch_size=256, frames_per_proc=None, discount=0.99, lr=0.001, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, mem=False)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): Tanh()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): Tanh()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): Tanh()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): Tanh()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

U 21 | F 043008 | FPS 0295 | D 6 | Reward:米考mM 0.00 0.00 0.00 0.00 |  entropy 1.794 | value -0.013 | policy_loss -0.002 | value_loss 0.000 | grad_norm 0.003
U 22 | F 045056 | FPS 0790 | D 9 | Reward:米考mM 0.00 0.00 0.00 0.00 |  entropy 1.794 | value -0.010 | policy_loss -0.003 | value_loss 0.000 | grad_norm 0.006
U 23 | F 047104 | FPS 0767 | D 12 | Reward:米考mM 0.00 0.00 0.00 0.00 |  entropy 1.795 | value -0.008 | policy_loss -0.003 | value_loss 0.000 | grad_norm 0.005
U 24 | F 049152 | FPS 0759 | D 14 | Reward:米考mM 0.00 0.00 0.00 0.00 |  entropy 1.795 | value -0.007 | policy_loss -0.001 | value_loss 0.000 | grad_norm 0.002
U 25 | F 051200 | FPS 0719 | D 17 | Reward:米考mM 0.00 0.00 0.00 0.00 |  entropy 1.795 | value -0.005 | policy_loss -0.002 | value_loss 0.000 | grad_norm 0.004
U 26 | F 053248 | FPS 0695 | D 20 | Reward:米考mM -0.18 0.38 -1.00 0.00 |  entropy 1.760 | value -0.026 | policy_loss 0.020 | value_loss 0.012 | grad_norm 0.072
U 27 | F 055296 | FPS 0759 | D 23 | Reward:米考mM -0.06 0.24 -1.00 0.00 |  entropy 1.793 | value -0.020 | policy_loss -0.004 | value_loss 0.000 | grad_norm 0.013
U 28 | F 057344 | FPS 0746 | D 26 | Reward:米考mM 0.00 0.00 0.00 0.00 |  entropy 1.793 | value -0.014 | policy_loss -0.005 | value_loss 0.000 | grad_norm 0.006
U 29 | F 059392 | FPS 0760 | D 28 | Reward:米考mM 0.00 0.00 0.00 0.00 |  entropy 1.793 | value -0.010 | policy_loss -0.004 | value_loss 0.000 | grad_norm 0.005
U 30 | F 061440 | FPS 0760 | D 31 | Reward:米考mM 0.00 0.00 0.00 0.00 |  entropy 1.794 | value -0.008 | policy_loss -0.002 | value_loss 0.000 | grad_norm 0.003
Status saved
U 31 | F 063488 | FPS 0772 | D 34 | Reward:米考mM 0.00 0.00 0.00 0.00 |  entropy 1.794 | value -0.006 | policy_loss -0.002 | value_loss 0.000 | grad_norm 0.004
U 32 | F 065536 | FPS 0762 | D 36 | Reward:米考mM 0.00 0.00 0.00 0.00 |  entropy 1.794 | value -0.004 | policy_loss -0.002 | value_loss 0.000 | grad_norm 0.003
U 33 | F 067584 | FPS 0780 | D 39 | Reward:米考mM 0.00 0.00 0.00 0.00 |  entropy 1.794 | value -0.003 | policy_loss -0.001 | value_loss 0.000 | grad_norm 0.001
U 34 | F 069632 | FPS 0771 | D 42 | Reward:米考mM 0.00 0.00 0.00 0.00 |  entropy 1.794 | value -0.002 | policy_loss -0.001 | value_loss 0.000 | grad_norm 0.001
U 35 | F 071680 | FPS 0696 | D 45 | Reward:米考mM -0.12 0.33 -1.00 0.00 |  entropy 1.754 | value -0.012 | policy_loss 0.010 | value_loss 0.007 | grad_norm 0.076
U 36 | F 073728 | FPS 0775 | D 47 | Reward:米考mM 0.00 0.00 0.00 0.00 |  entropy 1.792 | value -0.022 | policy_loss -0.004 | value_loss 0.000 | grad_norm 0.023
U 37 | F 075776 | FPS 0793 | D 50 | Reward:米考mM 0.00 0.00 0.00 0.00 |  entropy 1.792 | value -0.015 | policy_loss -0.005 | value_loss 0.000 | grad_norm 0.012
U 38 | F 077824 | FPS 0774 | D 53 | Reward:米考mM 0.00 0.00 0.00 0.00 |  entropy 1.793 | value -0.012 | policy_loss -0.004 | value_loss 0.000 | grad_norm 0.008
U 39 | F 079872 | FPS 0727 | D 55 | Reward:米考mM -0.06 0.24 -1.00 0.00 |  entropy 1.777 | value -0.016 | policy_loss 0.006 | value_loss 0.004 | grad_norm 0.072
U 40 | F 081920 | FPS 0748 | D 58 | Reward:米考mM -0.12 0.33 -1.00 0.00 |  entropy 1.790 | value -0.013 | policy_loss -0.000 | value_loss 0.002 | grad_norm 0.024
Status saved
U 41 | F 083968 | FPS 0802 | D 61 | Reward:米考mM 0.00 0.00 0.00 0.00 |  entropy 1.792 | value -0.007 | policy_loss -0.002 | value_loss 0.000 | grad_norm 0.005
U 42 | F 086016 | FPS 0752 | D 63 | Reward:米考mM 0.00 0.00 0.00 0.00 |  entropy 1.792 | value -0.006 | policy_loss -0.001 | value_loss 0.000 | grad_norm 0.001
U 43 | F 088064 | FPS 0785 | D 66 | Reward:米考mM 0.00 0.00 0.00 0.00 |  entropy 1.792 | value -0.005 | policy_loss -0.001 | value_loss 0.000 | grad_norm 0.004
U 44 | F 090112 | FPS 0780 | D 69 | Reward:米考mM 0.00 0.00 0.00 0.00 |  entropy 1.792 | value -0.004 | policy_loss -0.001 | value_loss 0.000 | grad_norm 0.002
U 45 | F 092160 | FPS 0777 | D 71 | Reward:米考mM 0.00 0.00 0.00 0.00 |  entropy 1.792 | value -0.004 | policy_loss -0.001 | value_loss 0.000 | grad_norm 0.001
U 46 | F 094208 | FPS 0804 | D 74 | Reward:米考mM 0.00 0.00 0.00 0.00 |  entropy 1.792 | value -0.003 | policy_loss -0.001 | value_loss 0.000 | grad_norm 0.001
U 47 | F 096256 | FPS 0792 | D 76 | Reward:米考mM 0.00 0.00 0.00 0.00 |  entropy 1.792 | value -0.002 | policy_loss -0.001 | value_loss 0.000 | grad_norm 0.001
U 48 | F 098304 | FPS 0761 | D 79 | Reward:米考mM 0.00 0.00 0.00 0.00 |  entropy 1.792 | value -0.002 | policy_loss -0.000 | value_loss 0.000 | grad_norm 0.001
U 49 | F 100352 | FPS 0786 | D 82 | Reward:米考mM 0.00 0.00 0.00 0.00 |  entropy 1.792 | value -0.002 | policy_loss -0.000 | value_loss 0.000 | grad_norm 0.001
train.py --algo ppo --env MiniGrid-ConfigWorld-v0 --model Model1 --save-interval 10 --frames 100000

Namespace(algo='ppo', env='MiniGrid-ConfigWorld-v0', model='Model1', seed=1, log_interval=1, save_interval=10, procs=16, frames=100000, epochs=4, batch_size=256, frames_per_proc=None, discount=0.99, lr=0.001, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, mem=False)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): Tanh()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): Tanh()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): Tanh()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): Tanh()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

U 41 | F 083968 | FPS 0304 | D 6 | Reward:米考mM 0.00 0.00 0.00 0.00 |  entropy 1.792 | value -0.008 | policy_loss -0.001 | value_loss 0.000 | grad_norm 0.004
train.py --algo ppo --env MiniGrid-ConfigWorld-v0 --model Model1 --save-interval 10 --frames 100000

Namespace(algo='ppo', env='MiniGrid-ConfigWorld-v0', model='Model1', seed=1, log_interval=1, save_interval=10, procs=16, frames=100000, epochs=4, batch_size=256, frames_per_proc=None, discount=0.99, lr=0.001, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, mem=False)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): Tanh()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): Tanh()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): Tanh()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): Tanh()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

train.py --algo ppo --env MiniGrid-ConfigWorld-v0 --model Model1 --save-interval 10 --frames 100000

Namespace(algo='ppo', env='MiniGrid-ConfigWorld-v0', model='Model1', seed=1, log_interval=1, save_interval=10, procs=16, frames=100000, epochs=4, batch_size=256, frames_per_proc=None, discount=0.99, lr=0.001, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, mem=False)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): Tanh()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): Tanh()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): Tanh()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): Tanh()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

U 41 | F 083968 | FPS 0294 | D 6 | Reward:米考mM 0.00 0.00 0.00 0.00 |  entropy 1.792 | value -0.008 | policy_loss -0.001 | value_loss 0.000 | grad_norm 0.004
U 42 | F 086016 | FPS 0791 | D 9 | Reward:米考mM 0.00 0.00 0.00 0.00 |  entropy 1.792 | value -0.007 | policy_loss -0.001 | value_loss 0.000 | grad_norm 0.002
U 43 | F 088064 | FPS 0746 | D 12 | Reward:米考mM 0.00 0.00 0.00 0.00 |  entropy 1.792 | value -0.005 | policy_loss -0.002 | value_loss 0.000 | grad_norm 0.003
U 44 | F 090112 | FPS 0766 | D 14 | Reward:米考mM 0.00 0.00 0.00 0.00 |  entropy 1.792 | value -0.005 | policy_loss -0.001 | value_loss 0.000 | grad_norm 0.001
U 45 | F 092160 | FPS 0761 | D 17 | Reward:米考mM 0.00 0.00 0.00 0.00 |  entropy 1.792 | value -0.004 | policy_loss -0.001 | value_loss 0.000 | grad_norm 0.001
U 46 | F 094208 | FPS 0731 | D 20 | Reward:米考mM -0.06 0.24 -1.00 0.00 |  entropy 1.784 | value -0.008 | policy_loss 0.005 | value_loss 0.004 | grad_norm 0.084
U 47 | F 096256 | FPS 0779 | D 23 | Reward:米考mM -0.06 0.24 -1.00 0.00 |  entropy 1.792 | value 0.001 | policy_loss 0.000 | value_loss 0.000 | grad_norm 0.002
U 48 | F 098304 | FPS 0788 | D 25 | Reward:米考mM -0.06 0.24 -1.00 0.00 |  entropy 1.792 | value 0.001 | policy_loss 0.000 | value_loss 0.000 | grad_norm 0.000
U 49 | F 100352 | FPS 0781 | D 28 | Reward:米考mM 0.00 0.00 0.00 0.00 |  entropy 1.792 | value 0.001 | policy_loss 0.000 | value_loss 0.000 | grad_norm 0.000
