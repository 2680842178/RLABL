discover.py --task-config task1 --discover 0 --algo ppo --env MiniGrid-ConfigWorld-v0-havekey --lr 0.0001 --AnomalyNN test_24 --model 20240725-seed1 --discount 0.99

Namespace(task_config='task1', discover=0, algo='ppo', env='MiniGrid-ConfigWorld-v0-havekey', model='20240725-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_24', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.0001, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cpu

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

discover.py --task-config task1 --discover 0 --algo ppo --env MiniGrid-ConfigWorld-v0-havekey --lr 0.0001 --AnomalyNN test_24 --model 20240725-seed1 --discount 0.99

Namespace(task_config='task1', discover=0, algo='ppo', env='MiniGrid-ConfigWorld-v0-havekey', model='20240725-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_24', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.0001, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

U 1 | F 000256 | FPS 0077 | D 3 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.126'] | value_loss ['None', 'None', '0.052']
U 2 | F 000512 | FPS 0086 | D 6 | Reward:μσmM -0.67 0.47 -1.00 0.00 | policy_loss ['None', 'None', '0.180'] | value_loss ['None', 'None', '0.094']
U 3 | F 000768 | FPS 0124 | D 8 | Reward:μσmM -0.75 0.43 -1.00 0.00 | policy_loss ['None', 'None', '0.136'] | value_loss ['None', 'None', '0.068']
U 4 | F 001024 | FPS 0125 | D 10 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.062'] | value_loss ['None', 'None', '0.031']
U 5 | F 001280 | FPS 0125 | D 12 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.002'] | value_loss ['None', 'None', '0.000']
U 6 | F 001536 | FPS 0125 | D 14 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.002'] | value_loss ['None', 'None', '0.000']
U 7 | F 001792 | FPS 0090 | D 17 | Reward:μσmM -0.86 0.35 -1.00 0.00 | policy_loss ['None', 'None', '0.247'] | value_loss ['None', 'None', '0.100']
U 8 | F 002048 | FPS 0088 | D 20 | Reward:μσmM -0.83 0.37 -1.00 0.00 | policy_loss ['None', 'None', '0.139'] | value_loss ['None', 'None', '0.051']
U 9 | F 002304 | FPS 0125 | D 22 | Reward:μσmM -0.88 0.33 -1.00 0.00 | policy_loss ['None', 'None', '0.167'] | value_loss ['None', 'None', '0.037']
U 10 | F 002560 | FPS 0090 | D 25 | Reward:μσmM -0.67 0.47 -1.00 0.00 | policy_loss ['None', 'None', '-0.031'] | value_loss ['None', 'None', '0.011']
U 10 | Test reward:μσmM -1.00 0.00 -1.00 -1.00 | Test num frames:μσmM 51.50 56.32 3.00 203.00
Status saved
U 11 | F 002816 | FPS 0126 | D 28 | Reward:μσmM -0.67 0.47 -1.00 0.00 | policy_loss ['None', 'None', '-0.022'] | value_loss ['None', 'None', '0.009']
U 12 | F 003072 | FPS 0126 | D 30 | Reward:μσmM -0.67 0.47 -1.00 0.00 | policy_loss ['None', 'None', '-0.041'] | value_loss ['None', 'None', '0.011']
U 13 | F 003328 | FPS 0090 | D 33 | Reward:μσmM -0.75 0.43 -1.00 0.00 | policy_loss ['None', 'None', '-0.009'] | value_loss ['None', 'None', '0.013']
U 14 | F 003584 | FPS 0125 | D 35 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.076'] | value_loss ['None', 'None', '0.001']
U 15 | F 003840 | FPS 0089 | D 38 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.067'] | value_loss ['None', 'None', '0.000']
U 16 | F 004096 | FPS 0088 | D 41 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.054'] | value_loss ['None', 'None', '0.000']
U 17 | F 004352 | FPS 0084 | D 44 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.044'] | value_loss ['None', 'None', '0.000']
U 18 | F 004608 | FPS 0065 | D 48 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.038'] | value_loss ['None', 'None', '0.000']
U 19 | F 004864 | FPS 0077 | D 51 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.032'] | value_loss ['None', 'None', '0.000']
U 20 | F 005120 | FPS 0100 | D 54 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.029'] | value_loss ['None', 'None', '0.000']
U 10 | Test reward:μσmM -0.40 0.49 -1.00 0.00 | Test num frames:μσmM 209.90 60.91 92.00 256.00
Status saved
U 21 | F 005376 | FPS 0121 | D 61 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.027'] | value_loss ['None', 'None', '0.022']
U 22 | F 005632 | FPS 0119 | D 63 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.015'] | value_loss ['None', 'None', '0.013']
U 23 | F 005888 | FPS 0122 | D 66 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.012'] | value_loss ['None', 'None', '0.017']
U 24 | F 006144 | FPS 0088 | D 68 | Reward:μσmM -0.67 0.47 -1.00 0.00 | policy_loss ['None', 'None', '0.028'] | value_loss ['None', 'None', '0.019']
U 25 | F 006400 | FPS 0098 | D 71 | Reward:μσmM -0.83 0.37 -1.00 0.00 | policy_loss ['None', 'None', '0.142'] | value_loss ['None', 'None', '0.045']
U 26 | F 006656 | FPS 0121 | D 73 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '-0.037'] | value_loss ['None', 'None', '0.011']
U 27 | F 006912 | FPS 0125 | D 75 | Reward:μσmM -0.67 0.47 -1.00 0.00 | policy_loss ['None', 'None', '-0.002'] | value_loss ['None', 'None', '0.016']
U 28 | F 007168 | FPS 0123 | D 77 | Reward:μσmM -0.67 0.47 -1.00 0.00 | policy_loss ['None', 'None', '0.012'] | value_loss ['None', 'None', '0.012']
U 29 | F 007424 | FPS 0122 | D 79 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '-0.033'] | value_loss ['None', 'None', '0.011']
U 30 | F 007680 | FPS 0123 | D 82 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.064'] | value_loss ['None', 'None', '0.000']
U 10 | Test reward:μσmM -0.40 0.49 -1.00 0.00 | Test num frames:μσmM 196.00 85.00 25.00 256.00
Status saved
U 31 | F 007936 | FPS 0123 | D 88 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.052'] | value_loss ['None', 'None', '0.000']
U 32 | F 008192 | FPS 0123 | D 90 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.046'] | value_loss ['None', 'None', '0.000']
U 33 | F 008448 | FPS 0117 | D 93 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.012'] | value_loss ['None', 'None', '0.014']
U 34 | F 008704 | FPS 0109 | D 95 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '-0.006'] | value_loss ['None', 'None', '0.018']
U 35 | F 008960 | FPS 0117 | D 97 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.039'] | value_loss ['None', 'None', '0.000']
U 36 | F 009216 | FPS 0116 | D 99 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.029'] | value_loss ['None', 'None', '0.000']
U 37 | F 009472 | FPS 0120 | D 102 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.026'] | value_loss ['None', 'None', '0.000']
U 38 | F 009728 | FPS 0122 | D 104 | Reward:μσmM -0.75 0.43 -1.00 0.00 | policy_loss ['None', 'None', '0.105'] | value_loss ['None', 'None', '0.046']
U 39 | F 009984 | FPS 0123 | D 106 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.008'] | value_loss ['None', 'None', '0.006']
U 40 | F 010240 | FPS 0089 | D 109 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.014'] | value_loss ['None', 'None', '0.019']
U 10 | Test reward:μσmM -0.40 0.49 -1.00 0.00 | Test num frames:μσmM 214.00 65.02 78.00 256.00
Status saved
U 41 | F 010496 | FPS 0122 | D 116 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.028'] | value_loss ['None', 'None', '0.001']
U 42 | F 010752 | FPS 0122 | D 118 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.037'] | value_loss ['None', 'None', '0.000']
U 43 | F 011008 | FPS 0122 | D 120 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.022'] | value_loss ['None', 'None', '0.021']
U 44 | F 011264 | FPS 0123 | D 122 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.045'] | value_loss ['None', 'None', '0.000']
U 45 | F 011520 | FPS 0122 | D 124 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.035'] | value_loss ['None', 'None', '0.000']
U 46 | F 011776 | FPS 0123 | D 126 | Reward:μσmM -0.75 0.43 -1.00 0.00 | policy_loss ['None', 'None', '0.098'] | value_loss ['None', 'None', '0.036']
U 47 | F 012032 | FPS 0122 | D 128 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.045'] | value_loss ['None', 'None', '0.000']
U 48 | F 012288 | FPS 0122 | D 130 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.037'] | value_loss ['None', 'None', '0.000']
U 49 | F 012544 | FPS 0123 | D 133 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.029'] | value_loss ['None', 'None', '0.000']
U 50 | F 012800 | FPS 0123 | D 135 | Reward:μσmM -0.67 0.47 -1.00 0.00 | policy_loss ['None', 'None', '0.073'] | value_loss ['None', 'None', '0.029']
U 10 | Test reward:μσmM -0.50 0.50 -1.00 0.00 | Test num frames:μσmM 176.50 88.77 32.00 256.00
Status saved
U 51 | F 013056 | FPS 0122 | D 141 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.011'] | value_loss ['None', 'None', '0.019']
U 52 | F 013312 | FPS 0123 | D 143 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.039'] | value_loss ['None', 'None', '0.001']
U 53 | F 013568 | FPS 0123 | D 145 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.018'] | value_loss ['None', 'None', '0.021']
U 54 | F 013824 | FPS 0088 | D 148 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.032'] | value_loss ['None', 'None', '0.000']
U 55 | F 014080 | FPS 0121 | D 150 | Reward:μσmM -1.00 0.00 -1.00 -1.00 | policy_loss ['None', 'None', '0.029'] | value_loss ['None', 'None', '0.018']
U 56 | F 014336 | FPS 0123 | D 152 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '-0.003'] | value_loss ['None', 'None', '0.014']
U 57 | F 014592 | FPS 0123 | D 154 | Reward:μσmM -0.67 0.47 -1.00 0.00 | policy_loss ['None', 'None', '0.040'] | value_loss ['None', 'None', '0.031']
U 58 | F 014848 | FPS 0123 | D 156 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.040'] | value_loss ['None', 'None', '0.001']
U 59 | F 015104 | FPS 0089 | D 159 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.033'] | value_loss ['None', 'None', '0.001']
U 60 | F 015360 | FPS 0088 | D 162 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.011'] | value_loss ['None', 'None', '0.006']
U 10 | Test reward:μσmM -0.75 0.52 -1.00 0.54 | Test num frames:μσmM 123.00 70.47 31.00 256.00
Status saved
U 61 | F 015616 | FPS 0088 | D 171 | Reward:μσmM -0.75 0.43 -1.00 0.00 | policy_loss ['None', 'None', '0.100'] | value_loss ['None', 'None', '0.035']
U 62 | F 015872 | FPS 0122 | D 173 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.019'] | value_loss ['None', 'None', '0.011']
U 63 | F 016128 | FPS 0089 | D 176 | Reward:μσmM -0.75 0.43 -1.00 0.00 | policy_loss ['None', 'None', '0.069'] | value_loss ['None', 'None', '0.025']
U 64 | F 016384 | FPS 0089 | D 179 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '-0.028'] | value_loss ['None', 'None', '0.014']
U 65 | F 016640 | FPS 0088 | D 181 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.045'] | value_loss ['None', 'None', '0.001']
U 66 | F 016896 | FPS 0088 | D 184 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.006'] | value_loss ['None', 'None', '0.017']
U 67 | F 017152 | FPS 0080 | D 188 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.035'] | value_loss ['None', 'None', '0.000']
U 68 | F 017408 | FPS 0088 | D 190 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.025'] | value_loss ['None', 'None', '0.000']
U 69 | F 017664 | FPS 0088 | D 193 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.019'] | value_loss ['None', 'None', '0.000']
U 70 | F 017920 | FPS 0120 | D 195 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.019'] | value_loss ['None', 'None', '0.000']
U 10 | Test reward:μσmM -0.60 0.49 -1.00 0.00 | Test num frames:μσmM 200.60 64.00 89.00 256.00
Status saved
U 71 | F 018176 | FPS 0122 | D 202 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.014'] | value_loss ['None', 'None', '0.000']
U 72 | F 018432 | FPS 0123 | D 205 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.017'] | value_loss ['None', 'None', '0.000']
U 73 | F 018688 | FPS 0123 | D 207 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.015'] | value_loss ['None', 'None', '0.000']
U 74 | F 018944 | FPS 0122 | D 209 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.023'] | value_loss ['None', 'None', '0.017']
U 75 | F 019200 | FPS 0123 | D 211 | Reward:μσmM -0.67 0.47 -1.00 0.00 | policy_loss ['None', 'None', '0.062'] | value_loss ['None', 'None', '0.031']
U 76 | F 019456 | FPS 0123 | D 213 | Reward:μσmM -1.00 0.00 -1.00 -1.00 | policy_loss ['None', 'None', '0.025'] | value_loss ['None', 'None', '0.023']
U 77 | F 019712 | FPS 0089 | D 216 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.044'] | value_loss ['None', 'None', '0.000']
U 78 | F 019968 | FPS 0088 | D 219 | Reward:μσmM -0.67 0.47 -1.00 0.00 | policy_loss ['None', 'None', '0.057'] | value_loss ['None', 'None', '0.015']
U 79 | F 020224 | FPS 0123 | D 221 | Reward:μσmM -1.00 0.00 -1.00 -1.00 | policy_loss ['None', 'None', '0.066'] | value_loss ['None', 'None', '0.023']
U 80 | F 020480 | FPS 0121 | D 223 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.066'] | value_loss ['None', 'None', '0.000']
U 10 | Test reward:μσmM -0.60 0.49 -1.00 0.00 | Test num frames:μσmM 171.80 95.34 1.00 256.00
Status saved
U 81 | F 020736 | FPS 0120 | D 229 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.062'] | value_loss ['None', 'None', '0.000']
U 82 | F 020992 | FPS 0122 | D 231 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.053'] | value_loss ['None', 'None', '0.000']
U 83 | F 021248 | FPS 0122 | D 233 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.002'] | value_loss ['None', 'None', '0.008']
U 84 | F 021504 | FPS 0123 | D 236 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.054'] | value_loss ['None', 'None', '0.000']
U 85 | F 021760 | FPS 0123 | D 238 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.033'] | value_loss ['None', 'None', '0.000']
U 86 | F 022016 | FPS 0089 | D 240 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.025'] | value_loss ['None', 'None', '0.000']
U 87 | F 022272 | FPS 0122 | D 243 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.021'] | value_loss ['None', 'None', '0.000']
U 88 | F 022528 | FPS 0112 | D 245 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.016'] | value_loss ['None', 'None', '0.000']
U 89 | F 022784 | FPS 0095 | D 248 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.014'] | value_loss ['None', 'None', '0.000']
U 90 | F 023040 | FPS 0087 | D 251 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.044'] | value_loss ['None', 'None', '0.020']
U 10 | Test reward:μσmM -0.60 0.49 -1.00 0.00 | Test num frames:μσmM 168.70 79.19 39.00 256.00
Status saved
U 91 | F 023296 | FPS 0120 | D 257 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.043'] | value_loss ['None', 'None', '0.022']
U 92 | F 023552 | FPS 0121 | D 259 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.033'] | value_loss ['None', 'None', '0.012']
U 93 | F 023808 | FPS 0121 | D 261 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.039'] | value_loss ['None', 'None', '0.013']
U 94 | F 024064 | FPS 0121 | D 263 | Reward:μσmM -0.80 0.40 -1.00 0.00 | policy_loss ['None', 'None', '0.119'] | value_loss ['None', 'None', '0.052']
U 95 | F 024320 | FPS 0122 | D 265 | Reward:μσmM -0.67 0.47 -1.00 0.00 | policy_loss ['None', 'None', '0.042'] | value_loss ['None', 'None', '0.028']
U 96 | F 024576 | FPS 0121 | D 268 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '-0.008'] | value_loss ['None', 'None', '0.009']
U 97 | F 024832 | FPS 0088 | D 270 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.012'] | value_loss ['None', 'None', '0.007']
U 98 | F 025088 | FPS 0087 | D 273 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.030'] | value_loss ['None', 'None', '0.000']
U 99 | F 025344 | FPS 0122 | D 275 | Reward:μσmM -0.67 0.47 -1.00 0.00 | policy_loss ['None', 'None', '0.069'] | value_loss ['None', 'None', '0.030']
U 100 | F 025600 | FPS 0122 | D 278 | Reward:μσmM -0.75 0.43 -1.00 0.00 | policy_loss ['None', 'None', '0.077'] | value_loss ['None', 'None', '0.028']
U 10 | Test reward:μσmM -0.40 0.49 -1.00 0.00 | Test num frames:μσmM 201.80 95.27 3.00 256.00
Status saved
U 101 | F 025856 | FPS 0119 | D 285 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '-0.003'] | value_loss ['None', 'None', '0.006']
U 102 | F 026112 | FPS 0118 | D 287 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.044'] | value_loss ['None', 'None', '0.002']
U 103 | F 026368 | FPS 0122 | D 289 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.024'] | value_loss ['None', 'None', '0.000']
U 104 | F 026624 | FPS 0121 | D 291 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.026'] | value_loss ['None', 'None', '0.000']
U 105 | F 026880 | FPS 0121 | D 293 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.018'] | value_loss ['None', 'None', '0.012']
U 106 | F 027136 | FPS 0121 | D 295 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.020'] | value_loss ['None', 'None', '0.000']
U 107 | F 027392 | FPS 0101 | D 298 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.004'] | value_loss ['None', 'None', '0.014']
U 108 | F 027648 | FPS 0088 | D 301 | Reward:μσmM -0.45 0.71 -1.00 0.74 | policy_loss ['None', 'None', '0.046'] | value_loss ['None', 'None', '0.059']
U 109 | F 027904 | FPS 0120 | D 303 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.045'] | value_loss ['None', 'None', '0.000']
U 110 | F 028160 | FPS 0121 | D 305 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.035'] | value_loss ['None', 'None', '0.000']
U 10 | Test reward:μσmM -0.50 0.50 -1.00 0.00 | Test num frames:μσmM 184.50 81.45 54.00 256.00
Status saved
U 111 | F 028416 | FPS 0119 | D 312 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.027'] | value_loss ['None', 'None', '0.000']
U 112 | F 028672 | FPS 0093 | D 315 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.021'] | value_loss ['None', 'None', '0.000']
U 113 | F 028928 | FPS 0088 | D 317 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.028'] | value_loss ['None', 'None', '0.018']
U 114 | F 029184 | FPS 0097 | D 320 | Reward:μσmM -0.06 0.75 -1.00 0.83 | policy_loss ['None', 'None', '-0.043'] | value_loss ['None', 'None', '0.029']
U 115 | F 029440 | FPS 0088 | D 323 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '-0.040'] | value_loss ['None', 'None', '0.004']
U 116 | F 029696 | FPS 0088 | D 326 | Reward:μσmM 0.27 0.27 0.00 0.54 | policy_loss ['None', 'None', '-0.076'] | value_loss ['None', 'None', '0.002']
U 117 | F 029952 | FPS 0088 | D 329 | Reward:μσmM 0.49 0.35 0.00 0.79 | policy_loss ['None', 'None', '-0.092'] | value_loss ['None', 'None', '0.007']
U 118 | F 030208 | FPS 0096 | D 332 | Reward:μσmM 0.60 0.35 0.00 0.90 | policy_loss ['None', 'None', '-0.141'] | value_loss ['None', 'None', '0.007']
U 119 | F 030464 | FPS 0089 | D 334 | Reward:μσmM 0.80 0.31 0.00 0.95 | policy_loss ['None', 'None', '-0.210'] | value_loss ['None', 'None', '0.014']
U 120 | F 030720 | FPS 0122 | D 336 | Reward:μσmM 0.78 0.32 0.00 0.95 | policy_loss ['None', 'None', '-0.118'] | value_loss ['None', 'None', '0.010']
U 10 | Test reward:μσmM 0.93 0.02 0.88 0.96 | Test num frames:μσmM 28.50 9.08 18.00 50.00
Status saved
U 121 | F 030976 | FPS 0123 | D 339 | Reward:μσmM 0.82 0.31 0.00 0.96 | policy_loss ['None', 'None', '-0.132'] | value_loss ['None', 'None', '0.005']
U 122 | F 031232 | FPS 0113 | D 342 | Reward:μσmM 0.86 0.27 0.00 0.97 | policy_loss ['None', 'None', '-0.057'] | value_loss ['None', 'None', '0.001']
U 123 | F 031488 | FPS 0125 | D 344 | Reward:μσmM 0.87 0.26 0.00 0.97 | policy_loss ['None', 'None', '-0.026'] | value_loss ['None', 'None', '0.001']
U 124 | F 031744 | FPS 0125 | D 346 | Reward:μσmM 0.87 0.26 0.00 0.97 | policy_loss ['None', 'None', '-0.017'] | value_loss ['None', 'None', '0.001']
U 125 | F 032000 | FPS 0123 | D 348 | Reward:μσmM 0.42 0.82 -1.00 0.97 | policy_loss ['None', 'None', '0.147'] | value_loss ['None', 'None', '0.210']
U 126 | F 032256 | FPS 0123 | D 350 | Reward:μσmM 0.25 0.86 -1.00 0.95 | policy_loss ['None', 'None', '0.062'] | value_loss ['None', 'None', '0.141']
U 127 | F 032512 | FPS 0124 | D 352 | Reward:μσmM 0.81 0.31 0.00 0.95 | policy_loss ['None', 'None', '-0.088'] | value_loss ['None', 'None', '0.004']
U 128 | F 032768 | FPS 0089 | D 355 | Reward:μσmM 0.74 0.33 0.00 0.93 | policy_loss ['None', 'None', '-0.023'] | value_loss ['None', 'None', '0.004']
U 129 | F 033024 | FPS 0089 | D 358 | Reward:μσmM 0.83 0.29 0.00 0.96 | policy_loss ['None', 'None', '-0.035'] | value_loss ['None', 'None', '0.002']
U 130 | F 033280 | FPS 0089 | D 361 | Reward:μσmM 0.81 0.31 0.00 0.95 | policy_loss ['None', 'None', '0.008'] | value_loss ['None', 'None', '0.002']
U 10 | Test reward:μσmM 0.52 0.76 -1.00 0.96 | Test num frames:μσmM 40.00 21.66 18.00 84.00
Status saved
U 131 | F 033536 | FPS 0088 | D 365 | Reward:μσmM 0.83 0.29 0.00 0.96 | policy_loss ['None', 'None', '-0.008'] | value_loss ['None', 'None', '0.002']
U 132 | F 033792 | FPS 0089 | D 368 | Reward:μσmM 0.50 0.69 -1.00 0.93 | policy_loss ['None', 'None', '0.106'] | value_loss ['None', 'None', '0.094']
U 133 | F 034048 | FPS 0089 | D 371 | Reward:μσmM 0.41 0.71 -1.00 0.91 | policy_loss ['None', 'None', '0.087'] | value_loss ['None', 'None', '0.074']
U 134 | F 034304 | FPS 0122 | D 373 | Reward:μσmM 0.15 0.74 -1.00 0.81 | policy_loss ['None', 'None', '0.050'] | value_loss ['None', 'None', '0.036']
U 135 | F 034560 | FPS 0087 | D 376 | Reward:μσmM 0.19 0.78 -1.00 0.89 | policy_loss ['None', 'None', '0.039'] | value_loss ['None', 'None', '0.054']
U 136 | F 034816 | FPS 0125 | D 378 | Reward:μσmM -0.12 0.91 -1.00 0.94 | policy_loss ['None', 'None', '0.058'] | value_loss ['None', 'None', '0.140']
U 137 | F 035072 | FPS 0118 | D 380 | Reward:μσmM 0.44 0.77 -1.00 0.95 | policy_loss ['None', 'None', '-0.068'] | value_loss ['None', 'None', '0.013']
U 138 | F 035328 | FPS 0121 | D 383 | Reward:μσmM 0.85 0.28 0.00 0.95 | policy_loss ['None', 'None', '-0.093'] | value_loss ['None', 'None', '0.010']
U 139 | F 035584 | FPS 0087 | D 386 | Reward:μσmM 0.86 0.27 0.00 0.97 | policy_loss ['None', 'None', '-0.040'] | value_loss ['None', 'None', '0.005']
U 140 | F 035840 | FPS 0106 | D 388 | Reward:μσmM 0.58 0.72 -1.00 0.97 | policy_loss ['None', 'None', '0.097'] | value_loss ['None', 'None', '0.152']
U 10 | Test reward:μσmM 0.93 0.03 0.89 0.97 | Test num frames:μσmM 29.30 11.79 14.00 48.00
Status saved
U 141 | F 036096 | FPS 0097 | D 392 | Reward:μσmM 0.65 0.62 -1.00 0.95 | policy_loss ['None', 'None', '-0.012'] | value_loss ['None', 'None', '0.082']
U 142 | F 036352 | FPS 0110 | D 394 | Reward:μσmM 0.45 0.78 -1.00 0.96 | policy_loss ['None', 'None', '0.068'] | value_loss ['None', 'None', '0.156']
U 143 | F 036608 | FPS 0113 | D 396 | Reward:μσmM 0.70 0.35 0.00 0.91 | policy_loss ['None', 'None', '-0.037'] | value_loss ['None', 'None', '0.004']
U 144 | F 036864 | FPS 0098 | D 399 | Reward:μσmM 0.74 0.33 0.00 0.93 | policy_loss ['None', 'None', '-0.035'] | value_loss ['None', 'None', '0.002']
U 145 | F 037120 | FPS 0083 | D 402 | Reward:μσmM 0.41 0.70 -1.00 0.93 | policy_loss ['None', 'None', '0.028'] | value_loss ['None', 'None', '0.032']
discover.py --task-config task1 --discover 1 --algo ppo --env MiniGrid-ConfigWorld-v0-havekey --lr 0.0001 --AnomalyNN test_24 --model 20240725-seed1 --discount 0.99

Namespace(task_config='task1', discover=1, algo='ppo', env='MiniGrid-ConfigWorld-v0-havekey', model='20240725-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_24', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.0001, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': 0.7325000047683716, 'std': 0.5779932253755193, 'min': -1.0, 'max': 0.9549999833106995})
successful test! Start from: 2, reward per episode: OrderedDict({'mean': 0.9351052701473236, 'std': 0.018834531597451472, 'min': 0.8981578946113586, 'max': 0.9597368240356445})
discover.py --task-config task1 --discover 1 --algo ppo --env MiniGrid-ConfigWorld-v0-havekey --lr 0.0001 --AnomalyNN test_24 --model 20240725-seed1 --discount 0.99

Namespace(task_config='task1', discover=1, algo='ppo', env='MiniGrid-ConfigWorld-v0-havekey', model='20240725-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_24', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.0001, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': -0.7, 'std': 0.45825756949558394, 'min': -1.0, 'max': 0.0})
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0118 | D 2 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.760 | value -0.690 | policy_loss 0.100 | value_loss 0.018 | grad_norm 0.851
U 2 | F 000512 | FPS 0135 | D 4 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.752 | value -0.763 | policy_loss -0.030 | value_loss 0.010 | grad_norm 0.593
U 3 | F 000768 | FPS 0135 | D 5 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.747 | value -0.629 | policy_loss -0.027 | value_loss 0.013 | grad_norm 0.500
U 4 | F 001024 | FPS 0135 | D 7 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.749 | value -0.654 | policy_loss -0.104 | value_loss 0.005 | grad_norm 0.404
U 5 | F 001280 | FPS 0136 | D 9 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.714 | value -0.535 | policy_loss -0.080 | value_loss 0.001 | grad_norm 0.199
U 6 | F 001536 | FPS 0136 | D 11 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.606 | value -0.430 | policy_loss -0.091 | value_loss 0.000 | grad_norm 0.318
U 7 | F 001792 | FPS 0136 | D 13 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.645 | value -0.426 | policy_loss -0.009 | value_loss 0.012 | grad_norm 0.228
U 8 | F 002048 | FPS 0136 | D 15 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.611 | value -0.329 | policy_loss -0.062 | value_loss 0.001 | grad_norm 0.135
U 9 | F 002304 | FPS 0136 | D 17 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.648 | value -0.271 | policy_loss -0.056 | value_loss 0.000 | grad_norm 0.128
U 10 | F 002560 | FPS 0135 | D 19 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.607 | value -0.230 | policy_loss -0.046 | value_loss 0.000 | grad_norm 0.116
U 11 | F 002816 | FPS 0098 | D 21 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.660 | value -0.192 | policy_loss -0.037 | value_loss 0.000 | grad_norm 0.054
U 12 | F 003072 | FPS 0099 | D 24 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.726 | value -0.159 | policy_loss -0.033 | value_loss 0.000 | grad_norm 0.043
U 13 | F 003328 | FPS 0136 | D 26 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.814 | value -0.224 | policy_loss 0.073 | value_loss 0.027 | grad_norm 0.118
U 14 | F 003584 | FPS 0135 | D 28 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.853 | value -0.202 | policy_loss 0.012 | value_loss 0.014 | grad_norm 0.077
U 15 | F 003840 | FPS 0097 | D 30 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.828 | value -0.315 | policy_loss 0.085 | value_loss 0.038 | grad_norm 0.243
U 16 | F 004096 | FPS 0135 | D 32 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.857 | value -0.340 | policy_loss 0.054 | value_loss 0.023 | grad_norm 0.126
U 17 | F 004352 | FPS 0134 | D 34 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.818 | value -0.470 | policy_loss 0.063 | value_loss 0.018 | grad_norm 0.186
U 18 | F 004608 | FPS 0097 | D 37 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.782 | value -0.496 | policy_loss 0.055 | value_loss 0.024 | grad_norm 0.173
U 19 | F 004864 | FPS 0099 | D 39 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.755 | value -0.430 | policy_loss -0.038 | value_loss 0.008 | grad_norm 0.131
U 20 | F 005120 | FPS 0135 | D 41 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.793 | value -0.443 | policy_loss 0.017 | value_loss 0.023 | grad_norm 0.204
Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': -0.5108947396278382, 'std': 0.6468261275493679, 'min': -1.0, 'max': 0.8910526037216187})
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0097 | D 2 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.731 | value -0.415 | policy_loss -0.049 | value_loss 0.007 | grad_norm 0.112
U 2 | F 000512 | FPS 0134 | D 4 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.755 | value -0.390 | policy_loss -0.084 | value_loss 0.002 | grad_norm 0.115
U 3 | F 000768 | FPS 0088 | D 7 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.762 | value -0.333 | policy_loss -0.071 | value_loss 0.000 | grad_norm 0.108
U 4 | F 001024 | FPS 0127 | D 9 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.723 | value -0.291 | policy_loss -0.058 | value_loss 0.000 | grad_norm 0.055
U 5 | F 001280 | FPS 0135 | D 11 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.672 | value -0.241 | policy_loss -0.050 | value_loss 0.000 | grad_norm 0.060
U 6 | F 001536 | FPS 0134 | D 13 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.677 | value -0.205 | policy_loss -0.041 | value_loss 0.000 | grad_norm 0.061
U 7 | F 001792 | FPS 0122 | D 15 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.785 | value -0.258 | policy_loss 0.061 | value_loss 0.026 | grad_norm 0.238
U 8 | F 002048 | FPS 0126 | D 17 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.727 | value -0.205 | policy_loss -0.040 | value_loss 0.001 | grad_norm 0.099
U 9 | F 002304 | FPS 0134 | D 19 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.782 | value -0.159 | policy_loss -0.029 | value_loss 0.000 | grad_norm 0.028
U 10 | F 002560 | FPS 0135 | D 21 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.756 | value -0.132 | policy_loss -0.030 | value_loss 0.000 | grad_norm 0.030
U 11 | F 002816 | FPS 0134 | D 23 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.811 | value -0.116 | policy_loss -0.023 | value_loss 0.000 | grad_norm 0.023
U 12 | F 003072 | FPS 0132 | D 25 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.810 | value -0.184 | policy_loss 0.080 | value_loss 0.023 | grad_norm 0.102
U 13 | F 003328 | FPS 0134 | D 27 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.864 | value -0.157 | policy_loss 0.028 | value_loss 0.009 | grad_norm 0.039
U 14 | F 003584 | FPS 0134 | D 28 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.862 | value -0.085 | policy_loss -0.020 | value_loss 0.000 | grad_norm 0.021
U 15 | F 003840 | FPS 0132 | D 30 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.833 | value -0.120 | policy_loss 0.037 | value_loss 0.015 | grad_norm 0.031
U 16 | F 004096 | FPS 0134 | D 32 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.816 | value -0.155 | policy_loss -0.038 | value_loss 0.003 | grad_norm 0.079
U 17 | F 004352 | FPS 0135 | D 34 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.854 | value -0.105 | policy_loss -0.022 | value_loss 0.000 | grad_norm 0.021
U 18 | F 004608 | FPS 0132 | D 36 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.835 | value -0.181 | policy_loss 0.059 | value_loss 0.033 | grad_norm 0.172
U 19 | F 004864 | FPS 0099 | D 39 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.868 | value -0.105 | policy_loss -0.020 | value_loss 0.000 | grad_norm 0.021
U 20 | F 005120 | FPS 0098 | D 41 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.834 | value -0.145 | policy_loss 0.028 | value_loss 0.016 | grad_norm 0.104
Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': -0.7220263183116913, 'std': 0.5826465017706373, 'min': -1.0, 'max': 0.7797368168830872})
Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': -0.9, 'std': 0.30000000000000004, 'min': -1.0, 'max': 0.0})
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0130 | D 1 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.819 | value -0.149 | policy_loss 0.027 | value_loss 0.015 | grad_norm 0.030
U 2 | F 000512 | FPS 0134 | D 3 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.839 | value -0.107 | policy_loss -0.027 | value_loss 0.000 | grad_norm 0.048
U 3 | F 000768 | FPS 0097 | D 6 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.774 | value -0.093 | policy_loss -0.018 | value_loss 0.000 | grad_norm 0.021
U 4 | F 001024 | FPS 0097 | D 9 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.815 | value -0.218 | policy_loss 0.085 | value_loss 0.033 | grad_norm 0.137
U 5 | F 001280 | FPS 0133 | D 11 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.803 | value -0.257 | policy_loss 0.090 | value_loss 0.040 | grad_norm 0.081
U 6 | F 001536 | FPS 0131 | D 13 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.780 | value -0.262 | policy_loss 0.000 | value_loss 0.013 | grad_norm 0.084
U 7 | F 001792 | FPS 0132 | D 14 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.825 | value -0.212 | policy_loss -0.047 | value_loss 0.000 | grad_norm 0.059
U 8 | F 002048 | FPS 0132 | D 16 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.817 | value -0.161 | policy_loss -0.031 | value_loss 0.000 | grad_norm 0.031
U 9 | F 002304 | FPS 0132 | D 18 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.831 | value -0.191 | policy_loss 0.023 | value_loss 0.020 | grad_norm 0.130
U 10 | F 002560 | FPS 0104 | D 21 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.786 | value -0.236 | policy_loss 0.006 | value_loss 0.014 | grad_norm 0.093
U 11 | F 002816 | FPS 0131 | D 23 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.807 | value -0.244 | policy_loss -0.043 | value_loss 0.002 | grad_norm 0.093
U 12 | F 003072 | FPS 0131 | D 25 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.819 | value -0.242 | policy_loss 0.002 | value_loss 0.006 | grad_norm 0.041
U 13 | F 003328 | FPS 0131 | D 27 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.795 | value -0.206 | policy_loss 0.014 | value_loss 0.010 | grad_norm 0.061
U 14 | F 003584 | FPS 0097 | D 29 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.826 | value -0.309 | policy_loss -0.011 | value_loss 0.006 | grad_norm 0.103
U 15 | F 003840 | FPS 0097 | D 32 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.842 | value -0.221 | policy_loss -0.036 | value_loss 0.001 | grad_norm 0.117
U 16 | F 004096 | FPS 0133 | D 34 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.841 | value -0.192 | policy_loss 0.020 | value_loss 0.020 | grad_norm 0.043
U 17 | F 004352 | FPS 0132 | D 36 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.836 | value -0.228 | policy_loss -0.039 | value_loss 0.001 | grad_norm 0.055
U 18 | F 004608 | FPS 0119 | D 38 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.795 | value -0.210 | policy_loss -0.041 | value_loss 0.000 | grad_norm 0.053
U 19 | F 004864 | FPS 0086 | D 41 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.810 | value -0.151 | policy_loss -0.033 | value_loss 0.000 | grad_norm 0.041
U 20 | F 005120 | FPS 0113 | D 43 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.850 | value -0.127 | policy_loss -0.026 | value_loss 0.000 | grad_norm 0.022
Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': -0.3741315722465515, 'std': 0.7941975439298228, 'min': -1.0, 'max': 0.8176316022872925})
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0114 | D 2 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.841 | value -0.103 | policy_loss -0.019 | value_loss 0.000 | grad_norm 0.021
U 2 | F 000512 | FPS 0096 | D 4 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.853 | value -0.099 | policy_loss -0.018 | value_loss 0.000 | grad_norm 0.023
U 3 | F 000768 | FPS 0121 | D 7 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.815 | value -0.139 | policy_loss 0.036 | value_loss 0.018 | grad_norm 0.087
U 4 | F 001024 | FPS 0082 | D 10 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.845 | value -0.124 | policy_loss 0.008 | value_loss 0.018 | grad_norm 0.032
U 5 | F 001280 | FPS 0128 | D 12 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.838 | value -0.134 | policy_loss -0.030 | value_loss 0.000 | grad_norm 0.098
U 6 | F 001536 | FPS 0125 | D 14 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.860 | value -0.101 | policy_loss -0.021 | value_loss 0.000 | grad_norm 0.019
U 7 | F 001792 | FPS 0120 | D 16 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.886 | value -0.145 | policy_loss 0.034 | value_loss 0.022 | grad_norm 0.045
U 8 | F 002048 | FPS 0092 | D 19 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.849 | value -0.185 | policy_loss 0.019 | value_loss 0.018 | grad_norm 0.096
U 9 | F 002304 | FPS 0099 | D 21 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.865 | value -0.226 | policy_loss 0.068 | value_loss 0.035 | grad_norm 0.071
U 10 | F 002560 | FPS 0124 | D 23 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.853 | value -0.262 | policy_loss 0.012 | value_loss 0.018 | grad_norm 0.095
U 11 | F 002816 | FPS 0130 | D 25 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.871 | value -0.154 | policy_loss -0.034 | value_loss 0.000 | grad_norm 0.049
U 12 | F 003072 | FPS 0096 | D 28 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.868 | value -0.181 | policy_loss 0.018 | value_loss 0.011 | grad_norm 0.047
U 13 | F 003328 | FPS 0130 | D 30 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.870 | value -0.200 | policy_loss 0.056 | value_loss 0.022 | grad_norm 0.033
U 14 | F 003584 | FPS 0128 | D 32 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.860 | value -0.157 | policy_loss -0.036 | value_loss 0.002 | grad_norm 0.041
U 15 | F 003840 | FPS 0129 | D 34 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.887 | value -0.212 | policy_loss 0.069 | value_loss 0.041 | grad_norm 0.081
U 16 | F 004096 | FPS 0130 | D 36 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.813 | value -0.318 | policy_loss 0.040 | value_loss 0.016 | grad_norm 0.154
U 17 | F 004352 | FPS 0130 | D 38 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.843 | value -0.213 | policy_loss -0.034 | value_loss 0.001 | grad_norm 0.037
U 18 | F 004608 | FPS 0097 | D 40 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.849 | value -0.159 | policy_loss -0.031 | value_loss 0.000 | grad_norm 0.022
U 19 | F 004864 | FPS 0095 | D 43 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.822 | value -0.133 | policy_loss -0.027 | value_loss 0.000 | grad_norm 0.023
U 20 | F 005120 | FPS 0131 | D 45 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.797 | value -0.208 | policy_loss 0.018 | value_loss 0.013 | grad_norm 0.038
Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': -0.7198947370052338, 'std': 0.5881497251778428, 'min': -1.0, 'max': 0.8010526299476624})
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0128 | D 1 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.826 | value -0.114 | policy_loss -0.026 | value_loss 0.000 | grad_norm 0.031
U 2 | F 000512 | FPS 0130 | D 3 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.799 | value -0.101 | policy_loss -0.020 | value_loss 0.000 | grad_norm 0.013
U 3 | F 000768 | FPS 0131 | D 5 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.839 | value -0.086 | policy_loss -0.017 | value_loss 0.000 | grad_norm 0.008
U 4 | F 001024 | FPS 0112 | D 8 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.910 | value -0.123 | policy_loss 0.039 | value_loss 0.018 | grad_norm 0.042
U 5 | F 001280 | FPS 0129 | D 10 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.820 | value -0.258 | policy_loss 0.063 | value_loss 0.017 | grad_norm 0.085
U 6 | F 001536 | FPS 0097 | D 12 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.750 | value -0.330 | policy_loss -0.054 | value_loss 0.000 | grad_norm 0.086
U 7 | F 001792 | FPS 0097 | D 15 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.839 | value -0.179 | policy_loss -0.039 | value_loss 0.000 | grad_norm 0.035
U 8 | F 002048 | FPS 0133 | D 17 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.806 | value -0.149 | policy_loss -0.034 | value_loss 0.000 | grad_norm 0.055
U 9 | F 002304 | FPS 0095 | D 20 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.818 | value -0.128 | policy_loss -0.025 | value_loss 0.000 | grad_norm 0.025
U 10 | F 002560 | FPS 0091 | D 22 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.763 | value -0.166 | policy_loss 0.027 | value_loss 0.015 | grad_norm 0.027
U 11 | F 002816 | FPS 0119 | D 25 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.787 | value -0.116 | policy_loss -0.026 | value_loss 0.000 | grad_norm 0.059
U 12 | F 003072 | FPS 0132 | D 26 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.749 | value -0.197 | policy_loss 0.014 | value_loss 0.015 | grad_norm 0.060
U 13 | F 003328 | FPS 0131 | D 28 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.751 | value -0.159 | policy_loss -0.023 | value_loss 0.000 | grad_norm 0.060
U 14 | F 003584 | FPS 0129 | D 30 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.789 | value -0.136 | policy_loss -0.024 | value_loss 0.000 | grad_norm 0.022
U 15 | F 003840 | FPS 0098 | D 33 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.829 | value -0.110 | policy_loss -0.011 | value_loss 0.000 | grad_norm 0.014
U 16 | F 004096 | FPS 0131 | D 35 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.851 | value -0.100 | policy_loss -0.018 | value_loss 0.000 | grad_norm 0.013
U 17 | F 004352 | FPS 0131 | D 37 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.869 | value -0.093 | policy_loss -0.017 | value_loss 0.000 | grad_norm 0.013
U 18 | F 004608 | FPS 0131 | D 39 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.805 | value -0.182 | policy_loss 0.046 | value_loss 0.027 | grad_norm 0.101
U 19 | F 004864 | FPS 0130 | D 41 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.851 | value -0.188 | policy_loss 0.022 | value_loss 0.018 | grad_norm 0.043
U 20 | F 005120 | FPS 0097 | D 43 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.759 | value -0.327 | policy_loss 0.088 | value_loss 0.038 | grad_norm 0.242
Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': -0.7298421025276184, 'std': 0.5626293728161704, 'min': -1.0, 'max': 0.7015789747238159})
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0130 | D 1 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.803 | value -0.314 | policy_loss 0.032 | value_loss 0.012 | grad_norm 0.065
U 2 | F 000512 | FPS 0097 | D 4 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.786 | value -0.206 | policy_loss -0.039 | value_loss 0.000 | grad_norm 0.026
U 3 | F 000768 | FPS 0131 | D 6 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.799 | value -0.190 | policy_loss -0.039 | value_loss 0.001 | grad_norm 0.059
U 4 | F 001024 | FPS 0130 | D 8 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.786 | value -0.258 | policy_loss 0.007 | value_loss 0.005 | grad_norm 0.057
U 5 | F 001280 | FPS 0131 | D 10 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.780 | value -0.197 | policy_loss 0.018 | value_loss 0.015 | grad_norm 0.058
U 6 | F 001536 | FPS 0131 | D 12 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.839 | value -0.128 | policy_loss -0.030 | value_loss 0.000 | grad_norm 0.033
U 7 | F 001792 | FPS 0098 | D 15 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.853 | value -0.114 | policy_loss -0.024 | value_loss 0.000 | grad_norm 0.010
U 8 | F 002048 | FPS 0097 | D 17 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.841 | value -0.139 | policy_loss 0.032 | value_loss 0.016 | grad_norm 0.040
U 9 | F 002304 | FPS 0131 | D 19 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.837 | value -0.103 | policy_loss -0.021 | value_loss 0.000 | grad_norm 0.042
U 10 | F 002560 | FPS 0131 | D 21 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.894 | value -0.083 | policy_loss -0.017 | value_loss 0.000 | grad_norm 0.006
U 11 | F 002816 | FPS 0127 | D 23 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.806 | value -0.229 | policy_loss 0.138 | value_loss 0.048 | grad_norm 0.071
U 12 | F 003072 | FPS 0103 | D 26 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.798 | value -0.196 | policy_loss -0.040 | value_loss 0.000 | grad_norm 0.027
U 13 | F 003328 | FPS 0121 | D 28 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.769 | value -0.163 | policy_loss -0.035 | value_loss 0.000 | grad_norm 0.023
U 14 | F 003584 | FPS 0087 | D 31 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.788 | value -0.136 | policy_loss -0.027 | value_loss 0.000 | grad_norm 0.018
U 15 | F 003840 | FPS 0082 | D 34 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.812 | value -0.114 | policy_loss -0.023 | value_loss 0.000 | grad_norm 0.017
U 16 | F 004096 | FPS 0097 | D 36 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.826 | value -0.102 | policy_loss -0.021 | value_loss 0.000 | grad_norm 0.014
U 17 | F 004352 | FPS 0132 | D 38 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.863 | value -0.087 | policy_loss -0.017 | value_loss 0.000 | grad_norm 0.008
U 18 | F 004608 | FPS 0133 | D 40 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.868 | value -0.075 | policy_loss -0.018 | value_loss 0.000 | grad_norm 0.009
U 19 | F 004864 | FPS 0132 | D 42 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.803 | value -0.196 | policy_loss 0.136 | value_loss 0.047 | grad_norm 0.065
U 20 | F 005120 | FPS 0133 | D 44 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.824 | value -0.207 | policy_loss -0.055 | value_loss 0.000 | grad_norm 0.051
Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': -0.6170526325702668, 'std': 0.6229355554737065, 'min': -1.0, 'max': 0.8294736742973328})
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0131 | D 1 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.831 | value -0.155 | policy_loss -0.028 | value_loss 0.000 | grad_norm 0.029
U 2 | F 000512 | FPS 0133 | D 3 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.812 | value -0.126 | policy_loss -0.028 | value_loss 0.000 | grad_norm 0.019
U 3 | F 000768 | FPS 0133 | D 5 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.842 | value -0.102 | policy_loss -0.020 | value_loss 0.000 | grad_norm 0.015
U 4 | F 001024 | FPS 0132 | D 7 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.863 | value -0.086 | policy_loss -0.017 | value_loss 0.000 | grad_norm 0.015
U 5 | F 001280 | FPS 0131 | D 9 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.869 | value -0.125 | policy_loss 0.031 | value_loss 0.019 | grad_norm 0.040
U 6 | F 001536 | FPS 0131 | D 11 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.876 | value -0.186 | policy_loss 0.065 | value_loss 0.035 | grad_norm 0.126
U 7 | F 001792 | FPS 0097 | D 14 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.843 | value -0.281 | policy_loss 0.113 | value_loss 0.046 | grad_norm 0.099
U 8 | F 002048 | FPS 0132 | D 16 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.832 | value -0.391 | policy_loss 0.101 | value_loss 0.051 | grad_norm 0.132
U 9 | F 002304 | FPS 0097 | D 18 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.798 | value -0.439 | policy_loss 0.065 | value_loss 0.037 | grad_norm 0.089
U 10 | F 002560 | FPS 0133 | D 20 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.798 | value -0.425 | policy_loss -0.088 | value_loss 0.002 | grad_norm 0.080
U 11 | F 002816 | FPS 0131 | D 22 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.724 | value -0.330 | policy_loss -0.069 | value_loss 0.000 | grad_norm 0.054
U 12 | F 003072 | FPS 0132 | D 24 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.672 | value -0.280 | policy_loss -0.057 | value_loss 0.000 | grad_norm 0.043
U 13 | F 003328 | FPS 0131 | D 26 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.680 | value -0.235 | policy_loss -0.048 | value_loss 0.000 | grad_norm 0.054
U 14 | F 003584 | FPS 0131 | D 28 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.670 | value -0.247 | policy_loss 0.008 | value_loss 0.015 | grad_norm 0.045
U 15 | F 003840 | FPS 0132 | D 30 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.752 | value -0.217 | policy_loss -0.044 | value_loss 0.000 | grad_norm 0.076
U 16 | F 004096 | FPS 0126 | D 32 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.788 | value -0.179 | policy_loss -0.036 | value_loss 0.002 | grad_norm 0.028
U 17 | F 004352 | FPS 0118 | D 34 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.819 | value -0.152 | policy_loss -0.020 | value_loss 0.000 | grad_norm 0.028
U 18 | F 004608 | FPS 0130 | D 36 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.792 | value -0.241 | policy_loss 0.064 | value_loss 0.021 | grad_norm 0.036
U 19 | F 004864 | FPS 0128 | D 38 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.802 | value -0.162 | policy_loss -0.010 | value_loss 0.002 | grad_norm 0.027
U 20 | F 005120 | FPS 0129 | D 40 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.821 | value -0.153 | policy_loss -0.032 | value_loss 0.002 | grad_norm 0.042
Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': -0.8094736814498902, 'std': 0.5715789556503296, 'min': -1.0, 'max': 0.9052631855010986})
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0128 | D 2 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.800 | value -0.142 | policy_loss -0.023 | value_loss 0.000 | grad_norm 0.023
U 2 | F 000512 | FPS 0128 | D 4 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.818 | value -0.122 | policy_loss -0.027 | value_loss 0.000 | grad_norm 0.014
U 3 | F 000768 | FPS 0129 | D 5 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.846 | value -0.281 | policy_loss 0.181 | value_loss 0.070 | grad_norm 0.247
U 4 | F 001024 | FPS 0095 | D 8 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.835 | value -0.263 | policy_loss 0.002 | value_loss 0.015 | grad_norm 0.078
U 5 | F 001280 | FPS 0123 | D 10 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.806 | value -0.309 | policy_loss 0.034 | value_loss 0.027 | grad_norm 0.102
U 6 | F 001536 | FPS 0130 | D 12 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.827 | value -0.322 | policy_loss 0.025 | value_loss 0.017 | grad_norm 0.158
U 7 | F 001792 | FPS 0129 | D 14 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.720 | value -0.270 | policy_loss -0.060 | value_loss 0.001 | grad_norm 0.104
U 8 | F 002048 | FPS 0131 | D 16 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.826 | value -0.214 | policy_loss -0.048 | value_loss 0.000 | grad_norm 0.054
U 9 | F 002304 | FPS 0131 | D 18 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.724 | value -0.183 | policy_loss -0.039 | value_loss 0.000 | grad_norm 0.062
U 10 | F 002560 | FPS 0096 | D 21 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.817 | value -0.156 | policy_loss -0.032 | value_loss 0.000 | grad_norm 0.022
U 11 | F 002816 | FPS 0131 | D 23 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.766 | value -0.187 | policy_loss 0.021 | value_loss 0.016 | grad_norm 0.064
U 12 | F 003072 | FPS 0130 | D 25 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.750 | value -0.187 | policy_loss -0.045 | value_loss 0.000 | grad_norm 0.057
U 13 | F 003328 | FPS 0130 | D 27 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.826 | value -0.124 | policy_loss -0.024 | value_loss 0.000 | grad_norm 0.017
U 14 | F 003584 | FPS 0096 | D 29 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.779 | value -0.121 | policy_loss -0.026 | value_loss 0.000 | grad_norm 0.056
U 15 | F 003840 | FPS 0131 | D 31 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.857 | value -0.095 | policy_loss -0.019 | value_loss 0.000 | grad_norm 0.019
U 16 | F 004096 | FPS 0131 | D 33 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.875 | value -0.136 | policy_loss 0.039 | value_loss 0.016 | grad_norm 0.053
U 17 | F 004352 | FPS 0130 | D 35 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.833 | value -0.232 | policy_loss 0.121 | value_loss 0.057 | grad_norm 0.138
U 18 | F 004608 | FPS 0131 | D 37 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.837 | value -0.196 | policy_loss -0.042 | value_loss 0.003 | grad_norm 0.111
U 19 | F 004864 | FPS 0131 | D 39 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.843 | value -0.319 | policy_loss 0.142 | value_loss 0.045 | grad_norm 0.104
U 20 | F 005120 | FPS 0130 | D 41 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.819 | value -0.301 | policy_loss 0.038 | value_loss 0.025 | grad_norm 0.145
Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': -0.4682105302810669, 'std': 0.8123984794403557, 'min': -1.0, 'max': 0.7963157892227173})
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0131 | D 1 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.792 | value -0.293 | policy_loss -0.013 | value_loss 0.013 | grad_norm 0.076
U 2 | F 000512 | FPS 0130 | D 3 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.798 | value -0.359 | policy_loss 0.025 | value_loss 0.017 | grad_norm 0.066
U 3 | F 000768 | FPS 0131 | D 5 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.710 | value -0.252 | policy_loss -0.060 | value_loss 0.000 | grad_norm 0.093
U 4 | F 001024 | FPS 0130 | D 7 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.805 | value -0.201 | policy_loss -0.041 | value_loss 0.000 | grad_norm 0.034
U 5 | F 001280 | FPS 0130 | D 9 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.809 | value -0.173 | policy_loss -0.035 | value_loss 0.000 | grad_norm 0.022
U 6 | F 001536 | FPS 0131 | D 11 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.808 | value -0.147 | policy_loss -0.030 | value_loss 0.000 | grad_norm 0.021
U 7 | F 001792 | FPS 0131 | D 13 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.805 | value -0.124 | policy_loss -0.026 | value_loss 0.000 | grad_norm 0.022
U 8 | F 002048 | FPS 0129 | D 15 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.836 | value -0.105 | policy_loss -0.021 | value_loss 0.000 | grad_norm 0.021
U 9 | F 002304 | FPS 0131 | D 17 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.839 | value -0.089 | policy_loss -0.018 | value_loss 0.000 | grad_norm 0.013
U 10 | F 002560 | FPS 0126 | D 19 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.853 | value -0.075 | policy_loss -0.015 | value_loss 0.000 | grad_norm 0.015
U 11 | F 002816 | FPS 0124 | D 21 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.891 | value -0.063 | policy_loss -0.012 | value_loss 0.000 | grad_norm 0.012
U 12 | F 003072 | FPS 0130 | D 23 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.847 | value -0.182 | policy_loss 0.088 | value_loss 0.023 | grad_norm 0.068
U 13 | F 003328 | FPS 0130 | D 25 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.878 | value -0.153 | policy_loss 0.033 | value_loss 0.022 | grad_norm 0.055
U 14 | F 003584 | FPS 0129 | D 27 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.824 | value -0.279 | policy_loss 0.154 | value_loss 0.049 | grad_norm 0.119
U 15 | F 003840 | FPS 0130 | D 29 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.777 | value -0.220 | policy_loss -0.036 | value_loss 0.002 | grad_norm 0.076
U 16 | F 004096 | FPS 0128 | D 31 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.837 | value -0.227 | policy_loss 0.012 | value_loss 0.003 | grad_norm 0.030
U 17 | F 004352 | FPS 0097 | D 34 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.799 | value -0.154 | policy_loss -0.030 | value_loss 0.000 | grad_norm 0.022
U 18 | F 004608 | FPS 0130 | D 36 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.811 | value -0.203 | policy_loss 0.046 | value_loss 0.023 | grad_norm 0.066
U 19 | F 004864 | FPS 0110 | D 38 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.791 | value -0.254 | policy_loss 0.059 | value_loss 0.035 | grad_norm 0.082
U 20 | F 005120 | FPS 0131 | D 40 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.751 | value -0.273 | policy_loss -0.006 | value_loss 0.008 | grad_norm 0.047
Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': -0.6116052627563476, 'std': 0.6356691736213754, 'min': -1.0, 'max': 0.8839473724365234})
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0129 | D 1 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.789 | value -0.182 | policy_loss -0.039 | value_loss 0.000 | grad_norm 0.067
U 2 | F 000512 | FPS 0130 | D 3 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.816 | value -0.203 | policy_loss 0.015 | value_loss 0.008 | grad_norm 0.035
U 3 | F 000768 | FPS 0129 | D 5 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.778 | value -0.196 | policy_loss -0.045 | value_loss 0.001 | grad_norm 0.064
U 4 | F 001024 | FPS 0131 | D 7 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.839 | value -0.147 | policy_loss -0.023 | value_loss 0.000 | grad_norm 0.030
U 5 | F 001280 | FPS 0130 | D 9 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.830 | value -0.111 | policy_loss -0.023 | value_loss 0.000 | grad_norm 0.032
U 6 | F 001536 | FPS 0130 | D 11 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.811 | value -0.152 | policy_loss 0.034 | value_loss 0.023 | grad_norm 0.075
U 7 | F 001792 | FPS 0131 | D 13 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.771 | value -0.177 | policy_loss -0.028 | value_loss 0.000 | grad_norm 0.038
U 8 | F 002048 | FPS 0097 | D 16 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.826 | value -0.135 | policy_loss -0.025 | value_loss 0.000 | grad_norm 0.022
U 9 | F 002304 | FPS 0130 | D 18 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.836 | value -0.106 | policy_loss -0.015 | value_loss 0.002 | grad_norm 0.010
U 10 | F 002560 | FPS 0130 | D 20 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.823 | value -0.153 | policy_loss 0.034 | value_loss 0.023 | grad_norm 0.083
U 11 | F 002816 | FPS 0097 | D 23 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.800 | value -0.205 | policy_loss 0.026 | value_loss 0.021 | grad_norm 0.271
U 12 | F 003072 | FPS 0131 | D 24 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.851 | value -0.177 | policy_loss -0.025 | value_loss 0.002 | grad_norm 0.024
U 13 | F 003328 | FPS 0131 | D 26 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.873 | value -0.188 | policy_loss 0.024 | value_loss 0.016 | grad_norm 0.044
U 14 | F 003584 | FPS 0131 | D 28 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.818 | value -0.159 | policy_loss -0.038 | value_loss 0.000 | grad_norm 0.042
U 15 | F 003840 | FPS 0131 | D 30 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.833 | value -0.136 | policy_loss -0.028 | value_loss 0.000 | grad_norm 0.020
U 16 | F 004096 | FPS 0131 | D 32 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.802 | value -0.175 | policy_loss 0.026 | value_loss 0.014 | grad_norm 0.044
U 17 | F 004352 | FPS 0129 | D 34 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.781 | value -0.238 | policy_loss 0.008 | value_loss 0.018 | grad_norm 0.091
U 18 | F 004608 | FPS 0130 | D 36 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.768 | value -0.194 | policy_loss -0.041 | value_loss 0.001 | grad_norm 0.082
U 19 | F 004864 | FPS 0131 | D 38 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.833 | value -0.199 | policy_loss 0.019 | value_loss 0.011 | grad_norm 0.028
U 20 | F 005120 | FPS 0130 | D 40 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.835 | value -0.125 | policy_loss -0.028 | value_loss 0.000 | grad_norm 0.026
Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': -0.9, 'std': 0.30000000000000004, 'min': -1.0, 'max': 0.0})
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0131 | D 1 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.779 | value -0.166 | policy_loss 0.028 | value_loss 0.022 | grad_norm 0.048
U 2 | F 000512 | FPS 0131 | D 3 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.849 | value -0.173 | policy_loss 0.015 | value_loss 0.023 | grad_norm 0.071
U 3 | F 000768 | FPS 0129 | D 5 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.807 | value -0.348 | policy_loss 0.096 | value_loss 0.035 | grad_norm 0.096
U 4 | F 001024 | FPS 0131 | D 7 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.807 | value -0.261 | policy_loss -0.001 | value_loss 0.004 | grad_norm 0.031
U 5 | F 001280 | FPS 0131 | D 9 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.775 | value -0.190 | policy_loss -0.025 | value_loss 0.000 | grad_norm 0.026
U 6 | F 001536 | FPS 0132 | D 11 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.750 | value -0.269 | policy_loss 0.014 | value_loss 0.014 | grad_norm 0.073
U 7 | F 001792 | FPS 0131 | D 13 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.761 | value -0.216 | policy_loss 0.020 | value_loss 0.013 | grad_norm 0.072
U 8 | F 002048 | FPS 0108 | D 16 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.784 | value -0.158 | policy_loss -0.036 | value_loss 0.000 | grad_norm 0.041
U 9 | F 002304 | FPS 0133 | D 18 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.799 | value -0.134 | policy_loss -0.024 | value_loss 0.000 | grad_norm 0.020
U 10 | F 002560 | FPS 0131 | D 19 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.850 | value -0.173 | policy_loss 0.027 | value_loss 0.019 | grad_norm 0.049
U 11 | F 002816 | FPS 0125 | D 22 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.824 | value -0.195 | policy_loss 0.013 | value_loss 0.007 | grad_norm 0.070
U 12 | F 003072 | FPS 0131 | D 23 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.807 | value -0.196 | policy_loss -0.027 | value_loss 0.002 | grad_norm 0.085
U 13 | F 003328 | FPS 0132 | D 25 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.800 | value -0.131 | policy_loss -0.026 | value_loss 0.000 | grad_norm 0.040
U 14 | F 003584 | FPS 0131 | D 27 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.843 | value -0.130 | policy_loss -0.003 | value_loss 0.017 | grad_norm 0.013
U 15 | F 003840 | FPS 0132 | D 29 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.874 | value -0.151 | policy_loss 0.026 | value_loss 0.019 | grad_norm 0.033
U 16 | F 004096 | FPS 0132 | D 31 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.828 | value -0.157 | policy_loss -0.028 | value_loss 0.001 | grad_norm 0.026
U 17 | F 004352 | FPS 0130 | D 33 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.808 | value -0.235 | policy_loss 0.040 | value_loss 0.023 | grad_norm 0.075
U 18 | F 004608 | FPS 0132 | D 35 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.815 | value -0.138 | policy_loss -0.032 | value_loss 0.000 | grad_norm 0.057
U 19 | F 004864 | FPS 0131 | D 37 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.783 | value -0.291 | policy_loss 0.021 | value_loss 0.009 | grad_norm 0.052
U 20 | F 005120 | FPS 0131 | D 39 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.825 | value -0.192 | policy_loss 0.017 | value_loss 0.021 | grad_norm 0.054
Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': -0.6134999990463257, 'std': 0.6312212392022427, 'min': -1.0, 'max': 0.8650000095367432})
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0129 | D 1 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.852 | value -0.183 | policy_loss 0.021 | value_loss 0.016 | grad_norm 0.022
U 2 | F 000512 | FPS 0131 | D 3 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.867 | value -0.181 | policy_loss 0.024 | value_loss 0.014 | grad_norm 0.047
U 3 | F 000768 | FPS 0130 | D 5 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.868 | value -0.141 | policy_loss -0.028 | value_loss 0.000 | grad_norm 0.026
U 4 | F 001024 | FPS 0129 | D 7 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.819 | value -0.282 | policy_loss 0.121 | value_loss 0.055 | grad_norm 0.091
U 5 | F 001280 | FPS 0129 | D 9 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.794 | value -0.325 | policy_loss -0.075 | value_loss 0.007 | grad_norm 0.100
U 6 | F 001536 | FPS 0129 | D 11 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.795 | value -0.280 | policy_loss -0.040 | value_loss 0.004 | grad_norm 0.073
U 7 | F 001792 | FPS 0130 | D 13 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.792 | value -0.228 | policy_loss -0.049 | value_loss 0.000 | grad_norm 0.046
U 8 | F 002048 | FPS 0126 | D 15 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.818 | value -0.190 | policy_loss -0.038 | value_loss 0.000 | grad_norm 0.030
U 9 | F 002304 | FPS 0096 | D 18 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.786 | value -0.168 | policy_loss -0.029 | value_loss 0.000 | grad_norm 0.018
U 10 | F 002560 | FPS 0105 | D 20 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.763 | value -0.140 | policy_loss -0.029 | value_loss 0.000 | grad_norm 0.025
U 11 | F 002816 | FPS 0097 | D 23 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.837 | value -0.118 | policy_loss -0.023 | value_loss 0.000 | grad_norm 0.024
U 12 | F 003072 | FPS 0096 | D 26 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.843 | value -0.099 | policy_loss -0.019 | value_loss 0.000 | grad_norm 0.017
U 13 | F 003328 | FPS 0132 | D 28 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.845 | value -0.083 | policy_loss -0.016 | value_loss 0.000 | grad_norm 0.010
U 14 | F 003584 | FPS 0132 | D 30 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.876 | value -0.070 | policy_loss -0.014 | value_loss 0.000 | grad_norm 0.013
U 15 | F 003840 | FPS 0132 | D 32 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.854 | value -0.121 | policy_loss 0.040 | value_loss 0.022 | grad_norm 0.024
U 16 | F 004096 | FPS 0132 | D 34 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.830 | value -0.161 | policy_loss 0.072 | value_loss 0.032 | grad_norm 0.086
U 17 | F 004352 | FPS 0132 | D 36 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.848 | value -0.099 | policy_loss -0.026 | value_loss 0.000 | grad_norm 0.036
U 18 | F 004608 | FPS 0132 | D 37 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.848 | value -0.108 | policy_loss 0.015 | value_loss 0.013 | grad_norm 0.043
U 19 | F 004864 | FPS 0131 | D 39 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.844 | value -0.270 | policy_loss 0.128 | value_loss 0.047 | grad_norm 0.087
U 20 | F 005120 | FPS 0131 | D 41 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.822 | value -0.213 | policy_loss -0.004 | value_loss 0.020 | grad_norm 0.086
Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': -0.6168157875537872, 'std': 0.623485698494655, 'min': -1.0, 'max': 0.8318421244621277})
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0130 | D 1 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.799 | value -0.219 | policy_loss 0.012 | value_loss 0.011 | grad_norm 0.044
U 2 | F 000512 | FPS 0131 | D 3 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.816 | value -0.233 | policy_loss 0.010 | value_loss 0.011 | grad_norm 0.032
U 3 | F 000768 | FPS 0132 | D 5 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.833 | value -0.263 | policy_loss 0.063 | value_loss 0.030 | grad_norm 0.057
U 4 | F 001024 | FPS 0131 | D 7 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.812 | value -0.231 | policy_loss -0.001 | value_loss 0.016 | grad_norm 0.077
U 5 | F 001280 | FPS 0095 | D 10 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.805 | value -0.272 | policy_loss -0.001 | value_loss 0.005 | grad_norm 0.041
U 6 | F 001536 | FPS 0085 | D 13 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.764 | value -0.210 | policy_loss -0.053 | value_loss 0.002 | grad_norm 0.039
U 7 | F 001792 | FPS 0113 | D 15 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.728 | value -0.154 | policy_loss -0.033 | value_loss 0.000 | grad_norm 0.026
U 8 | F 002048 | FPS 0090 | D 18 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.761 | value -0.130 | policy_loss -0.026 | value_loss 0.000 | grad_norm 0.022
discover.py --task-config task1 --discover 1 --algo ppo --env MiniGrid-ConfigWorld-v0-havekey --lr 0.0001 --AnomalyNN test_24 --model 20240725-seed1 --discount 0.99

Namespace(task_config='task1', discover=1, algo='ppo', env='MiniGrid-ConfigWorld-v0-havekey', model='20240725-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_24', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.0001, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': -0.4371842086315155, 'std': 0.7358107104248323, 'min': -1.0, 'max': 0.8176316022872925})
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0090 | D 2 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.757 | value -0.687 | policy_loss 0.109 | value_loss 0.024 | grad_norm 2.015
U 2 | F 000512 | FPS 0097 | D 5 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.779 | value -0.554 | policy_loss -0.038 | value_loss 0.006 | grad_norm 0.551
U 3 | F 000768 | FPS 0125 | D 7 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.740 | value -0.646 | policy_loss -0.021 | value_loss 0.013 | grad_norm 0.446
U 4 | F 001024 | FPS 0124 | D 9 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.759 | value -0.579 | policy_loss -0.049 | value_loss 0.007 | grad_norm 0.461
U 5 | F 001280 | FPS 0130 | D 11 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.738 | value -0.482 | policy_loss -0.021 | value_loss 0.010 | grad_norm 0.282
U 6 | F 001536 | FPS 0123 | D 13 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.730 | value -0.478 | policy_loss 0.026 | value_loss 0.008 | grad_norm 0.102
U 7 | F 001792 | FPS 0131 | D 15 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.713 | value -0.464 | policy_loss -0.031 | value_loss 0.012 | grad_norm 0.278
U 8 | F 002048 | FPS 0130 | D 17 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.616 | value -0.361 | policy_loss -0.067 | value_loss 0.002 | grad_norm 0.256
U 9 | F 002304 | FPS 0132 | D 19 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.601 | value -0.360 | policy_loss 0.002 | value_loss 0.022 | grad_norm 0.290
U 10 | F 002560 | FPS 0133 | D 21 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.592 | value -0.287 | policy_loss -0.060 | value_loss 0.000 | grad_norm 0.140
U 11 | F 002816 | FPS 0130 | D 23 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.643 | value -0.281 | policy_loss -0.006 | value_loss 0.005 | grad_norm 0.078
U 12 | F 003072 | FPS 0129 | D 25 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.730 | value -0.418 | policy_loss -0.040 | value_loss 0.004 | grad_norm 0.172
U 13 | F 003328 | FPS 0131 | D 27 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.714 | value -0.345 | policy_loss 0.032 | value_loss 0.024 | grad_norm 0.167
U 14 | F 003584 | FPS 0133 | D 29 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.702 | value -0.308 | policy_loss -0.070 | value_loss 0.000 | grad_norm 0.138
U 15 | F 003840 | FPS 0132 | D 31 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.708 | value -0.286 | policy_loss -0.017 | value_loss 0.015 | grad_norm 0.095
U 16 | F 004096 | FPS 0130 | D 33 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.718 | value -0.369 | policy_loss 0.070 | value_loss 0.022 | grad_norm 0.110
U 17 | F 004352 | FPS 0132 | D 35 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.674 | value -0.256 | policy_loss -0.066 | value_loss 0.001 | grad_norm 0.113
U 18 | F 004608 | FPS 0132 | D 37 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.711 | value -0.194 | policy_loss -0.038 | value_loss 0.000 | grad_norm 0.047
U 19 | F 004864 | FPS 0132 | D 38 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.727 | value -0.218 | policy_loss 0.011 | value_loss 0.007 | grad_norm 0.090
U 20 | F 005120 | FPS 0132 | D 40 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.754 | value -0.291 | policy_loss -0.029 | value_loss 0.002 | grad_norm 0.080
Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': -0.7, 'std': 0.45825756949558394, 'min': -1.0, 'max': 0.0})
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0129 | D 1 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.757 | value -0.355 | policy_loss -0.007 | value_loss 0.016 | grad_norm 0.175
U 2 | F 000512 | FPS 0130 | D 3 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.789 | value -0.372 | policy_loss -0.071 | value_loss 0.000 | grad_norm 0.169
U 3 | F 000768 | FPS 0133 | D 5 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.820 | value -0.322 | policy_loss -0.014 | value_loss 0.012 | grad_norm 0.089
U 4 | F 001024 | FPS 0102 | D 8 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.664 | value -0.229 | policy_loss -0.045 | value_loss 0.000 | grad_norm 0.068
U 5 | F 001280 | FPS 0132 | D 10 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.610 | value -0.192 | policy_loss -0.038 | value_loss 0.000 | grad_norm 0.049
U 6 | F 001536 | FPS 0131 | D 12 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.720 | value -0.160 | policy_loss -0.032 | value_loss 0.000 | grad_norm 0.032
U 7 | F 001792 | FPS 0131 | D 14 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.782 | value -0.234 | policy_loss 0.047 | value_loss 0.024 | grad_norm 0.127
U 8 | F 002048 | FPS 0132 | D 16 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.837 | value -0.124 | policy_loss -0.027 | value_loss 0.000 | grad_norm 0.035
U 9 | F 002304 | FPS 0132 | D 18 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.774 | value -0.208 | policy_loss 0.015 | value_loss 0.021 | grad_norm 0.120
U 10 | F 002560 | FPS 0133 | D 20 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.796 | value -0.216 | policy_loss 0.017 | value_loss 0.012 | grad_norm 0.113
U 11 | F 002816 | FPS 0125 | D 22 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.833 | value -0.306 | policy_loss 0.045 | value_loss 0.017 | grad_norm 0.074
U 12 | F 003072 | FPS 0132 | D 24 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.790 | value -0.329 | policy_loss 0.065 | value_loss 0.029 | grad_norm 0.120
U 13 | F 003328 | FPS 0132 | D 25 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.806 | value -0.331 | policy_loss 0.040 | value_loss 0.022 | grad_norm 0.105
U 14 | F 003584 | FPS 0133 | D 27 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.769 | value -0.290 | policy_loss -0.047 | value_loss 0.001 | grad_norm 0.110
U 15 | F 003840 | FPS 0131 | D 29 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.806 | value -0.367 | policy_loss 0.036 | value_loss 0.014 | grad_norm 0.074
U 16 | F 004096 | FPS 0132 | D 31 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.803 | value -0.213 | policy_loss -0.044 | value_loss 0.000 | grad_norm 0.045
U 17 | F 004352 | FPS 0131 | D 33 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.798 | value -0.213 | policy_loss -0.023 | value_loss 0.001 | grad_norm 0.041
U 18 | F 004608 | FPS 0130 | D 35 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.767 | value -0.234 | policy_loss 0.003 | value_loss 0.003 | grad_norm 0.047
U 19 | F 004864 | FPS 0098 | D 38 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.786 | value -0.243 | policy_loss -0.062 | value_loss 0.000 | grad_norm 0.075
U 20 | F 005120 | FPS 0131 | D 40 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.829 | value -0.233 | policy_loss 0.010 | value_loss 0.018 | grad_norm 0.075
Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': -0.6092368423938751, 'std': 0.6412565614917973, 'min': -1.0, 'max': 0.9076315760612488})
Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': -0.4388421058654785, 'std': 0.8572707455339054, 'min': -1.0, 'max': 0.8957894444465637})
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0128 | D 1 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.802 | value -0.185 | policy_loss -0.042 | value_loss 0.000 | grad_norm 0.058
U 2 | F 000512 | FPS 0131 | D 3 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.806 | value -0.153 | policy_loss -0.032 | value_loss 0.000 | grad_norm 0.028
U 3 | F 000768 | FPS 0132 | D 5 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.796 | value -0.128 | policy_loss -0.026 | value_loss 0.000 | grad_norm 0.022
U 4 | F 001024 | FPS 0133 | D 7 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.822 | value -0.164 | policy_loss 0.032 | value_loss 0.022 | grad_norm 0.072
U 5 | F 001280 | FPS 0132 | D 9 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.806 | value -0.265 | policy_loss 0.080 | value_loss 0.039 | grad_norm 0.084
U 6 | F 001536 | FPS 0131 | D 11 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.820 | value -0.327 | policy_loss 0.039 | value_loss 0.018 | grad_norm 0.118
U 7 | F 001792 | FPS 0131 | D 13 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.779 | value -0.222 | policy_loss -0.056 | value_loss 0.003 | grad_norm 0.152
U 8 | F 002048 | FPS 0132 | D 15 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.779 | value -0.262 | policy_loss -0.052 | value_loss 0.002 | grad_norm 0.142
U 9 | F 002304 | FPS 0132 | D 17 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.801 | value -0.293 | policy_loss 0.046 | value_loss 0.023 | grad_norm 0.073
U 10 | F 002560 | FPS 0098 | D 20 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.808 | value -0.258 | policy_loss -0.008 | value_loss 0.011 | grad_norm 0.141
U 11 | F 002816 | FPS 0097 | D 22 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.826 | value -0.247 | policy_loss -0.049 | value_loss 0.001 | grad_norm 0.064
U 12 | F 003072 | FPS 0132 | D 24 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.757 | value -0.212 | policy_loss -0.050 | value_loss 0.000 | grad_norm 0.057
U 13 | F 003328 | FPS 0131 | D 26 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.822 | value -0.224 | policy_loss 0.011 | value_loss 0.012 | grad_norm 0.100
U 14 | F 003584 | FPS 0131 | D 28 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.830 | value -0.179 | policy_loss -0.038 | value_loss 0.000 | grad_norm 0.051
U 15 | F 003840 | FPS 0131 | D 30 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.721 | value -0.136 | policy_loss -0.030 | value_loss 0.000 | grad_norm 0.029
U 16 | F 004096 | FPS 0131 | D 32 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.806 | value -0.192 | policy_loss 0.074 | value_loss 0.023 | grad_norm 0.072
U 17 | F 004352 | FPS 0131 | D 34 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.810 | value -0.222 | policy_loss 0.062 | value_loss 0.025 | grad_norm 0.062
U 18 | F 004608 | FPS 0132 | D 36 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.826 | value -0.181 | policy_loss -0.019 | value_loss 0.002 | grad_norm 0.044
U 19 | F 004864 | FPS 0131 | D 38 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.845 | value -0.262 | policy_loss 0.070 | value_loss 0.035 | grad_norm 0.083
U 20 | F 005120 | FPS 0131 | D 40 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.831 | value -0.256 | policy_loss -0.015 | value_loss 0.010 | grad_norm 0.054
Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': -0.6409736812114716, 'std': 0.7181559437342394, 'min': -1.0, 'max': 0.8223684430122375})
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0132 | D 1 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.804 | value -0.351 | policy_loss 0.051 | value_loss 0.031 | grad_norm 0.172
U 2 | F 000512 | FPS 0098 | D 4 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.749 | value -0.283 | policy_loss -0.061 | value_loss 0.000 | grad_norm 0.068
U 3 | F 000768 | FPS 0131 | D 6 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.801 | value -0.227 | policy_loss -0.002 | value_loss 0.006 | grad_norm 0.048
U 4 | F 001024 | FPS 0132 | D 8 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.725 | value -0.299 | policy_loss -0.069 | value_loss 0.001 | grad_norm 0.108
U 5 | F 001280 | FPS 0098 | D 11 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.708 | value -0.225 | policy_loss -0.043 | value_loss 0.000 | grad_norm 0.047
U 6 | F 001536 | FPS 0131 | D 13 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.830 | value -0.190 | policy_loss 0.017 | value_loss 0.010 | grad_norm 0.046
U 7 | F 001792 | FPS 0133 | D 14 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.656 | value -0.150 | policy_loss -0.033 | value_loss 0.000 | grad_norm 0.079
U 8 | F 002048 | FPS 0131 | D 16 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.742 | value -0.213 | policy_loss 0.080 | value_loss 0.037 | grad_norm 0.064
U 9 | F 002304 | FPS 0097 | D 19 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.726 | value -0.205 | policy_loss -0.023 | value_loss 0.007 | grad_norm 0.110
U 10 | F 002560 | FPS 0098 | D 22 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.724 | value -0.118 | policy_loss -0.039 | value_loss 0.000 | grad_norm 0.048
U 11 | F 002816 | FPS 0098 | D 24 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.576 | value -0.110 | policy_loss -0.033 | value_loss 0.000 | grad_norm 0.031
U 12 | F 003072 | FPS 0131 | D 26 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.708 | value -0.080 | policy_loss -0.019 | value_loss 0.000 | grad_norm 0.017
U 13 | F 003328 | FPS 0097 | D 29 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.666 | value -0.073 | policy_loss -0.012 | value_loss 0.000 | grad_norm 0.007
U 14 | F 003584 | FPS 0101 | D 31 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.769 | value -0.273 | policy_loss 0.089 | value_loss 0.036 | grad_norm 0.119
U 15 | F 003840 | FPS 0103 | D 34 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.747 | value -0.144 | policy_loss -0.046 | value_loss 0.001 | grad_norm 0.070
U 16 | F 004096 | FPS 0097 | D 37 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.826 | value -0.132 | policy_loss -0.027 | value_loss 0.000 | grad_norm 0.051
U 17 | F 004352 | FPS 0097 | D 39 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.827 | value -0.169 | policy_loss 0.028 | value_loss 0.016 | grad_norm 0.067
U 18 | F 004608 | FPS 0097 | D 42 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.845 | value -0.222 | policy_loss 0.096 | value_loss 0.042 | grad_norm 0.062
U 19 | F 004864 | FPS 0119 | D 44 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.860 | value -0.221 | policy_loss 0.012 | value_loss 0.009 | grad_norm 0.090
U 20 | F 005120 | FPS 0119 | D 46 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.876 | value -0.246 | policy_loss 0.066 | value_loss 0.033 | grad_norm 0.092
Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': -0.4656052589416504, 'std': 0.8164281100229206, 'min': -1.0, 'max': 0.8010526299476624})
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0129 | D 1 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.853 | value -0.278 | policy_loss 0.047 | value_loss 0.020 | grad_norm 0.058
U 2 | F 000512 | FPS 0093 | D 4 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.798 | value -0.241 | policy_loss -0.043 | value_loss 0.001 | grad_norm 0.057
U 3 | F 000768 | FPS 0113 | D 6 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.768 | value -0.299 | policy_loss 0.061 | value_loss 0.032 | grad_norm 0.104
U 4 | F 001024 | FPS 0114 | D 9 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.688 | value -0.365 | policy_loss 0.016 | value_loss 0.015 | grad_norm 0.078
U 5 | F 001280 | FPS 0101 | D 11 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.738 | value -0.333 | policy_loss -0.069 | value_loss 0.001 | grad_norm 0.067
U 6 | F 001536 | FPS 0124 | D 13 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.800 | value -0.280 | policy_loss -0.061 | value_loss 0.000 | grad_norm 0.071
U 7 | F 001792 | FPS 0105 | D 16 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.737 | value -0.285 | policy_loss -0.058 | value_loss 0.001 | grad_norm 0.099
U 8 | F 002048 | FPS 0122 | D 18 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.736 | value -0.230 | policy_loss -0.043 | value_loss 0.000 | grad_norm 0.039
U 9 | F 002304 | FPS 0130 | D 20 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.725 | value -0.201 | policy_loss -0.038 | value_loss 0.000 | grad_norm 0.036
U 10 | F 002560 | FPS 0131 | D 22 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.760 | value -0.203 | policy_loss -0.004 | value_loss 0.015 | grad_norm 0.027
U 11 | F 002816 | FPS 0130 | D 24 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.788 | value -0.167 | policy_loss -0.041 | value_loss 0.000 | grad_norm 0.036
U 12 | F 003072 | FPS 0129 | D 26 | Reward:μσmM 0.60 0.00 0.60 0.60 |  entropy 1.751 | value -0.112 | policy_loss -0.088 | value_loss 0.008 | grad_norm 0.092
U 13 | F 003328 | FPS 0130 | D 28 | Reward:μσmM 0.60 0.00 0.60 0.60 |  entropy 1.790 | value -0.100 | policy_loss -0.020 | value_loss 0.000 | grad_norm 0.012
U 14 | F 003584 | FPS 0111 | D 30 | Reward:μσmM 0.60 0.00 0.60 0.60 |  entropy 1.803 | value -0.081 | policy_loss -0.017 | value_loss 0.000 | grad_norm 0.008
U 15 | F 003840 | FPS 0098 | D 33 | Reward:μσmM 0.60 0.00 0.60 0.60 |  entropy 1.824 | value -0.070 | policy_loss -0.014 | value_loss 0.000 | grad_norm 0.009
U 16 | F 004096 | FPS 0130 | D 35 | Reward:μσmM 0.60 0.00 0.60 0.60 |  entropy 1.863 | value -0.058 | policy_loss -0.012 | value_loss 0.000 | grad_norm 0.005
U 17 | F 004352 | FPS 0130 | D 37 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.868 | value -0.143 | policy_loss 0.091 | value_loss 0.031 | grad_norm 0.060
U 18 | F 004608 | FPS 0129 | D 39 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.859 | value -0.111 | policy_loss -0.026 | value_loss 0.000 | grad_norm 0.029
U 19 | F 004864 | FPS 0127 | D 41 | Reward:μσmM 0.61 0.00 0.61 0.61 |  entropy 1.814 | value -0.066 | policy_loss -0.085 | value_loss 0.006 | grad_norm 0.026
U 20 | F 005120 | FPS 0129 | D 43 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.808 | value -0.041 | policy_loss 0.021 | value_loss 0.015 | grad_norm 0.066
Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': -0.7227368414402008, 'std': 0.5808161625948514, 'min': -1.0, 'max': 0.7726315855979919})
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0128 | D 2 | Reward:μσmM -0.56 0.77 -1.00 0.77 |  entropy 1.739 | value -0.080 | policy_loss 0.015 | value_loss 0.041 | grad_norm 0.105
U 2 | F 000512 | FPS 0127 | D 4 | Reward:μσmM -0.37 0.90 -1.00 0.90 |  entropy 1.698 | value -0.114 | policy_loss 0.011 | value_loss 0.049 | grad_norm 0.129
U 3 | F 000768 | FPS 0128 | D 6 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.766 | value -0.104 | policy_loss 0.032 | value_loss 0.016 | grad_norm 0.039
U 4 | F 001024 | FPS 0126 | D 8 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.723 | value -0.076 | policy_loss 0.008 | value_loss 0.014 | grad_norm 0.105
U 5 | F 001280 | FPS 0130 | D 10 | Reward:μσmM -0.07 0.93 -1.00 0.86 |  entropy 1.683 | value -0.032 | policy_loss -0.030 | value_loss 0.032 | grad_norm 0.114
U 6 | F 001536 | FPS 0127 | D 12 | Reward:μσmM -0.09 0.91 -1.00 0.82 |  entropy 1.770 | value -0.012 | policy_loss -0.048 | value_loss 0.039 | grad_norm 0.171
U 7 | F 001792 | FPS 0125 | D 14 | Reward:μσmM 0.82 0.04 0.77 0.86 |  entropy 1.530 | value 0.297 | policy_loss -0.159 | value_loss 0.016 | grad_norm 0.157
U 8 | F 002048 | FPS 0124 | D 16 | Reward:μσmM 0.89 0.03 0.84 0.94 |  entropy 1.404 | value 0.539 | policy_loss -0.162 | value_loss 0.011 | grad_norm 0.228
U 9 | F 002304 | FPS 0126 | D 18 | Reward:μσmM 0.72 0.15 0.57 0.87 |  entropy 1.544 | value 0.469 | policy_loss -0.011 | value_loss 0.009 | grad_norm 0.120
U 10 | F 002560 | FPS 0123 | D 20 | Reward:μσmM 0.90 0.01 0.87 0.91 |  entropy 1.383 | value 0.624 | policy_loss -0.098 | value_loss 0.003 | grad_norm 0.133
U 11 | F 002816 | FPS 0119 | D 22 | Reward:μσmM 0.91 0.05 0.83 0.96 |  entropy 1.387 | value 0.605 | policy_loss -0.068 | value_loss 0.009 | grad_norm 0.121
U 12 | F 003072 | FPS 0087 | D 25 | Reward:μσmM 0.91 0.04 0.86 0.96 |  entropy 1.338 | value 0.680 | policy_loss -0.048 | value_loss 0.003 | grad_norm 0.092
U 13 | F 003328 | FPS 0102 | D 27 | Reward:μσmM 0.68 0.64 -1.00 0.95 |  entropy 1.404 | value 0.763 | policy_loss -0.015 | value_loss 0.012 | grad_norm 0.085
U 14 | F 003584 | FPS 0121 | D 30 | Reward:μσmM 0.89 0.04 0.81 0.93 |  entropy 1.576 | value 0.719 | policy_loss 0.022 | value_loss 0.002 | grad_norm 0.052
U 15 | F 003840 | FPS 0120 | D 32 | Reward:μσmM -0.15 0.95 -1.00 0.95 |  entropy 1.588 | value 0.543 | policy_loss 0.282 | value_loss 0.302 | grad_norm 0.654
U 16 | F 004096 | FPS 0123 | D 34 | Reward:μσmM 0.88 0.04 0.83 0.94 |  entropy 1.568 | value 0.637 | policy_loss -0.060 | value_loss 0.003 | grad_norm 0.129
U 17 | F 004352 | FPS 0122 | D 36 | Reward:μσmM 0.92 0.03 0.87 0.94 |  entropy 1.476 | value 0.732 | policy_loss -0.050 | value_loss 0.004 | grad_norm 0.064
U 18 | F 004608 | FPS 0101 | D 38 | Reward:μσmM 0.92 0.03 0.86 0.96 |  entropy 1.443 | value 0.730 | policy_loss -0.032 | value_loss 0.002 | grad_norm 0.095
U 19 | F 004864 | FPS 0115 | D 41 | Reward:μσmM 0.44 0.83 -1.00 0.95 |  entropy 1.568 | value 0.630 | policy_loss 0.134 | value_loss 0.167 | grad_norm 0.192
U 20 | F 005120 | FPS 0122 | D 43 | Reward:μσmM 0.20 0.93 -1.00 0.95 |  entropy 1.540 | value 0.557 | policy_loss 0.089 | value_loss 0.124 | grad_norm 0.156
U 141 | F 036096 | FPS 0070 | D 3 | Reward:μσmM 1.58 0.71 0.00 1.94 | policy_loss ['None', 'None', '-0.075', '-0.423'] | value_loss ['None', 'None', '0.002', '0.200']
U 142 | F 036352 | FPS 0113 | D 5 | Reward:μσmM 1.63 0.67 0.00 1.94 | policy_loss ['None', 'None', '-0.021', '-0.548'] | value_loss ['None', 'None', '0.002', '0.149']
U 143 | F 036608 | FPS 0115 | D 8 | Reward:μσmM 1.55 0.39 1.00 1.85 | policy_loss ['None', 'None', '0.074', '-0.318'] | value_loss ['None', 'None', '0.001', '0.032']
U 144 | F 036864 | FPS 0100 | D 10 | Reward:μσmM 1.66 0.38 1.00 1.91 | policy_loss ['None', 'None', '0.018', '-0.177'] | value_loss ['None', 'None', '0.002', '0.020']
U 145 | F 037120 | FPS 0096 | D 13 | Reward:μσmM 1.74 0.33 1.00 1.93 | policy_loss ['None', 'None', '-0.017', '-0.077'] | value_loss ['None', 'None', '0.005', '0.005']
U 146 | F 037376 | FPS 0097 | D 16 | Reward:μσmM 1.13 0.86 0.00 1.95 | policy_loss ['None', 'None', '0.142', '0.129'] | value_loss ['None', 'None', '0.123', '0.137']
U 147 | F 037632 | FPS 0114 | D 18 | Reward:μσmM 1.63 0.67 0.00 1.95 | policy_loss ['None', 'None', '-0.116', '-0.011'] | value_loss ['None', 'None', '0.007', '0.013']
U 148 | F 037888 | FPS 0113 | D 20 | Reward:μσmM 1.51 0.69 0.00 1.96 | policy_loss ['None', 'None', '0.060', '-0.064'] | value_loss ['None', 'None', '0.080', '0.034']
U 149 | F 038144 | FPS 0113 | D 22 | Reward:μσmM 1.44 0.83 0.00 1.93 | policy_loss ['None', 'None', '0.034', '-0.028'] | value_loss ['None', 'None', '0.127', '0.015']
U 150 | F 038400 | FPS 0114 | D 25 | Reward:μσmM 1.68 0.64 0.00 1.94 | policy_loss ['None', 'None', '-0.091', '-0.045'] | value_loss ['None', 'None', '0.006', '0.007']
U 10 | Test reward:μσmM 0.93 0.03 0.88 0.96 | Test num frames:μσmM 28.70 10.84 18.00 50.00
Status saved
U 151 | F 038656 | FPS 0103 | D 31 | Reward:μσmM 1.93 0.02 1.90 1.95 | policy_loss ['None', 'None', '-0.055', '-0.083'] | value_loss ['None', 'None', '0.002', '0.005']
U 152 | F 038912 | FPS 0106 | D 34 | Reward:μσmM 1.86 0.27 1.00 1.97 | policy_loss ['None', 'None', '-0.030', '-0.101'] | value_loss ['None', 'None', '0.001', '0.004']
U 153 | F 039168 | FPS 0114 | D 36 | Reward:μσmM 1.83 0.29 1.00 1.96 | policy_loss ['None', 'None', '0.022', '-0.021'] | value_loss ['None', 'None', '0.002', '0.003']
U 154 | F 039424 | FPS 0112 | D 38 | Reward:μσmM 1.85 0.28 1.00 1.96 | policy_loss ['None', 'None', '-0.011', '0.002'] | value_loss ['None', 'None', '0.002', '0.007']
U 155 | F 039680 | FPS 0113 | D 40 | Reward:μσmM 1.80 0.52 0.00 1.97 | policy_loss ['None', 'None', '-0.029', '-0.062'] | value_loss ['None', 'None', '0.001', '0.001']
U 156 | F 039936 | FPS 0091 | D 43 | Reward:μσmM 1.46 0.78 0.00 1.97 | policy_loss ['None', 'None', '0.235', '0.094'] | value_loss ['None', 'None', '0.216', '0.044']
U 157 | F 040192 | FPS 0116 | D 45 | Reward:μσmM 1.78 0.32 1.00 1.96 | policy_loss ['None', 'None', '-0.025', '0.016'] | value_loss ['None', 'None', '0.003', '0.024']
U 158 | F 040448 | FPS 0114 | D 48 | Reward:μσmM 1.83 0.29 1.00 1.96 | policy_loss ['None', 'None', '-0.006', '-0.031'] | value_loss ['None', 'None', '0.002', '0.002']
U 159 | F 040704 | FPS 0114 | D 50 | Reward:μσmM 1.24 0.84 0.00 1.95 | policy_loss ['None', 'None', '0.236', '0.110'] | value_loss ['None', 'None', '0.202', '0.019']
U 160 | F 040960 | FPS 0112 | D 52 | Reward:μσmM 0.81 1.41 -1.00 1.93 | policy_loss ['None', 'None', '-0.081', '0.381'] | value_loss ['None', 'None', '0.002', '0.707']
U 10 | Test reward:μσmM 0.14 0.93 -1.00 0.95 | Test num frames:μσmM 32.60 20.62 2.00 70.00
Status saved
U 161 | F 041216 | FPS 0114 | D 59 | Reward:μσmM 0.82 1.41 -1.00 1.95 | policy_loss ['None', 'None', '-0.023', '0.499'] | value_loss ['None', 'None', '0.002', '0.957']
U 162 | F 041472 | FPS 0110 | D 61 | Reward:μσmM 1.69 0.64 0.00 1.96 | policy_loss ['None', 'None', '-0.046', '-0.231'] | value_loss ['None', 'None', '0.002', '0.061']
U 163 | F 041728 | FPS 0115 | D 63 | Reward:μσmM 1.46 1.00 -1.00 1.96 | policy_loss ['None', 'None', '-0.032', '0.077'] | value_loss ['None', 'None', '0.001', '0.259']
U 164 | F 041984 | FPS 0109 | D 66 | Reward:μσmM 1.86 0.27 1.00 1.96 | policy_loss ['None', 'None', '-0.009', '-0.220'] | value_loss ['None', 'None', '0.001', '0.020']
U 165 | F 042240 | FPS 0093 | D 69 | Reward:μσmM 1.86 0.27 1.00 1.97 | policy_loss ['None', 'None', '-0.015', '-0.035'] | value_loss ['None', 'None', '0.001', '0.001']
U 166 | F 042496 | FPS 0113 | D 71 | Reward:μσmM 1.18 0.88 0.00 1.96 | policy_loss ['None', 'None', '0.238', '0.239'] | value_loss ['None', 'None', '0.310', '0.165']
U 167 | F 042752 | FPS 0113 | D 73 | Reward:μσmM 1.17 1.10 -1.00 1.95 | policy_loss ['None', 'None', '0.091', '0.064'] | value_loss ['None', 'None', '0.180', '0.115']
U 168 | F 043008 | FPS 0111 | D 75 | Reward:μσmM 1.90 0.02 1.87 1.92 | policy_loss ['None', 'None', '-0.052', '0.009'] | value_loss ['None', 'None', '0.001', '0.007']
U 169 | F 043264 | FPS 0115 | D 78 | Reward:μσmM 1.45 0.73 0.00 1.96 | policy_loss ['None', 'None', '0.144', '-0.058'] | value_loss ['None', 'None', '0.122', '0.008']
U 170 | F 043520 | FPS 0111 | D 80 | Reward:μσmM 1.09 1.09 -1.00 1.95 | policy_loss ['None', 'None', '0.096', '0.054'] | value_loss ['None', 'None', '0.081', '0.075']
U 10 | Test reward:μσmM 0.55 0.77 -1.00 0.96 | Test num frames:μσmM 28.50 8.21 17.00 43.00
Status saved
U 171 | F 043776 | FPS 0112 | D 86 | Reward:μσmM 1.80 0.30 1.00 1.95 | policy_loss ['None', 'None', '-0.077', '-0.087'] | value_loss ['None', 'None', '0.004', '0.008']
U 172 | F 044032 | FPS 0115 | D 88 | Reward:μσmM 1.94 0.02 1.91 1.96 | policy_loss ['None', 'None', '-0.043', '-0.136'] | value_loss ['None', 'None', '0.001', '0.004']
U 173 | F 044288 | FPS 0111 | D 91 | Reward:μσmM 1.79 0.54 0.00 1.97 | policy_loss ['None', 'None', '-0.024', '-0.076'] | value_loss ['None', 'None', '0.001', '0.002']
U 174 | F 044544 | FPS 0107 | D 93 | Reward:μσmM 1.54 0.80 0.00 1.97 | policy_loss ['None', 'None', '0.198', '0.043'] | value_loss ['None', 'None', '0.212', '0.055']
U 175 | F 044800 | FPS 0116 | D 95 | Reward:μσmM 1.77 0.56 0.00 1.96 | policy_loss ['None', 'None', '-0.047', '-0.026'] | value_loss ['None', 'None', '0.003', '0.006']
U 176 | F 045056 | FPS 0110 | D 97 | Reward:μσmM 1.77 0.56 0.00 1.95 | policy_loss ['None', 'None', '0.005', '-0.034'] | value_loss ['None', 'None', '0.000', '0.001']
U 177 | F 045312 | FPS 0092 | D 100 | Reward:μσmM 1.39 0.80 0.00 1.95 | policy_loss ['None', 'None', '0.174', '0.116'] | value_loss ['None', 'None', '0.177', '0.090']
U 178 | F 045568 | FPS 0115 | D 102 | Reward:μσmM 1.76 0.34 1.00 1.94 | policy_loss ['None', 'None', '-0.022', '0.085'] | value_loss ['None', 'None', '0.008', '0.018']
U 179 | F 045824 | FPS 0112 | D 105 | Reward:μσmM 1.75 0.34 1.00 1.94 | policy_loss ['None', 'None', '-0.043', '0.036'] | value_loss ['None', 'None', '0.006', '0.013']
U 180 | F 046080 | FPS 0099 | D 107 | Reward:μσmM 1.17 1.10 -1.00 1.95 | policy_loss ['None', 'None', '0.032', '0.092'] | value_loss ['None', 'None', '0.079', '0.292']
U 10 | Test reward:μσmM 0.93 0.02 0.89 0.95 | Test num frames:μσmM 31.60 8.04 20.00 47.00
Status saved
U 181 | F 046336 | FPS 0113 | D 114 | Reward:μσmM 1.78 0.32 1.00 1.96 | policy_loss ['None', 'None', '-0.021', '-0.069'] | value_loss ['None', 'None', '0.001', '0.012']
U 182 | F 046592 | FPS 0111 | D 116 | Reward:μσmM 1.83 0.29 1.00 1.96 | policy_loss ['None', 'None', '0.000', '-0.086'] | value_loss ['None', 'None', '0.001', '0.004']
U 183 | F 046848 | FPS 0111 | D 119 | Reward:μσmM 1.78 0.32 1.00 1.96 | policy_loss ['None', 'None', '0.008', '-0.019'] | value_loss ['None', 'None', '0.008', '0.007']
U 184 | F 047104 | FPS 0114 | D 121 | Reward:μσmM 1.81 0.31 1.00 1.95 | policy_loss ['None', 'None', '0.002', '-0.010'] | value_loss ['None', 'None', '0.004', '0.002']
U 185 | F 047360 | FPS 0113 | D 123 | Reward:μσmM 1.81 0.31 1.00 1.96 | policy_loss ['None', 'None', '-0.016', '0.021'] | value_loss ['None', 'None', '0.005', '0.006']
U 186 | F 047616 | FPS 0115 | D 125 | Reward:μσmM 1.56 0.66 0.00 1.94 | policy_loss ['None', 'None', '0.043', '0.068'] | value_loss ['None', 'None', '0.084', '0.060']
U 187 | F 047872 | FPS 0116 | D 127 | Reward:μσmM 1.44 0.73 0.00 1.95 | policy_loss ['None', 'None', '0.049', '0.122'] | value_loss ['None', 'None', '0.043', '0.064']
U 188 | F 048128 | FPS 0107 | D 130 | Reward:μσmM 1.90 0.06 1.77 1.94 | policy_loss ['None', 'None', '-0.049', '0.050'] | value_loss ['None', 'None', '0.002', '0.020']
U 189 | F 048384 | FPS 0113 | D 132 | Reward:μσmM 1.68 0.64 0.00 1.94 | policy_loss ['None', 'None', '-0.009', '-0.069'] | value_loss ['None', 'None', '0.001', '0.006']
U 190 | F 048640 | FPS 0112 | D 134 | Reward:μσmM 1.57 0.67 0.00 1.96 | policy_loss ['None', 'None', '0.026', '0.089'] | value_loss ['None', 'None', '0.021', '0.108']
U 10 | Test reward:μσmM 0.92 0.03 0.86 0.95 | Test num frames:μσmM 33.50 10.88 19.00 58.00
Status saved
discover.py --task-config task2 --discover 1 --algo ppo --env MiniGrid-ConfigWorld-v0-havekey --lr 0.0001 --AnomalyNN test_24 --model 20240725-seed1 --discount 0.99

Namespace(task_config='task2', discover=1, algo='ppo', env='MiniGrid-ConfigWorld-v0-havekey', model='20240725-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_24', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.0001, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Optimizer loaded

Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': -0.9, 'std': 0.30000000000000004, 'min': -1.0, 'max': 0.0})
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0124 | D 2 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.810 | value 0.421 | policy_loss 0.286 | value_loss 0.172 | grad_norm 2.636
discover.py --task-config task2 --discover 1 --algo ppo --env MiniGrid-ConfigWorld-v0-havekey --lr 0.0001 --AnomalyNN test_24 --model 20240725-seed1 --discount 0.99

Namespace(task_config='task2', discover=1, algo='ppo', env='MiniGrid-ConfigWorld-v0-havekey', model='20240725-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_24', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.0001, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Optimizer loaded

Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': -0.438368421792984, 'std': 0.8583188864658825, 'min': -1.0, 'max': 0.9407894611358643})
Optimizer loaded

Start discovering in 5000 steps.

discover.py --task-config task2 --discover 1 --algo ppo --env MiniGrid-ConfigWorld-v0-havekey --lr 0.0001 --AnomalyNN test_24 --model 20240725-seed1 --discount 0.99

Namespace(task_config='task2', discover=1, algo='ppo', env='MiniGrid-ConfigWorld-v0-havekey', model='20240725-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_24', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.0001, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Optimizer loaded

discover.py --task-config task2 --discover 1 --algo ppo --env MiniGrid-ConfigWorld-v0-havekey --lr 0.0001 --AnomalyNN test_24 --model 20240725-seed1 --discount 0.99

Namespace(task_config='task2', discover=1, algo='ppo', env='MiniGrid-ConfigWorld-v0-havekey', model='20240725-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_24', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.0001, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Optimizer loaded

Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': 0.7306052684783936, 'std': 0.5771378202709944, 'min': -1.0, 'max': 0.9573684334754944})
successful test! Start from: 2, reward per episode: OrderedDict({'mean': 0.9036052644252777, 'std': 0.04140403987460671, 'min': 0.7915789484977722, 'max': 0.9431579113006592})
discover.py --task-config task2 --discover 1 --algo ppo --env MiniGrid-ConfigWorld-v0-havekey --lr 0.0001 --AnomalyNN test_24 --model 20240725-seed1 --discount 0.99

Namespace(task_config='task2', discover=1, algo='ppo', env='MiniGrid-ConfigWorld-v0-havekey', model='20240725-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_24', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.0001, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Optimizer loaded

Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': 0.6571842133998871, 'std': 0.5547152957916007, 'min': -1.0, 'max': 0.926578938961029})
Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': 0.485368424654007, 'std': 0.7460489594213118, 'min': -1.0, 'max': 0.9549999833106995})
Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': 0.5879736840724945, 'std': 0.5894499997063584, 'min': -1.0, 'max': 0.921842098236084})
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0101 | D 2 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.745 | value 0.610 | policy_loss 0.179 | value_loss 0.137 | grad_norm 1.348
U 2 | F 000512 | FPS 0128 | D 4 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.755 | value 0.391 | policy_loss 0.274 | value_loss 0.183 | grad_norm 0.802
U 3 | F 000768 | FPS 0123 | D 6 | Reward:μσmM -0.50 0.87 -1.00 1.00 |  entropy 1.726 | value 0.284 | policy_loss 0.166 | value_loss 0.122 | grad_norm 0.531
U 4 | F 001024 | FPS 0098 | D 9 | Reward:μσmM -0.33 0.94 -1.00 1.00 |  entropy 1.688 | value 0.232 | policy_loss 0.143 | value_loss 0.106 | grad_norm 0.675
U 5 | F 001280 | FPS 0131 | D 11 | Reward:μσmM -0.33 0.94 -1.00 1.00 |  entropy 1.712 | value 0.144 | policy_loss 0.089 | value_loss 0.123 | grad_norm 0.654
U 6 | F 001536 | FPS 0132 | D 13 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.678 | value 0.073 | policy_loss -0.046 | value_loss 0.009 | grad_norm 0.352
U 7 | F 001792 | FPS 0134 | D 15 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.703 | value 0.244 | policy_loss -0.017 | value_loss 0.012 | grad_norm 0.282
U 8 | F 002048 | FPS 0104 | D 17 | Reward:μσmM 0.33 0.94 -1.00 1.00 |  entropy 1.681 | value 0.157 | policy_loss -0.023 | value_loss 0.032 | grad_norm 0.366
U 9 | F 002304 | FPS 0111 | D 19 | Reward:μσmM 0.33 0.94 -1.00 1.00 |  entropy 1.725 | value 0.090 | policy_loss -0.033 | value_loss 0.018 | grad_norm 0.204
U 10 | F 002560 | FPS 0133 | D 21 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.738 | value -0.052 | policy_loss -0.050 | value_loss 0.003 | grad_norm 0.257
U 11 | F 002816 | FPS 0100 | D 24 | Reward:μσmM 0.33 0.94 -1.00 1.00 |  entropy 1.755 | value 0.308 | policy_loss 0.053 | value_loss 0.017 | grad_norm 0.162
U 12 | F 003072 | FPS 0095 | D 26 | Reward:μσmM 0.20 0.98 -1.00 1.00 |  entropy 1.767 | value 0.166 | policy_loss 0.060 | value_loss 0.033 | grad_norm 0.263
U 13 | F 003328 | FPS 0131 | D 28 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.709 | value -0.024 | policy_loss 0.070 | value_loss 0.080 | grad_norm 0.160
U 14 | F 003584 | FPS 0133 | D 30 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.702 | value -0.117 | policy_loss -0.069 | value_loss 0.003 | grad_norm 0.168
U 15 | F 003840 | FPS 0130 | D 32 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.767 | value -0.110 | policy_loss 0.001 | value_loss 0.013 | grad_norm 0.122
U 16 | F 004096 | FPS 0124 | D 34 | Reward:μσmM 0.20 0.98 -1.00 1.00 |  entropy 1.748 | value -0.120 | policy_loss 0.024 | value_loss 0.020 | grad_norm 0.114
U 17 | F 004352 | FPS 0130 | D 36 | Reward:μσmM 0.33 0.94 -1.00 1.00 |  entropy 1.788 | value -0.104 | policy_loss -0.008 | value_loss 0.011 | grad_norm 0.071
U 18 | F 004608 | FPS 0133 | D 38 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.801 | value 0.060 | policy_loss -0.017 | value_loss 0.003 | grad_norm 0.066
U 19 | F 004864 | FPS 0133 | D 40 | Reward:μσmM -0.33 0.94 -1.00 1.00 |  entropy 1.732 | value 0.259 | policy_loss 0.174 | value_loss 0.091 | grad_norm 0.207
U 20 | F 005120 | FPS 0132 | D 42 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.764 | value 0.181 | policy_loss 0.086 | value_loss 0.062 | grad_norm 0.278
discover.py --task-config task2 --discover 1 --algo ppo --env MiniGrid-ConfigWorld-v0 --lr 0.0001 --AnomalyNN test_24 --model 20240725-seed1 --discount 0.99

Namespace(task_config='task2', discover=1, algo='ppo', env='MiniGrid-ConfigWorld-v0', model='20240725-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_24', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.0001, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Optimizer loaded

Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': -0.3, 'std': 0.45825756949558394, 'min': -1.0, 'max': 0.0})
Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': -0.6, 'std': 0.48989794855663565, 'min': -1.0, 'max': 0.0})
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0118 | D 2 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.812 | value 0.373 | policy_loss 0.407 | value_loss 0.256 | grad_norm 1.483
U 2 | F 000512 | FPS 0134 | D 4 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.738 | value 0.179 | policy_loss 0.382 | value_loss 0.251 | grad_norm 1.792
U 3 | F 000768 | FPS 0135 | D 5 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.787 | value 0.173 | policy_loss 0.280 | value_loss 0.171 | grad_norm 1.536
U 4 | F 001024 | FPS 0135 | D 7 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.738 | value -0.073 | policy_loss 0.062 | value_loss 0.033 | grad_norm 0.198
U 5 | F 001280 | FPS 0134 | D 9 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.759 | value -0.127 | policy_loss 0.131 | value_loss 0.051 | grad_norm 0.296
U 6 | F 001536 | FPS 0135 | D 11 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.727 | value -0.060 | policy_loss -0.019 | value_loss 0.001 | grad_norm 0.243
U 7 | F 001792 | FPS 0135 | D 13 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.883 | value -0.025 | policy_loss -0.004 | value_loss 0.000 | grad_norm 0.025
U 8 | F 002048 | FPS 0099 | D 16 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.825 | value -0.183 | policy_loss 0.133 | value_loss 0.061 | grad_norm 0.185
U 9 | F 002304 | FPS 0135 | D 18 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.838 | value -0.236 | policy_loss 0.048 | value_loss 0.031 | grad_norm 0.151
U 10 | F 002560 | FPS 0134 | D 19 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.801 | value -0.297 | policy_loss 0.076 | value_loss 0.042 | grad_norm 0.250
U 11 | F 002816 | FPS 0134 | D 21 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.858 | value -0.267 | policy_loss 0.044 | value_loss 0.017 | grad_norm 0.106
U 12 | F 003072 | FPS 0133 | D 23 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.768 | value -0.252 | policy_loss 0.043 | value_loss 0.034 | grad_norm 0.262
U 13 | F 003328 | FPS 0134 | D 25 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.807 | value -0.220 | policy_loss -0.045 | value_loss 0.000 | grad_norm 0.094
U 14 | F 003584 | FPS 0133 | D 27 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.828 | value -0.275 | policy_loss 0.057 | value_loss 0.025 | grad_norm 0.110
U 15 | F 003840 | FPS 0133 | D 29 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.832 | value -0.251 | policy_loss -0.001 | value_loss 0.017 | grad_norm 0.135
U 16 | F 004096 | FPS 0134 | D 31 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.840 | value -0.273 | policy_loss 0.059 | value_loss 0.023 | grad_norm 0.104
U 17 | F 004352 | FPS 0133 | D 33 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.834 | value -0.223 | policy_loss -0.059 | value_loss 0.001 | grad_norm 0.109
U 18 | F 004608 | FPS 0132 | D 35 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.804 | value -0.164 | policy_loss -0.042 | value_loss 0.000 | grad_norm 0.070
U 19 | F 004864 | FPS 0133 | D 37 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.771 | value -0.138 | policy_loss -0.029 | value_loss 0.003 | grad_norm 0.042
U 20 | F 005120 | FPS 0133 | D 39 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.756 | value -0.296 | policy_loss 0.135 | value_loss 0.048 | grad_norm 0.197
Test 10 turns results: Start from 3, reward per episode: OrderedDict({'mean': -0.1, 'std': 0.30000000000000004, 'min': -1.0, 'max': 0.0})
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0132 | D 1 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.697 | value -0.260 | policy_loss -0.043 | value_loss 0.005 | grad_norm 0.117
U 2 | F 000512 | FPS 0135 | D 3 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.746 | value -0.217 | policy_loss -0.049 | value_loss 0.000 | grad_norm 0.113
U 3 | F 000768 | FPS 0135 | D 5 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.798 | value -0.171 | policy_loss -0.035 | value_loss 0.000 | grad_norm 0.031
U 4 | F 001024 | FPS 0134 | D 7 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.759 | value -0.144 | policy_loss -0.029 | value_loss 0.000 | grad_norm 0.033
U 5 | F 001280 | FPS 0133 | D 9 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.791 | value -0.158 | policy_loss 0.025 | value_loss 0.012 | grad_norm 0.102
U 6 | F 001536 | FPS 0133 | D 11 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.791 | value -0.123 | policy_loss -0.030 | value_loss 0.000 | grad_norm 0.048
U 7 | F 001792 | FPS 0132 | D 13 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.786 | value -0.232 | policy_loss 0.138 | value_loss 0.042 | grad_norm 0.192
U 8 | F 002048 | FPS 0099 | D 16 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.828 | value -0.199 | policy_loss 0.016 | value_loss 0.016 | grad_norm 0.072
U 9 | F 002304 | FPS 0098 | D 18 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.800 | value -0.184 | policy_loss 0.017 | value_loss 0.012 | grad_norm 0.084
U 10 | F 002560 | FPS 0098 | D 21 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.826 | value -0.137 | policy_loss -0.029 | value_loss 0.000 | grad_norm 0.045
U 11 | F 002816 | FPS 0133 | D 23 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.786 | value -0.162 | policy_loss 0.021 | value_loss 0.023 | grad_norm 0.076
U 12 | F 003072 | FPS 0135 | D 25 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.768 | value -0.137 | policy_loss -0.028 | value_loss 0.000 | grad_norm 0.045
U 13 | F 003328 | FPS 0134 | D 26 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.829 | value -0.118 | policy_loss -0.019 | value_loss 0.003 | grad_norm 0.022
U 14 | F 003584 | FPS 0134 | D 28 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.855 | value -0.154 | policy_loss 0.032 | value_loss 0.018 | grad_norm 0.097
U 15 | F 003840 | FPS 0134 | D 30 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.828 | value -0.205 | policy_loss 0.017 | value_loss 0.021 | grad_norm 0.065
U 16 | F 004096 | FPS 0134 | D 32 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.847 | value -0.131 | policy_loss -0.030 | value_loss 0.000 | grad_norm 0.035
U 17 | F 004352 | FPS 0135 | D 34 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.838 | value -0.108 | policy_loss -0.024 | value_loss 0.000 | grad_norm 0.027
U 18 | F 004608 | FPS 0121 | D 36 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.829 | value -0.081 | policy_loss -0.085 | value_loss 0.031 | grad_norm 0.164
U 19 | F 004864 | FPS 0106 | D 39 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.807 | value 0.003 | policy_loss -0.100 | value_loss 0.018 | grad_norm 0.184
U 20 | F 005120 | FPS 0098 | D 41 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.781 | value 0.037 | policy_loss 0.004 | value_loss 0.014 | grad_norm 0.062
Test 10 turns results: Start from 3, reward per episode: OrderedDict({'mean': -0.1, 'std': 0.30000000000000004, 'min': -1.0, 'max': 0.0})
Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': -0.3, 'std': 0.45825756949558405, 'min': -1.0, 'max': 0.0})
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0131 | D 1 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.740 | value 0.056 | policy_loss -0.063 | value_loss 0.012 | grad_norm 0.178
U 2 | F 000512 | FPS 0133 | D 3 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.693 | value -0.134 | policy_loss -0.078 | value_loss 0.006 | grad_norm 0.145
U 3 | F 000768 | FPS 0130 | D 5 | Reward:μσmM 0.33 0.94 -1.00 1.00 |  entropy 1.631 | value 0.199 | policy_loss -0.077 | value_loss 0.014 | grad_norm 0.174
U 4 | F 001024 | FPS 0133 | D 7 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.584 | value -0.332 | policy_loss -0.118 | value_loss 0.004 | grad_norm 0.200
U 5 | F 001280 | FPS 0132 | D 9 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.522 | value -0.263 | policy_loss -0.098 | value_loss 0.005 | grad_norm 0.190
U 6 | F 001536 | FPS 0132 | D 11 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.432 | value -0.120 | policy_loss -0.066 | value_loss 0.004 | grad_norm 0.131
U 7 | F 001792 | FPS 0131 | D 13 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.604 | value 0.148 | policy_loss 0.072 | value_loss 0.063 | grad_norm 0.103
U 8 | F 002048 | FPS 0119 | D 15 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.560 | value -0.107 | policy_loss -0.051 | value_loss 0.003 | grad_norm 0.093
U 9 | F 002304 | FPS 0131 | D 17 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.656 | value 0.139 | policy_loss 0.069 | value_loss 0.071 | grad_norm 0.082
U 10 | F 002560 | FPS 0132 | D 19 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.645 | value 0.092 | policy_loss 0.061 | value_loss 0.087 | grad_norm 0.048
U 11 | F 002816 | FPS 0134 | D 21 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.759 | value 0.334 | policy_loss 0.302 | value_loss 0.178 | grad_norm 0.287
U 12 | F 003072 | FPS 0132 | D 23 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.720 | value 0.166 | policy_loss -0.035 | value_loss 0.003 | grad_norm 0.060
U 13 | F 003328 | FPS 0132 | D 25 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.817 | value 0.176 | policy_loss -0.025 | value_loss 0.000 | grad_norm 0.031
U 14 | F 003584 | FPS 0130 | D 27 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.780 | value 0.258 | policy_loss -0.010 | value_loss 0.001 | grad_norm 0.031
U 15 | F 003840 | FPS 0131 | D 29 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.767 | value 0.279 | policy_loss 0.068 | value_loss 0.035 | grad_norm 0.090
U 16 | F 004096 | FPS 0129 | D 31 | Reward:μσmM 0.33 0.94 -1.00 1.00 |  entropy 1.741 | value 0.398 | policy_loss -0.009 | value_loss 0.027 | grad_norm 0.132
U 17 | F 004352 | FPS 0119 | D 33 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.734 | value 0.355 | policy_loss 0.064 | value_loss 0.044 | grad_norm 0.115
U 18 | F 004608 | FPS 0123 | D 35 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.714 | value 0.234 | policy_loss 0.053 | value_loss 0.028 | grad_norm 0.102
U 19 | F 004864 | FPS 0099 | D 38 | Reward:μσmM 0.33 0.94 -1.00 1.00 |  entropy 1.787 | value -0.066 | policy_loss -0.024 | value_loss 0.017 | grad_norm 0.051
U 20 | F 005120 | FPS 0097 | D 40 | Reward:μσmM 0.33 0.94 -1.00 1.00 |  entropy 1.718 | value -0.067 | policy_loss -0.033 | value_loss 0.036 | grad_norm 0.279
Test 10 turns results: Start from 3, reward per episode: OrderedDict({'mean': -0.1, 'std': 0.30000000000000004, 'min': -1.0, 'max': 0.0})
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0130 | D 1 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.760 | value -0.144 | policy_loss -0.065 | value_loss 0.004 | grad_norm 0.069
U 2 | F 000512 | FPS 0131 | D 3 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.773 | value -0.038 | policy_loss -0.042 | value_loss 0.053 | grad_norm 0.136
U 3 | F 000768 | FPS 0132 | D 5 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.705 | value 0.201 | policy_loss -0.009 | value_loss 0.009 | grad_norm 0.137
U 4 | F 001024 | FPS 0132 | D 7 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.745 | value -0.127 | policy_loss -0.061 | value_loss 0.004 | grad_norm 0.076
U 5 | F 001280 | FPS 0132 | D 9 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.728 | value -0.088 | policy_loss -0.056 | value_loss 0.002 | grad_norm 0.049
U 6 | F 001536 | FPS 0131 | D 11 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.727 | value -0.012 | policy_loss -0.027 | value_loss 0.002 | grad_norm 0.045
U 7 | F 001792 | FPS 0126 | D 13 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.680 | value 0.010 | policy_loss -0.026 | value_loss 0.003 | grad_norm 0.043
U 8 | F 002048 | FPS 0128 | D 15 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.745 | value -0.039 | policy_loss -0.020 | value_loss 0.003 | grad_norm 0.037
U 9 | F 002304 | FPS 0130 | D 17 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.782 | value 0.032 | policy_loss -0.015 | value_loss 0.003 | grad_norm 0.036
U 10 | F 002560 | FPS 0132 | D 19 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.861 | value 0.134 | policy_loss 0.004 | value_loss 0.004 | grad_norm 0.020
U 11 | F 002816 | FPS 0129 | D 21 | Reward:μσmM 0.33 0.94 -1.00 1.00 |  entropy 1.829 | value 0.313 | policy_loss 0.058 | value_loss 0.020 | grad_norm 0.044
U 12 | F 003072 | FPS 0130 | D 23 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.834 | value 0.202 | policy_loss 0.047 | value_loss 0.014 | grad_norm 0.109
U 13 | F 003328 | FPS 0128 | D 25 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.788 | value 0.375 | policy_loss 0.053 | value_loss 0.022 | grad_norm 0.093
U 14 | F 003584 | FPS 0130 | D 27 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.805 | value 0.018 | policy_loss 0.012 | value_loss 0.014 | grad_norm 0.080
U 15 | F 003840 | FPS 0129 | D 29 | Reward:μσmM -0.50 0.87 -1.00 1.00 |  entropy 1.807 | value 0.223 | policy_loss 0.184 | value_loss 0.136 | grad_norm 0.085
U 16 | F 004096 | FPS 0130 | D 31 | Reward:μσmM -0.60 0.80 -1.00 1.00 |  entropy 1.791 | value 0.056 | policy_loss 0.171 | value_loss 0.140 | grad_norm 0.108
U 17 | F 004352 | FPS 0129 | D 33 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.723 | value 0.188 | policy_loss 0.045 | value_loss 0.065 | grad_norm 0.093
U 18 | F 004608 | FPS 0132 | D 35 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.750 | value 0.215 | policy_loss -0.023 | value_loss 0.002 | grad_norm 0.069
U 19 | F 004864 | FPS 0131 | D 37 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.771 | value 0.054 | policy_loss -0.056 | value_loss 0.002 | grad_norm 0.095
U 20 | F 005120 | FPS 0129 | D 39 | Reward:μσmM 0.33 0.94 -1.00 1.00 |  entropy 1.680 | value 0.407 | policy_loss 0.007 | value_loss 0.018 | grad_norm 0.127
Test 10 turns results: Start from 3, reward per episode: OrderedDict({'mean': -0.2, 'std': 0.4000000000000001, 'min': -1.0, 'max': 0.0})
Test 10 turns results: Start from 3, reward per episode: OrderedDict({'mean': -0.1, 'std': 0.30000000000000004, 'min': -1.0, 'max': 0.0})
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0131 | D 1 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.756 | value -0.013 | policy_loss -0.062 | value_loss 0.002 | grad_norm 0.097
U 2 | F 000512 | FPS 0131 | D 3 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.774 | value -0.028 | policy_loss -0.047 | value_loss 0.003 | grad_norm 0.051
U 3 | F 000768 | FPS 0132 | D 5 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.740 | value 0.032 | policy_loss -0.030 | value_loss 0.001 | grad_norm 0.053
U 4 | F 001024 | FPS 0131 | D 7 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.784 | value 0.032 | policy_loss -0.022 | value_loss 0.003 | grad_norm 0.031
U 5 | F 001280 | FPS 0113 | D 10 | Reward:μσmM 0.89 0.15 0.68 1.00 |  entropy 1.723 | value 0.441 | policy_loss -0.050 | value_loss 0.013 | grad_norm 0.072
U 6 | F 001536 | FPS 0115 | D 12 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.737 | value 0.484 | policy_loss 0.094 | value_loss 0.023 | grad_norm 0.091
U 7 | F 001792 | FPS 0130 | D 14 | Reward:μσmM 0.77 0.23 0.53 1.00 |  entropy 1.760 | value 0.351 | policy_loss -0.049 | value_loss 0.004 | grad_norm 0.078
U 8 | F 002048 | FPS 0130 | D 16 | Reward:μσmM -0.24 0.94 -1.00 1.00 |  entropy 1.772 | value 0.407 | policy_loss 0.219 | value_loss 0.217 | grad_norm 0.253
U 9 | F 002304 | FPS 0134 | D 18 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.855 | value 0.331 | policy_loss 0.136 | value_loss 0.057 | grad_norm 0.108
U 10 | F 002560 | FPS 0134 | D 20 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.836 | value 0.242 | policy_loss 0.126 | value_loss 0.045 | grad_norm 0.116
U 11 | F 002816 | FPS 0134 | D 21 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.801 | value 0.126 | policy_loss 0.098 | value_loss 0.036 | grad_norm 0.072
U 12 | F 003072 | FPS 0130 | D 23 | Reward:μσmM -0.33 0.94 -1.00 1.00 |  entropy 1.781 | value 0.116 | policy_loss 0.057 | value_loss 0.069 | grad_norm 0.147
U 13 | F 003328 | FPS 0131 | D 25 | Reward:μσmM 0.80 0.20 0.60 1.00 |  entropy 1.763 | value 0.175 | policy_loss -0.103 | value_loss 0.007 | grad_norm 0.093
U 14 | F 003584 | FPS 0130 | D 27 | Reward:μσmM -0.33 0.94 -1.00 1.00 |  entropy 1.728 | value 0.044 | policy_loss 0.065 | value_loss 0.052 | grad_norm 0.133
U 15 | F 003840 | FPS 0130 | D 29 | Reward:μσmM 0.91 0.09 0.81 1.00 |  entropy 1.586 | value 0.465 | policy_loss -0.123 | value_loss 0.027 | grad_norm 0.143
U 16 | F 004096 | FPS 0097 | D 32 | Reward:μσmM 0.87 0.13 0.71 1.00 |  entropy 1.655 | value 0.493 | policy_loss -0.128 | value_loss 0.010 | grad_norm 0.147
U 17 | F 004352 | FPS 0121 | D 34 | Reward:μσmM 0.92 0.10 0.72 1.00 |  entropy 1.401 | value 0.611 | policy_loss -0.124 | value_loss 0.009 | grad_norm 0.209
U 18 | F 004608 | FPS 0126 | D 36 | Reward:μσmM 0.91 0.09 0.79 1.00 |  entropy 1.320 | value 0.729 | policy_loss -0.070 | value_loss 0.002 | grad_norm 0.097
U 19 | F 004864 | FPS 0122 | D 38 | Reward:μσmM 0.95 0.05 0.87 1.00 |  entropy 1.235 | value 0.798 | policy_loss -0.056 | value_loss 0.002 | grad_norm 0.108
U 20 | F 005120 | FPS 0122 | D 40 | Reward:μσmM 0.76 0.59 -1.00 1.00 |  entropy 1.340 | value 0.769 | policy_loss 0.015 | value_loss 0.072 | grad_norm 0.180
Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': -0.5, 'std': 0.5, 'min': -1.0, 'max': 0.0})
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0126 | D 2 | Reward:μσmM 0.88 0.17 0.64 1.00 |  entropy 1.601 | value 0.475 | policy_loss 0.022 | value_loss 0.003 | grad_norm 0.122
U 2 | F 000512 | FPS 0124 | D 4 | Reward:μσmM 0.93 0.07 0.81 1.00 |  entropy 1.496 | value 0.780 | policy_loss -0.038 | value_loss 0.004 | grad_norm 0.104
U 3 | F 000768 | FPS 0124 | D 6 | Reward:μσmM 0.54 0.78 -1.00 1.00 |  entropy 1.529 | value 0.733 | policy_loss 0.078 | value_loss 0.132 | grad_norm 0.436
U 4 | F 001024 | FPS 0130 | D 8 | Reward:μσmM -0.05 0.95 -1.00 1.00 |  entropy 1.724 | value 0.538 | policy_loss 0.215 | value_loss 0.150 | grad_norm 0.166
U 5 | F 001280 | FPS 0126 | D 10 | Reward:μσmM 0.91 0.12 0.75 1.00 |  entropy 1.656 | value 0.629 | policy_loss -0.040 | value_loss 0.002 | grad_norm 0.102
U 6 | F 001536 | FPS 0103 | D 12 | Reward:μσmM 0.28 0.91 -1.00 1.00 |  entropy 1.594 | value 0.613 | policy_loss 0.040 | value_loss 0.127 | grad_norm 0.133
U 7 | F 001792 | FPS 0098 | D 15 | Reward:μσmM 0.25 0.89 -1.00 1.00 |  entropy 1.755 | value 0.564 | policy_loss 0.054 | value_loss 0.096 | grad_norm 0.143
U 8 | F 002048 | FPS 0124 | D 17 | Reward:μσmM 0.33 0.94 -1.00 1.00 |  entropy 1.747 | value 0.545 | policy_loss 0.018 | value_loss 0.044 | grad_norm 0.092
U 9 | F 002304 | FPS 0088 | D 20 | Reward:μσmM 0.56 0.78 -1.00 1.00 |  entropy 1.641 | value 0.588 | policy_loss -0.004 | value_loss 0.055 | grad_norm 0.106
U 10 | F 002560 | FPS 0092 | D 22 | Reward:μσmM 0.61 0.72 -1.00 1.00 |  entropy 1.606 | value 0.621 | policy_loss -0.004 | value_loss 0.065 | grad_norm 0.170
U 11 | F 002816 | FPS 0097 | D 25 | Reward:μσmM 0.16 0.95 -1.00 1.00 |  entropy 1.658 | value 0.444 | policy_loss 0.136 | value_loss 0.120 | grad_norm 0.157
U 12 | F 003072 | FPS 0096 | D 28 | Reward:μσmM 0.50 0.76 -1.00 1.00 |  entropy 1.652 | value 0.558 | policy_loss -0.012 | value_loss 0.037 | grad_norm 0.108
U 13 | F 003328 | FPS 0093 | D 31 | Reward:μσmM 0.30 0.92 -1.00 1.00 |  entropy 1.562 | value 0.551 | policy_loss 0.086 | value_loss 0.087 | grad_norm 0.122
U 14 | F 003584 | FPS 0094 | D 33 | Reward:μσmM 0.90 0.12 0.73 1.00 |  entropy 1.571 | value 0.614 | policy_loss -0.096 | value_loss 0.005 | grad_norm 0.114
U 15 | F 003840 | FPS 0091 | D 36 | Reward:μσmM 0.56 0.78 -1.00 1.00 |  entropy 1.556 | value 0.729 | policy_loss 0.031 | value_loss 0.111 | grad_norm 0.229
U 16 | F 004096 | FPS 0081 | D 39 | Reward:μσmM 0.94 0.07 0.84 1.00 |  entropy 1.539 | value 0.794 | policy_loss -0.080 | value_loss 0.002 | grad_norm 0.120
U 17 | F 004352 | FPS 0084 | D 42 | Reward:μσmM 0.94 0.06 0.82 1.00 |  entropy 1.491 | value 0.811 | policy_loss -0.048 | value_loss 0.002 | grad_norm 0.069
U 18 | F 004608 | FPS 0087 | D 45 | Reward:μσmM 0.94 0.06 0.85 1.00 |  entropy 1.492 | value 0.831 | policy_loss -0.020 | value_loss 0.001 | grad_norm 0.058
U 19 | F 004864 | FPS 0084 | D 48 | Reward:μσmM 0.95 0.06 0.82 1.00 |  entropy 1.490 | value 0.850 | policy_loss -0.003 | value_loss 0.002 | grad_norm 0.060
U 20 | F 005120 | FPS 0083 | D 51 | Reward:μσmM 0.72 0.61 -1.00 1.00 |  entropy 1.620 | value 0.806 | policy_loss 0.021 | value_loss 0.012 | grad_norm 0.061
U 191 | F 048896 | FPS 0077 | D 3 | Reward:μσmM 0.52 1.28 -1.00 1.81 | policy_loss ['None', 'None', 'None', '0.395', '-0.154'] | value_loss ['None', 'None', 'None', '0.143', '0.521']
U 192 | F 049152 | FPS 0116 | D 5 | Reward:μσmM 1.00 0.00 1.00 1.00 | policy_loss ['None', 'None', 'None', '0.170', '-0.304'] | value_loss ['None', 'None', 'None', '0.004', '0.052']
U 193 | F 049408 | FPS 0116 | D 7 | Reward:μσmM 0.50 0.50 0.00 1.00 | policy_loss ['None', 'None', 'None', '0.285', '-0.282'] | value_loss ['None', 'None', 'None', '0.170', '0.035']
U 194 | F 049664 | FPS 0113 | D 10 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', 'None', '0.751', '-0.075'] | value_loss ['None', 'None', 'None', '0.640', '0.012']
U 195 | F 049920 | FPS 0116 | D 12 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', 'None', '0.435', '0.035'] | value_loss ['None', 'None', 'None', '0.226', '0.004']
U 196 | F 050176 | FPS 0117 | D 14 | Reward:μσmM -0.20 0.75 -1.00 1.00 | policy_loss ['None', 'None', 'None', '0.479', '0.403'] | value_loss ['None', 'None', 'None', '0.260', '0.389']
U 197 | F 050432 | FPS 0117 | D 16 | Reward:μσmM -0.25 0.43 -1.00 0.00 | policy_loss ['None', 'None', 'None', '0.553', '0.185'] | value_loss ['None', 'None', 'None', '0.268', '0.068']
U 198 | F 050688 | FPS 0119 | D 18 | Reward:μσmM -0.33 0.94 -1.00 1.00 | policy_loss ['None', 'None', 'None', '-0.176', '0.299'] | value_loss ['None', 'None', 'None', '0.001', '0.177']
U 199 | F 050944 | FPS 0086 | D 21 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', 'None', 'None', '0.092'] | value_loss ['None', 'None', 'None', 'None', '0.000']
U 200 | F 051200 | FPS 0088 | D 24 | Reward:μσmM -0.80 0.40 -1.00 0.00 | policy_loss ['None', 'None', 'None', 'None', '0.360'] | value_loss ['None', 'None', 'None', 'None', '0.201']
U 10 | Test reward:μσmM -0.80 0.40 -1.00 0.00 | Test num frames:μσmM 109.60 85.28 17.00 256.00
Status saved
U 201 | F 051456 | FPS 0122 | D 50 | Reward:μσmM -0.75 0.43 -1.00 0.00 | policy_loss ['None', 'None', 'None', 'None', '0.180'] | value_loss ['None', 'None', 'None', 'None', '0.072']
U 202 | F 051712 | FPS 0122 | D 52 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', 'None', 'None', '0.015'] | value_loss ['None', 'None', 'None', 'None', '0.001']
U 203 | F 051968 | FPS 0123 | D 54 | Reward:μσmM -0.75 0.43 -1.00 0.00 | policy_loss ['None', 'None', 'None', 'None', '0.190'] | value_loss ['None', 'None', 'None', 'None', '0.080']
U 204 | F 052224 | FPS 0122 | D 56 | Reward:μσmM -0.67 0.47 -1.00 0.00 | policy_loss ['None', 'None', 'None', 'None', '0.111'] | value_loss ['None', 'None', 'None', 'None', '0.045']
U 205 | F 052480 | FPS 0119 | D 58 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', 'None', '0.068', '-0.112'] | value_loss ['None', 'None', 'None', '0.017', '0.017']
U 206 | F 052736 | FPS 0123 | D 60 | Reward:μσmM -0.89 0.31 -1.00 0.00 | policy_loss ['None', 'None', 'None', 'None', '0.292'] | value_loss ['None', 'None', 'None', 'None', '0.120']
U 207 | F 052992 | FPS 0119 | D 63 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', 'None', 'None', '-0.038'] | value_loss ['None', 'None', 'None', 'None', '0.001']
U 208 | F 053248 | FPS 0118 | D 65 | Reward:μσmM 1.00 0.00 1.00 1.00 | policy_loss ['None', 'None', 'None', '-0.025', '-0.076'] | value_loss ['None', 'None', 'None', '0.000', '0.010']
U 209 | F 053504 | FPS 0120 | D 67 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', 'None', '0.042', '0.005'] | value_loss ['None', 'None', 'None', '0.018', '0.030']
U 210 | F 053760 | FPS 0119 | D 69 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', 'None', '0.050', '-0.092'] | value_loss ['None', 'None', 'None', '0.016', '0.005']
U 10 | Test reward:μσmM 0.02 0.72 -1.00 0.87 | Test num frames:μσmM 153.30 81.70 15.00 256.00
Status saved
U 211 | F 054016 | FPS 0120 | D 103 | Reward:μσmM -0.33 0.47 -1.00 0.00 | policy_loss ['None', 'None', 'None', '0.057', '0.016'] | value_loss ['None', 'None', 'None', '0.013', '0.042']
U 212 | F 054272 | FPS 0117 | D 105 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', 'None', '-0.040', '-0.091'] | value_loss ['None', 'None', 'None', '0.005', '0.015']
U 213 | F 054528 | FPS 0120 | D 107 | Reward:μσmM 1.00 0.00 1.00 1.00 | policy_loss ['None', 'None', 'None', '-0.074', '-0.121'] | value_loss ['None', 'None', 'None', '0.001', '0.003']
U 214 | F 054784 | FPS 0118 | D 110 | Reward:μσmM 1.00 0.00 1.00 1.00 | policy_loss ['None', 'None', 'None', '-0.060', '-0.018'] | value_loss ['None', 'None', 'None', '0.000', '0.003']
U 215 | F 055040 | FPS 0102 | D 112 | Reward:μσmM 1.00 0.00 1.00 1.00 | policy_loss ['None', 'None', 'None', '-0.051', '-0.047'] | value_loss ['None', 'None', 'None', '0.000', '0.002']
U 216 | F 055296 | FPS 0071 | D 116 | Reward:μσmM 0.00 1.00 -1.00 1.00 | policy_loss ['None', 'None', 'None', '-0.041', '0.237'] | value_loss ['None', 'None', 'None', '0.000', '0.227']
U 217 | F 055552 | FPS 0088 | D 119 | Reward:μσmM 1.00 0.00 1.00 1.00 | policy_loss ['None', 'None', 'None', '-0.036', '-0.130'] | value_loss ['None', 'None', 'None', '0.000', '0.009']
U 218 | F 055808 | FPS 0069 | D 122 | Reward:μσmM 1.00 0.00 1.00 1.00 | policy_loss ['None', 'None', 'None', '-0.029', '-0.178'] | value_loss ['None', 'None', 'None', '0.000', '0.004']
U 219 | F 056064 | FPS 0081 | D 126 | Reward:μσmM 1.29 0.29 1.00 1.58 | policy_loss ['None', 'None', 'None', '-0.092', '-0.187'] | value_loss ['None', 'None', 'None', '0.012', '0.007']
U 220 | F 056320 | FPS 0080 | D 129 | Reward:μσmM 1.43 0.43 1.00 1.86 | policy_loss ['None', 'None', 'None', '-0.104', '-0.011'] | value_loss ['None', 'None', 'None', '0.016', '0.007']
U 10 | Test reward:μσmM 0.58 0.53 -1.00 0.91 | Test num frames:μσmM 101.10 34.93 36.00 164.00
Status saved
U 221 | F 056576 | FPS 0082 | D 160 | Reward:μσmM 1.49 0.35 1.00 1.74 | policy_loss ['None', 'None', 'None', '-0.148', '0.003'] | value_loss ['None', 'None', 'None', '0.008', '0.001']
U 222 | F 056832 | FPS 0078 | D 164 | Reward:μσmM 1.37 0.79 0.00 1.88 | policy_loss ['None', 'None', 'None', '-0.265', '-0.067'] | value_loss ['None', 'None', 'None', '0.016', '0.013']
U 223 | F 057088 | FPS 0072 | D 167 | Reward:μσmM 1.64 0.38 1.00 1.91 | policy_loss ['None', 'None', 'None', '-0.117', '-0.182'] | value_loss ['None', 'None', 'None', '0.010', '0.024']
U 224 | F 057344 | FPS 0078 | D 170 | Reward:μσmM 1.42 0.71 0.00 1.91 | policy_loss ['None', 'None', 'None', '-0.081', '-0.121'] | value_loss ['None', 'None', 'None', '0.092', '0.055']
U 225 | F 057600 | FPS 0078 | D 174 | Reward:μσmM 1.49 0.75 0.00 1.91 | policy_loss ['None', 'None', 'None', '-0.013', '-0.158'] | value_loss ['None', 'None', 'None', '0.005', '0.009']
U 226 | F 057856 | FPS 0079 | D 177 | Reward:μσmM 1.79 0.32 1.00 1.93 | policy_loss ['None', 'None', 'None', '-0.074', '-0.193'] | value_loss ['None', 'None', 'None', '0.003', '0.003']
U 227 | F 058112 | FPS 0084 | D 180 | Reward:μσmM 1.81 0.31 1.00 1.94 | policy_loss ['None', 'None', 'None', '-0.074', '-0.100'] | value_loss ['None', 'None', 'None', '0.002', '0.001']
U 228 | F 058368 | FPS 0105 | D 182 | Reward:μσmM 1.85 0.28 1.00 1.95 | policy_loss ['None', 'None', 'None', '-0.060', '-0.075'] | value_loss ['None', 'None', 'None', '0.001', '0.003']
U 229 | F 058624 | FPS 0108 | D 185 | Reward:μσmM 1.85 0.28 1.00 1.95 | policy_loss ['None', 'None', 'None', '-0.020', '-0.043'] | value_loss ['None', 'None', 'None', '0.001', '0.001']
U 230 | F 058880 | FPS 0101 | D 187 | Reward:μσmM 1.77 0.56 0.00 1.96 | policy_loss ['None', 'None', 'None', '-0.005', '-0.031'] | value_loss ['None', 'None', 'None', '0.000', '0.000']
U 10 | Test reward:μσmM 0.60 0.54 -1.00 0.89 | Test num frames:μσmM 102.30 35.14 46.00 159.00
Status saved
U 231 | F 059136 | FPS 0101 | D 213 | Reward:μσmM 1.74 0.58 0.00 1.95 | policy_loss ['None', 'None', 'None', '0.007', '-0.002'] | value_loss ['None', 'None', 'None', '0.001', '0.000']
U 232 | F 059392 | FPS 0111 | D 215 | Reward:μσmM 1.80 0.30 1.00 1.95 | policy_loss ['None', 'None', 'None', '-0.024', '0.050'] | value_loss ['None', 'None', 'None', '0.009', '0.005']
U 233 | F 059648 | FPS 0115 | D 217 | Reward:μσmM 1.10 0.90 0.00 1.90 | policy_loss ['None', 'None', 'None', '0.150', '0.175'] | value_loss ['None', 'None', 'None', '0.151', '0.046']
U 234 | F 059904 | FPS 0099 | D 220 | Reward:μσmM 0.55 1.14 -1.00 1.87 | policy_loss ['None', 'None', 'None', '0.236', '0.153'] | value_loss ['None', 'None', 'None', '0.296', '0.040']
U 235 | F 060160 | FPS 0088 | D 223 | Reward:μσmM -0.80 0.40 -1.00 0.00 | policy_loss ['None', 'None', 'None', 'None', '0.613'] | value_loss ['None', 'None', 'None', 'None', '0.605']
U 236 | F 060416 | FPS 0080 | D 226 | Reward:μσmM -0.24 1.09 -1.00 1.78 | policy_loss ['None', 'None', 'None', '-0.125', '0.244'] | value_loss ['None', 'None', 'None', '0.002', '0.201']
U 237 | F 060672 | FPS 0108 | D 228 | Reward:μσmM 0.84 0.84 0.00 1.68 | policy_loss ['None', 'None', 'None', '0.068', '0.013'] | value_loss ['None', 'None', 'None', '0.000', '0.007']
U 238 | F 060928 | FPS 0110 | D 231 | Reward:μσmM -0.83 0.37 -1.00 0.00 | policy_loss ['None', 'None', 'None', 'None', '0.383'] | value_loss ['None', 'None', 'None', 'None', '0.287']
U 239 | F 061184 | FPS 0094 | D 233 | Reward:μσmM -0.25 0.83 -1.00 1.00 | policy_loss ['None', 'None', 'None', '0.565', '0.043'] | value_loss ['None', 'None', 'None', '0.527', '0.103']
U 240 | F 061440 | FPS 0108 | D 236 | Reward:μσmM -0.04 1.20 -1.00 1.79 | policy_loss ['None', 'None', 'None', '-0.026', '0.122'] | value_loss ['None', 'None', 'None', '0.004', '0.256']
U 10 | Test reward:μσmM -1.00 0.00 -1.00 -1.00 | Test num frames:μσmM 58.70 39.96 5.00 123.00
Status saved
U 241 | F 061696 | FPS 0093 | D 253 | Reward:μσmM -0.33 0.47 -1.00 0.00 | policy_loss ['None', 'None', 'None', '0.376', '-0.128'] | value_loss ['None', 'None', 'None', '0.209', '0.035']
U 242 | F 061952 | FPS 0092 | D 256 | Reward:μσmM 1.37 0.37 1.00 1.74 | policy_loss ['None', 'None', 'None', '-0.053', '-0.261'] | value_loss ['None', 'None', 'None', '0.002', '0.033']
U 243 | F 062208 | FPS 0102 | D 258 | Reward:μσmM 1.39 0.39 1.00 1.78 | policy_loss ['None', 'None', 'None', '-0.157', '-0.048'] | value_loss ['None', 'None', 'None', '0.009', '0.013']
U 244 | F 062464 | FPS 0102 | D 261 | Reward:μσmM 1.30 0.30 1.00 1.61 | policy_loss ['None', 'None', 'None', '0.010', '-0.131'] | value_loss ['None', 'None', 'None', '0.001', '0.004']
U 245 | F 062720 | FPS 0111 | D 263 | Reward:μσmM 1.16 0.82 0.00 1.82 | policy_loss ['None', 'None', 'None', '-0.054', '-0.071'] | value_loss ['None', 'None', 'None', '0.003', '0.009']
U 246 | F 062976 | FPS 0114 | D 265 | Reward:μσmM 1.17 0.75 0.00 1.83 | policy_loss ['None', 'None', 'None', '-0.004', '-0.037'] | value_loss ['None', 'None', 'None', '0.084', '0.018']
U 247 | F 063232 | FPS 0113 | D 268 | Reward:μσmM 0.59 1.30 -1.00 1.90 | policy_loss ['None', 'None', 'None', '-0.096', '0.115'] | value_loss ['None', 'None', 'None', '0.011', '0.342']
U 248 | F 063488 | FPS 0115 | D 270 | Reward:μσmM 1.11 0.91 0.00 1.92 | policy_loss ['None', 'None', 'None', '0.056', '-0.042'] | value_loss ['None', 'None', 'None', '0.075', '0.051']
U 249 | F 063744 | FPS 0086 | D 273 | Reward:μσmM 0.89 1.13 -1.00 1.78 | policy_loss ['None', 'None', 'None', '0.028', '-0.045'] | value_loss ['None', 'None', 'None', '0.003', '0.152']
U 250 | F 064000 | FPS 0104 | D 275 | Reward:μσmM 1.58 0.70 0.00 1.92 | policy_loss ['None', 'None', 'None', '-0.112', '-0.182'] | value_loss ['None', 'None', 'None', '0.009', '0.013']
discover.py --task-config task3 --discover 0 --algo ppo --env MiniGrid-ConfigWorld-v0 --lr 0.0001 --AnomalyNN test_24 --model 20240725-seed1 --discount 0.99

Namespace(task_config='task3', discover=0, algo='ppo', env='MiniGrid-ConfigWorld-v0', model='20240725-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_24', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.0001, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Optimizer loaded

Optimizer loaded

discover.py --task-config task3 --discover 0 --algo ppo --env MiniGrid-ConfigWorld-v0 --lr 0.0001 --AnomalyNN test_24 --model 20240725-seed1 --discount 0.99

Namespace(task_config='task3', discover=0, algo='ppo', env='MiniGrid-ConfigWorld-v0', model='20240725-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_24', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.0001, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Optimizer loaded

Optimizer loaded

U 241 | F 061696 | FPS 0090 | D 2 | Reward:μσmM -0.62 0.70 -1.00 1.00 | policy_loss ['None', 'None', 'None', '0.319', '0.132'] | value_loss ['None', 'None', 'None', '0.270', '0.147']
U 242 | F 061952 | FPS 0105 | D 5 | Reward:μσmM -0.12 1.03 -1.00 1.52 | policy_loss ['None', 'None', 'None', '-0.036', '-0.112'] | value_loss ['None', 'None', 'None', '0.002', '0.075']
U 243 | F 062208 | FPS 0115 | D 7 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', 'None', '0.234', '0.132'] | value_loss ['None', 'None', 'None', '0.078', '0.192']
U 244 | F 062464 | FPS 0117 | D 9 | Reward:μσmM 1.00 0.00 1.00 1.00 | policy_loss ['None', 'None', 'None', '-0.006', '-0.015'] | value_loss ['None', 'None', 'None', '0.000', '0.011']
discover.py --task-config task2 --discover 1 --algo ppo --env MiniGrid-ConfigWorld-v0 --lr 0.0001 --AnomalyNN test_24 --model 20240725-seed1 --discount 0.99

Namespace(task_config='task2', discover=1, algo='ppo', env='MiniGrid-ConfigWorld-v0', model='20240725-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_24', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.0001, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Optimizer loaded

discover.py --task-config task2 --discover 1 --algo ppo --env MiniGrid-ConfigWorld-v0 --lr 0.0001 --AnomalyNN test_24 --model 20240725-seed1 --discount 0.99

Namespace(task_config='task2', discover=1, algo='ppo', env='MiniGrid-ConfigWorld-v0', model='20240725-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_24', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.0001, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Optimizer loaded

Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': -0.5, 'std': 0.5, 'min': -1.0, 'max': 0.0})
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0123 | D 2 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.822 | value 0.398 | policy_loss 0.379 | value_loss 0.271 | grad_norm 2.357
U 2 | F 000512 | FPS 0098 | D 4 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.780 | value 0.228 | policy_loss 0.146 | value_loss 0.072 | grad_norm 0.345
U 3 | F 000768 | FPS 0135 | D 6 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.660 | value -0.056 | policy_loss 0.247 | value_loss 0.119 | grad_norm 0.672
U 4 | F 001024 | FPS 0136 | D 8 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.848 | value -0.037 | policy_loss -0.012 | value_loss 0.000 | grad_norm 0.066
U 5 | F 001280 | FPS 0089 | D 11 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.792 | value -0.085 | policy_loss 0.067 | value_loss 0.037 | grad_norm 0.213
U 6 | F 001536 | FPS 0077 | D 14 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.676 | value -0.166 | policy_loss 0.039 | value_loss 0.017 | grad_norm 0.266
U 7 | F 001792 | FPS 0076 | D 18 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.707 | value -0.196 | policy_loss 0.065 | value_loss 0.033 | grad_norm 0.221
U 8 | F 002048 | FPS 0087 | D 21 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.751 | value -0.208 | policy_loss 0.013 | value_loss 0.010 | grad_norm 0.154
U 9 | F 002304 | FPS 0100 | D 23 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.786 | value -0.281 | policy_loss 0.061 | value_loss 0.011 | grad_norm 0.095
U 10 | F 002560 | FPS 0072 | D 27 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.753 | value -0.399 | policy_loss 0.041 | value_loss 0.028 | grad_norm 0.194
U 11 | F 002816 | FPS 0099 | D 29 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.768 | value -0.355 | policy_loss -0.012 | value_loss 0.012 | grad_norm 0.170
U 12 | F 003072 | FPS 0098 | D 32 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.580 | value -0.368 | policy_loss -0.007 | value_loss 0.014 | grad_norm 0.173
U 13 | F 003328 | FPS 0096 | D 34 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.768 | value -0.376 | policy_loss -0.026 | value_loss 0.011 | grad_norm 0.113
U 14 | F 003584 | FPS 0074 | D 38 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.740 | value -0.336 | policy_loss -0.068 | value_loss 0.001 | grad_norm 0.175
U 15 | F 003840 | FPS 0098 | D 41 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.642 | value -0.302 | policy_loss -0.060 | value_loss 0.000 | grad_norm 0.147
U 16 | F 004096 | FPS 0075 | D 44 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.648 | value -0.253 | policy_loss -0.050 | value_loss 0.000 | grad_norm 0.069
U 17 | F 004352 | FPS 0098 | D 47 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.711 | value -0.255 | policy_loss 0.004 | value_loss 0.013 | grad_norm 0.095
U 18 | F 004608 | FPS 0073 | D 50 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.749 | value -0.199 | policy_loss -0.032 | value_loss 0.000 | grad_norm 0.059
U 19 | F 004864 | FPS 0099 | D 53 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.677 | value -0.190 | policy_loss -0.032 | value_loss 0.000 | grad_norm 0.039
U 20 | F 005120 | FPS 0097 | D 55 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.814 | value -0.176 | policy_loss -0.035 | value_loss 0.000 | grad_norm 0.059
Test 10 turns results: Start from 3, reward per episode: OrderedDict({'mean': -0.7, 'std': 0.45825756949558405, 'min': -1.0, 'max': 0.0})
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0096 | D 2 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.812 | value -0.155 | policy_loss -0.031 | value_loss 0.000 | grad_norm 0.038
U 2 | F 000512 | FPS 0098 | D 5 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.851 | value -0.118 | policy_loss -0.024 | value_loss 0.000 | grad_norm 0.028
U 3 | F 000768 | FPS 0092 | D 8 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.828 | value -0.058 | policy_loss -0.108 | value_loss 0.022 | grad_norm 0.038
U 4 | F 001024 | FPS 0077 | D 11 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.850 | value -0.064 | policy_loss 0.039 | value_loss 0.025 | grad_norm 0.102
U 5 | F 001280 | FPS 0098 | D 14 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.864 | value -0.084 | policy_loss 0.040 | value_loss 0.014 | grad_norm 0.016
U 6 | F 001536 | FPS 0097 | D 16 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.872 | value -0.067 | policy_loss 0.015 | value_loss 0.019 | grad_norm 0.036
U 7 | F 001792 | FPS 0098 | D 19 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.856 | value -0.144 | policy_loss 0.068 | value_loss 0.035 | grad_norm 0.047
U 8 | F 002048 | FPS 0098 | D 21 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.855 | value -0.159 | policy_loss 0.037 | value_loss 0.013 | grad_norm 0.049
U 9 | F 002304 | FPS 0074 | D 25 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.880 | value -0.075 | policy_loss -0.014 | value_loss 0.000 | grad_norm 0.050
U 10 | F 002560 | FPS 0097 | D 28 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.829 | value -0.255 | policy_loss 0.041 | value_loss 0.022 | grad_norm 0.058
U 11 | F 002816 | FPS 0094 | D 30 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.822 | value -0.157 | policy_loss -0.136 | value_loss 0.034 | grad_norm 0.117
U 12 | F 003072 | FPS 0098 | D 33 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.796 | value -0.101 | policy_loss 0.023 | value_loss 0.014 | grad_norm 0.088
U 13 | F 003328 | FPS 0097 | D 35 | Reward:μσmM -0.33 0.94 -1.00 1.00 |  entropy 1.718 | value -0.194 | policy_loss -0.068 | value_loss 0.046 | grad_norm 0.209
U 14 | F 003584 | FPS 0099 | D 38 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.789 | value -0.205 | policy_loss -0.010 | value_loss 0.024 | grad_norm 0.077
U 15 | F 003840 | FPS 0095 | D 41 | Reward:μσmM 0.33 0.94 -1.00 1.00 |  entropy 1.744 | value -0.022 | policy_loss -0.173 | value_loss 0.042 | grad_norm 0.196
U 16 | F 004096 | FPS 0085 | D 44 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.709 | value 0.096 | policy_loss -0.073 | value_loss 0.006 | grad_norm 0.110
U 17 | F 004352 | FPS 0090 | D 47 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.723 | value -0.130 | policy_loss 0.248 | value_loss 0.110 | grad_norm 0.092
U 18 | F 004608 | FPS 0074 | D 50 | Reward:μσmM -0.33 0.94 -1.00 1.00 |  entropy 1.756 | value -0.105 | policy_loss -0.025 | value_loss 0.050 | grad_norm 0.178
U 19 | F 004864 | FPS 0074 | D 54 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.817 | value -0.180 | policy_loss 0.032 | value_loss 0.018 | grad_norm 0.077
U 20 | F 005120 | FPS 0075 | D 57 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.827 | value -0.148 | policy_loss 0.003 | value_loss 0.018 | grad_norm 0.046
Test 10 turns results: Start from 3, reward per episode: OrderedDict({'mean': -0.6, 'std': 0.4898979485566356, 'min': -1.0, 'max': 0.0})
Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': -0.6, 'std': 0.48989794855663565, 'min': -1.0, 'max': 0.0})
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0127 | D 2 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.640 | value -0.052 | policy_loss -0.096 | value_loss 0.015 | grad_norm 0.161
U 2 | F 000512 | FPS 0096 | D 4 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.585 | value -0.176 | policy_loss -0.125 | value_loss 0.005 | grad_norm 0.182
U 3 | F 000768 | FPS 0123 | D 6 | Reward:μσmM 0.33 0.94 -1.00 1.00 |  entropy 1.499 | value -0.057 | policy_loss -0.113 | value_loss 0.010 | grad_norm 0.222
U 4 | F 001024 | FPS 0127 | D 8 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.562 | value -0.206 | policy_loss -0.088 | value_loss 0.004 | grad_norm 0.143
U 5 | F 001280 | FPS 0130 | D 10 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.477 | value -0.109 | policy_loss -0.070 | value_loss 0.003 | grad_norm 0.095
U 6 | F 001536 | FPS 0129 | D 12 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.489 | value -0.162 | policy_loss -0.061 | value_loss 0.004 | grad_norm 0.110
U 7 | F 001792 | FPS 0129 | D 14 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.511 | value -0.094 | policy_loss -0.055 | value_loss 0.002 | grad_norm 0.095
U 8 | F 002048 | FPS 0129 | D 16 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.486 | value -0.042 | policy_loss -0.040 | value_loss 0.003 | grad_norm 0.068
U 9 | F 002304 | FPS 0128 | D 18 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.548 | value -0.025 | policy_loss -0.029 | value_loss 0.003 | grad_norm 0.079
U 10 | F 002560 | FPS 0129 | D 20 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.623 | value -0.031 | policy_loss -0.029 | value_loss 0.003 | grad_norm 0.038
U 11 | F 002816 | FPS 0125 | D 22 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.780 | value -0.038 | policy_loss 0.027 | value_loss 0.019 | grad_norm 0.052
U 12 | F 003072 | FPS 0124 | D 24 | Reward:μσmM -0.20 0.98 -1.00 1.00 |  entropy 1.755 | value 0.042 | policy_loss 0.024 | value_loss 0.055 | grad_norm 0.220
U 13 | F 003328 | FPS 0105 | D 27 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.732 | value 0.526 | policy_loss 0.076 | value_loss 0.013 | grad_norm 0.075
U 14 | F 003584 | FPS 0126 | D 29 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.728 | value 0.444 | policy_loss 0.151 | value_loss 0.076 | grad_norm 0.255
U 15 | F 003840 | FPS 0127 | D 31 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.737 | value 0.150 | policy_loss -0.004 | value_loss 0.008 | grad_norm 0.085
U 16 | F 004096 | FPS 0128 | D 33 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.742 | value 0.080 | policy_loss -0.042 | value_loss 0.004 | grad_norm 0.112
U 17 | F 004352 | FPS 0127 | D 35 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.759 | value -0.194 | policy_loss -0.078 | value_loss 0.004 | grad_norm 0.059
U 18 | F 004608 | FPS 0095 | D 37 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.804 | value -0.055 | policy_loss -0.028 | value_loss 0.016 | grad_norm 0.040
U 19 | F 004864 | FPS 0129 | D 39 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.792 | value 0.344 | policy_loss 0.023 | value_loss 0.006 | grad_norm 0.063
U 20 | F 005120 | FPS 0124 | D 42 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.772 | value 0.389 | policy_loss 0.072 | value_loss 0.051 | grad_norm 0.118
Test 10 turns results: Start from 3, reward per episode: OrderedDict({'mean': -0.8, 'std': 0.4, 'min': -1.0, 'max': 0.0})
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0127 | D 2 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.751 | value 0.220 | policy_loss -0.004 | value_loss 0.004 | grad_norm 0.052
U 2 | F 000512 | FPS 0131 | D 3 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.792 | value 0.349 | policy_loss 0.199 | value_loss 0.097 | grad_norm 0.207
U 3 | F 000768 | FPS 0108 | D 6 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.722 | value -0.047 | policy_loss -0.067 | value_loss 0.003 | grad_norm 0.138
U 4 | F 001024 | FPS 0099 | D 8 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.693 | value 0.359 | policy_loss 0.007 | value_loss 0.003 | grad_norm 0.052
U 5 | F 001280 | FPS 0113 | D 11 | Reward:μσmM -0.33 0.94 -1.00 1.00 |  entropy 1.745 | value -0.008 | policy_loss 0.084 | value_loss 0.116 | grad_norm 0.412
U 6 | F 001536 | FPS 0095 | D 13 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.734 | value -0.109 | policy_loss -0.008 | value_loss 0.022 | grad_norm 0.120
U 7 | F 001792 | FPS 0130 | D 15 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.750 | value -0.041 | policy_loss -0.070 | value_loss 0.006 | grad_norm 0.072
U 8 | F 002048 | FPS 0129 | D 17 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.729 | value -0.111 | policy_loss -0.015 | value_loss 0.025 | grad_norm 0.081
U 9 | F 002304 | FPS 0127 | D 19 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.720 | value -0.037 | policy_loss -0.068 | value_loss 0.007 | grad_norm 0.147
U 10 | F 002560 | FPS 0129 | D 21 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.761 | value -0.005 | policy_loss -0.037 | value_loss 0.003 | grad_norm 0.028
U 11 | F 002816 | FPS 0129 | D 23 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.792 | value -0.016 | policy_loss -0.026 | value_loss 0.003 | grad_norm 0.030
U 12 | F 003072 | FPS 0126 | D 25 | Reward:μσmM 0.33 0.94 -1.00 1.00 |  entropy 1.770 | value 0.089 | policy_loss 0.028 | value_loss 0.020 | grad_norm 0.055
U 13 | F 003328 | FPS 0128 | D 27 | Reward:μσmM -0.33 0.94 -1.00 1.00 |  entropy 1.743 | value 0.247 | policy_loss 0.159 | value_loss 0.189 | grad_norm 0.141
U 14 | F 003584 | FPS 0128 | D 29 | Reward:μσmM -0.33 0.94 -1.00 1.00 |  entropy 1.719 | value 0.461 | policy_loss 0.190 | value_loss 0.110 | grad_norm 0.150
U 15 | F 003840 | FPS 0128 | D 31 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.708 | value 0.235 | policy_loss 0.063 | value_loss 0.047 | grad_norm 0.111
U 16 | F 004096 | FPS 0130 | D 33 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.676 | value 0.297 | policy_loss 0.048 | value_loss 0.039 | grad_norm 0.141
U 17 | F 004352 | FPS 0129 | D 35 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.687 | value 0.107 | policy_loss 0.201 | value_loss 0.096 | grad_norm 0.231
U 18 | F 004608 | FPS 0129 | D 37 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.742 | value 0.018 | policy_loss -0.081 | value_loss 0.006 | grad_norm 0.113
U 19 | F 004864 | FPS 0129 | D 39 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.721 | value 0.170 | policy_loss -0.046 | value_loss 0.003 | grad_norm 0.082
U 20 | F 005120 | FPS 0127 | D 41 | Reward:μσmM -0.75 0.66 -1.00 1.00 |  entropy 1.734 | value 0.018 | policy_loss 0.205 | value_loss 0.134 | grad_norm 0.209
Test 10 turns results: Start from 3, reward per episode: OrderedDict({'mean': -0.6, 'std': 0.4898979485566356, 'min': -1.0, 'max': 0.0})
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0126 | D 2 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.700 | value 0.227 | policy_loss -0.037 | value_loss 0.006 | grad_norm 0.143
U 2 | F 000512 | FPS 0111 | D 4 | Reward:μσmM 0.33 0.94 -1.00 1.00 |  entropy 1.764 | value 0.088 | policy_loss -0.004 | value_loss 0.019 | grad_norm 0.059
U 3 | F 000768 | FPS 0110 | D 6 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.742 | value 0.210 | policy_loss -0.005 | value_loss 0.023 | grad_norm 0.117
U 4 | F 001024 | FPS 0105 | D 9 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.716 | value -0.052 | policy_loss -0.067 | value_loss 0.006 | grad_norm 0.141
U 5 | F 001280 | FPS 0095 | D 11 | Reward:μσmM 0.33 0.94 -1.00 1.00 |  entropy 1.759 | value 0.116 | policy_loss 0.004 | value_loss 0.020 | grad_norm 0.155
U 6 | F 001536 | FPS 0127 | D 13 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.790 | value -0.044 | policy_loss -0.050 | value_loss 0.002 | grad_norm 0.078
U 7 | F 001792 | FPS 0107 | D 16 | Reward:μσmM 0.33 0.94 -1.00 1.00 |  entropy 1.756 | value 0.201 | policy_loss 0.038 | value_loss 0.021 | grad_norm 0.074
U 8 | F 002048 | FPS 0098 | D 18 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.765 | value -0.081 | policy_loss -0.050 | value_loss 0.003 | grad_norm 0.139
U 9 | F 002304 | FPS 0095 | D 21 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.747 | value 0.262 | policy_loss 0.087 | value_loss 0.072 | grad_norm 0.077
U 10 | F 002560 | FPS 0119 | D 23 | Reward:μσmM 0.33 0.94 -1.00 1.00 |  entropy 1.753 | value 0.056 | policy_loss 0.006 | value_loss 0.012 | grad_norm 0.044
U 11 | F 002816 | FPS 0128 | D 25 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.764 | value -0.004 | policy_loss -0.050 | value_loss 0.003 | grad_norm 0.055
U 12 | F 003072 | FPS 0109 | D 28 | Reward:μσmM 0.33 0.94 -1.00 1.00 |  entropy 1.754 | value 0.167 | policy_loss 0.021 | value_loss 0.022 | grad_norm 0.108
U 13 | F 003328 | FPS 0128 | D 30 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.770 | value 0.163 | policy_loss -0.005 | value_loss 0.002 | grad_norm 0.071
U 14 | F 003584 | FPS 0100 | D 32 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.769 | value 0.577 | policy_loss 0.065 | value_loss 0.012 | grad_norm 0.140
U 15 | F 003840 | FPS 0096 | D 35 | Reward:μσmM -0.67 0.75 -1.00 1.00 |  entropy 1.773 | value 0.359 | policy_loss 0.290 | value_loss 0.269 | grad_norm 0.254
U 16 | F 004096 | FPS 0098 | D 37 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.744 | value 0.196 | policy_loss -0.022 | value_loss 0.003 | grad_norm 0.105
U 17 | F 004352 | FPS 0094 | D 40 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.680 | value 0.052 | policy_loss 0.025 | value_loss 0.059 | grad_norm 0.245
U 18 | F 004608 | FPS 0076 | D 43 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.620 | value 0.033 | policy_loss 0.023 | value_loss 0.070 | grad_norm 0.187
U 19 | F 004864 | FPS 0091 | D 46 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.794 | value 0.156 | policy_loss 0.017 | value_loss 0.028 | grad_norm 0.051
U 20 | F 005120 | FPS 0088 | D 49 | Reward:μσmM -0.50 0.87 -1.00 1.00 |  entropy 1.737 | value 0.107 | policy_loss -0.014 | value_loss 0.058 | grad_norm 0.133
Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': -0.3, 'std': 0.45825756949558394, 'min': -1.0, 'max': 0.0})
Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': -0.2, 'std': 0.4, 'min': -1.0, 'max': 0.0})
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0098 | D 2 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.691 | value 0.024 | policy_loss -0.060 | value_loss 0.007 | grad_norm 0.089
U 2 | F 000512 | FPS 0081 | D 5 | Reward:μσmM 0.33 0.94 -1.00 1.00 |  entropy 1.739 | value 0.129 | policy_loss 0.032 | value_loss 0.017 | grad_norm 0.094
U 3 | F 000768 | FPS 0106 | D 8 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.819 | value 0.443 | policy_loss 0.054 | value_loss 0.001 | grad_norm 0.075
U 4 | F 001024 | FPS 0097 | D 10 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.788 | value 0.099 | policy_loss -0.032 | value_loss 0.002 | grad_norm 0.053
U 5 | F 001280 | FPS 0096 | D 13 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.796 | value 0.078 | policy_loss -0.034 | value_loss 0.001 | grad_norm 0.043
U 6 | F 001536 | FPS 0112 | D 15 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.797 | value 0.111 | policy_loss -0.038 | value_loss 0.001 | grad_norm 0.035
U 7 | F 001792 | FPS 0106 | D 18 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.676 | value 0.266 | policy_loss 0.051 | value_loss 0.062 | grad_norm 0.142
U 8 | F 002048 | FPS 0109 | D 20 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.696 | value 0.251 | policy_loss 0.031 | value_loss 0.015 | grad_norm 0.099
U 9 | F 002304 | FPS 0093 | D 23 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.722 | value -0.029 | policy_loss 0.002 | value_loss 0.018 | grad_norm 0.119
U 10 | F 002560 | FPS 0085 | D 26 | Reward:μσmM 0.33 0.94 -1.00 1.00 |  entropy 1.669 | value 0.158 | policy_loss 0.008 | value_loss 0.013 | grad_norm 0.144
U 11 | F 002816 | FPS 0083 | D 29 | Reward:μσmM 0.33 0.94 -1.00 1.00 |  entropy 1.620 | value 0.111 | policy_loss -0.006 | value_loss 0.018 | grad_norm 0.084
U 12 | F 003072 | FPS 0094 | D 32 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.524 | value 0.017 | policy_loss -0.034 | value_loss 0.002 | grad_norm 0.063
U 13 | F 003328 | FPS 0095 | D 34 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.584 | value -0.078 | policy_loss -0.057 | value_loss 0.004 | grad_norm 0.097
U 14 | F 003584 | FPS 0083 | D 37 | Reward:μσmM 0.94 0.09 0.82 1.00 |  entropy 1.657 | value 0.298 | policy_loss -0.077 | value_loss 0.021 | grad_norm 0.081
U 15 | F 003840 | FPS 0074 | D 41 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.730 | value -0.048 | policy_loss 0.015 | value_loss 0.020 | grad_norm 0.054
U 16 | F 004096 | FPS 0080 | D 44 | Reward:μσmM 0.54 0.78 -1.00 1.00 |  entropy 1.656 | value 0.304 | policy_loss -0.028 | value_loss 0.018 | grad_norm 0.133
U 17 | F 004352 | FPS 0084 | D 47 | Reward:μσmM 0.20 0.98 -1.00 1.00 |  entropy 1.693 | value 0.215 | policy_loss 0.051 | value_loss 0.027 | grad_norm 0.101
U 18 | F 004608 | FPS 0073 | D 51 | Reward:μσmM 0.09 0.98 -1.00 1.00 |  entropy 1.716 | value 0.426 | policy_loss 0.119 | value_loss 0.144 | grad_norm 0.130
U 19 | F 004864 | FPS 0094 | D 53 | Reward:μσmM 0.48 0.86 -1.00 1.00 |  entropy 1.680 | value 0.117 | policy_loss -0.062 | value_loss 0.052 | grad_norm 0.110
U 20 | F 005120 | FPS 0092 | D 56 | Reward:μσmM 0.93 0.09 0.80 1.00 |  entropy 1.637 | value 0.460 | policy_loss -0.133 | value_loss 0.036 | grad_norm 0.252
Test 10 turns results: Start from 3, reward per episode: OrderedDict({'mean': -0.7, 'std': 0.45825756949558394, 'min': -1.0, 'max': 0.0})
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0093 | D 2 | Reward:μσmM 0.88 0.12 0.77 1.00 |  entropy 1.725 | value 0.685 | policy_loss 0.045 | value_loss 0.008 | grad_norm 0.108
U 2 | F 000512 | FPS 0088 | D 5 | Reward:μσmM 0.53 0.77 -1.00 1.00 |  entropy 1.688 | value 0.619 | policy_loss 0.088 | value_loss 0.080 | grad_norm 0.175
U 3 | F 000768 | FPS 0091 | D 8 | Reward:μσmM 0.23 0.88 -1.00 1.00 |  entropy 1.706 | value 0.575 | policy_loss 0.128 | value_loss 0.079 | grad_norm 0.419
U 4 | F 001024 | FPS 0084 | D 11 | Reward:μσmM 0.78 0.22 0.56 1.00 |  entropy 1.772 | value 0.499 | policy_loss 0.025 | value_loss 0.003 | grad_norm 0.079
U 5 | F 001280 | FPS 0083 | D 14 | Reward:μσmM 0.41 0.83 -1.00 1.00 |  entropy 1.744 | value 0.481 | policy_loss 0.059 | value_loss 0.069 | grad_norm 0.157
U 6 | F 001536 | FPS 0082 | D 17 | Reward:μσmM 0.86 0.14 0.72 1.00 |  entropy 1.718 | value 0.476 | policy_loss -0.001 | value_loss 0.004 | grad_norm 0.092
U 7 | F 001792 | FPS 0081 | D 20 | Reward:μσmM 0.58 0.72 -1.00 1.00 |  entropy 1.695 | value 0.560 | policy_loss -0.012 | value_loss 0.063 | grad_norm 0.123
U 8 | F 002048 | FPS 0085 | D 23 | Reward:μσmM 0.43 0.83 -1.00 1.00 |  entropy 1.726 | value 0.406 | policy_loss 0.034 | value_loss 0.048 | grad_norm 0.108
U 9 | F 002304 | FPS 0084 | D 26 | Reward:μσmM 0.33 0.94 -1.00 1.00 |  entropy 1.693 | value 0.496 | policy_loss 0.062 | value_loss 0.047 | grad_norm 0.160
U 10 | F 002560 | FPS 0092 | D 29 | Reward:μσmM -0.33 0.94 -1.00 1.00 |  entropy 1.717 | value 0.248 | policy_loss 0.129 | value_loss 0.099 | grad_norm 0.104
U 11 | F 002816 | FPS 0088 | D 32 | Reward:μσmM 0.89 0.15 0.68 1.00 |  entropy 1.706 | value 0.245 | policy_loss -0.090 | value_loss 0.007 | grad_norm 0.129
U 12 | F 003072 | FPS 0095 | D 35 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.795 | value 0.145 | policy_loss -0.023 | value_loss 0.001 | grad_norm 0.023
U 13 | F 003328 | FPS 0091 | D 38 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.646 | value 0.225 | policy_loss 0.060 | value_loss 0.043 | grad_norm 0.092
U 14 | F 003584 | FPS 0091 | D 40 | Reward:μσmM 0.41 0.83 -1.00 1.00 |  entropy 1.596 | value 0.312 | policy_loss -0.020 | value_loss 0.023 | grad_norm 0.143
U 15 | F 003840 | FPS 0092 | D 43 | Reward:μσmM 0.96 0.05 0.89 1.00 |  entropy 1.656 | value 0.184 | policy_loss -0.071 | value_loss 0.007 | grad_norm 0.060
U 16 | F 004096 | FPS 0081 | D 46 | Reward:μσmM 0.93 0.08 0.76 1.00 |  entropy 1.470 | value 0.521 | policy_loss -0.185 | value_loss 0.024 | grad_norm 0.277
U 17 | F 004352 | FPS 0089 | D 49 | Reward:μσmM 0.90 0.12 0.71 1.00 |  entropy 1.589 | value 0.527 | policy_loss -0.091 | value_loss 0.010 | grad_norm 0.233
U 18 | F 004608 | FPS 0089 | D 52 | Reward:μσmM 0.93 0.09 0.73 1.00 |  entropy 1.527 | value 0.727 | policy_loss -0.032 | value_loss 0.005 | grad_norm 0.075
U 19 | F 004864 | FPS 0075 | D 56 | Reward:μσmM 0.61 0.72 -1.00 1.00 |  entropy 1.670 | value 0.700 | policy_loss 0.079 | value_loss 0.034 | grad_norm 0.051
U 20 | F 005120 | FPS 0085 | D 59 | Reward:μσmM 0.63 0.67 -1.00 1.00 |  entropy 1.646 | value 0.715 | policy_loss -0.001 | value_loss 0.015 | grad_norm 0.145
Test 10 turns results: Start from 3, reward per episode: OrderedDict({'mean': -0.7, 'std': 0.45825756949558394, 'min': -1.0, 'max': 0.0})
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0087 | D 2 | Reward:μσmM 0.89 0.16 0.66 1.00 |  entropy 1.698 | value 0.683 | policy_loss 0.043 | value_loss 0.002 | grad_norm 0.065
U 2 | F 000512 | FPS 0106 | D 5 | Reward:μσmM 0.27 0.90 -1.00 1.00 |  entropy 1.728 | value 0.566 | policy_loss 0.104 | value_loss 0.105 | grad_norm 0.149
U 3 | F 000768 | FPS 0118 | D 7 | Reward:μσmM -0.44 0.88 -1.00 1.00 |  entropy 1.723 | value 0.369 | policy_loss 0.329 | value_loss 0.219 | grad_norm 0.285
U 4 | F 001024 | FPS 0118 | D 9 | Reward:μσmM 0.25 0.89 -1.00 1.00 |  entropy 1.694 | value 0.490 | policy_loss -0.027 | value_loss 0.045 | grad_norm 0.116
U 5 | F 001280 | FPS 0095 | D 12 | Reward:μσmM 0.92 0.10 0.76 1.00 |  entropy 1.654 | value 0.595 | policy_loss -0.095 | value_loss 0.008 | grad_norm 0.129
U 6 | F 001536 | FPS 0093 | D 15 | Reward:μσmM 0.89 0.12 0.72 1.00 |  entropy 1.693 | value 0.555 | policy_loss -0.030 | value_loss 0.006 | grad_norm 0.060
U 7 | F 001792 | FPS 0099 | D 17 | Reward:μσmM 0.92 0.10 0.78 1.00 |  entropy 1.659 | value 0.729 | policy_loss -0.029 | value_loss 0.001 | grad_norm 0.118
U 8 | F 002048 | FPS 0068 | D 21 | Reward:μσmM 0.20 0.98 -1.00 1.00 |  entropy 1.670 | value 0.635 | policy_loss 0.118 | value_loss 0.112 | grad_norm 0.113
U 9 | F 002304 | FPS 0092 | D 24 | Reward:μσmM 0.91 0.12 0.74 1.00 |  entropy 1.638 | value 0.529 | policy_loss -0.018 | value_loss 0.003 | grad_norm 0.086
U 10 | F 002560 | FPS 0116 | D 26 | Reward:μσmM 0.94 0.08 0.83 1.00 |  entropy 1.692 | value 0.495 | policy_loss 0.011 | value_loss 0.004 | grad_norm 0.059
U 11 | F 002816 | FPS 0118 | D 28 | Reward:μσmM 0.94 0.08 0.82 1.00 |  entropy 1.609 | value 0.761 | policy_loss -0.064 | value_loss 0.004 | grad_norm 0.104
U 12 | F 003072 | FPS 0109 | D 31 | Reward:μσmM 0.63 0.67 -1.00 1.00 |  entropy 1.632 | value 0.687 | policy_loss 0.023 | value_loss 0.047 | grad_norm 0.090
U 13 | F 003328 | FPS 0112 | D 33 | Reward:μσmM 0.92 0.09 0.77 1.00 |  entropy 1.616 | value 0.750 | policy_loss -0.014 | value_loss 0.003 | grad_norm 0.115
U 14 | F 003584 | FPS 0114 | D 35 | Reward:μσmM 0.64 0.67 -1.00 1.00 |  entropy 1.673 | value 0.699 | policy_loss 0.047 | value_loss 0.078 | grad_norm 0.145
U 15 | F 003840 | FPS 0095 | D 38 | Reward:μσmM 0.94 0.08 0.82 1.00 |  entropy 1.626 | value 0.735 | policy_loss -0.043 | value_loss 0.002 | grad_norm 0.102
U 16 | F 004096 | FPS 0097 | D 40 | Reward:μσmM 0.68 0.69 -1.00 1.00 |  entropy 1.586 | value 0.680 | policy_loss 0.062 | value_loss 0.055 | grad_norm 0.113
U 17 | F 004352 | FPS 0102 | D 43 | Reward:μσmM 0.93 0.07 0.83 1.00 |  entropy 1.551 | value 0.791 | policy_loss -0.034 | value_loss 0.001 | grad_norm 0.052
U 18 | F 004608 | FPS 0094 | D 46 | Reward:μσmM 0.76 0.59 -1.00 1.00 |  entropy 1.540 | value 0.789 | policy_loss -0.008 | value_loss 0.030 | grad_norm 0.123
U 19 | F 004864 | FPS 0091 | D 48 | Reward:μσmM 0.94 0.07 0.81 1.00 |  entropy 1.487 | value 0.808 | policy_loss -0.033 | value_loss 0.002 | grad_norm 0.080
U 20 | F 005120 | FPS 0086 | D 51 | Reward:μσmM 0.94 0.07 0.83 1.00 |  entropy 1.548 | value 0.812 | policy_loss -0.003 | value_loss 0.002 | grad_norm 0.070
U 241 | F 061696 | FPS 0069 | D 3 | Reward:μσmM 0.94 0.74 0.00 1.81 | policy_loss ['None', 'None', 'None', '0.120', '-0.057'] | value_loss ['None', 'None', 'None', '0.102', '0.009']
U 242 | F 061952 | FPS 0099 | D 6 | Reward:μσmM 1.13 0.92 0.00 1.92 | policy_loss ['None', 'None', 'None', '0.033', '-0.344'] | value_loss ['None', 'None', 'None', '0.110', '0.029']
U 243 | F 062208 | FPS 0079 | D 9 | Reward:μσmM 1.42 0.71 0.00 1.90 | policy_loss ['None', 'None', 'None', '-0.091', '-0.157'] | value_loss ['None', 'None', 'None', '0.057', '0.041']
discover.py --task-config task2 --discover 1 --algo ppo --env MiniGrid-ConfigWorld-v0 --lr 0.0001 --AnomalyNN test_24 --model 20240725-seed1 --discount 0.99

Namespace(task_config='task2', discover=1, algo='ppo', env='MiniGrid-ConfigWorld-v0', model='20240725-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_24', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.0001, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Optimizer loaded

Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': -0.5, 'std': 0.5, 'min': -1.0, 'max': 0.0})
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0123 | D 2 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.867 | value 0.260 | policy_loss 0.446 | value_loss 0.329 | grad_norm 2.825
U 2 | F 000512 | FPS 0135 | D 3 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.853 | value 0.187 | policy_loss 0.198 | value_loss 0.120 | grad_norm 1.507
U 3 | F 000768 | FPS 0135 | D 5 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.827 | value -0.017 | policy_loss 0.100 | value_loss 0.050 | grad_norm 0.283
U 4 | F 001024 | FPS 0135 | D 7 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.786 | value -0.021 | policy_loss -0.025 | value_loss 0.001 | grad_norm 0.123
U 5 | F 001280 | FPS 0134 | D 9 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.769 | value -0.102 | policy_loss 0.152 | value_loss 0.073 | grad_norm 0.219
U 6 | F 001536 | FPS 0134 | D 11 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.678 | value -0.170 | policy_loss 0.062 | value_loss 0.040 | grad_norm 0.233
U 7 | F 001792 | FPS 0098 | D 14 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.811 | value -0.203 | policy_loss 0.010 | value_loss 0.020 | grad_norm 0.138
U 8 | F 002048 | FPS 0128 | D 16 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.789 | value -0.181 | policy_loss 0.004 | value_loss 0.016 | grad_norm 0.186
U 9 | F 002304 | FPS 0135 | D 18 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.752 | value -0.232 | policy_loss 0.066 | value_loss 0.027 | grad_norm 0.228
U 10 | F 002560 | FPS 0135 | D 19 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.772 | value -0.248 | policy_loss 0.013 | value_loss 0.012 | grad_norm 0.128
U 11 | F 002816 | FPS 0134 | D 21 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.732 | value -0.175 | policy_loss -0.032 | value_loss 0.000 | grad_norm 0.064
U 12 | F 003072 | FPS 0134 | D 23 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.732 | value -0.156 | policy_loss -0.030 | value_loss 0.000 | grad_norm 0.044
U 13 | F 003328 | FPS 0134 | D 25 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.781 | value -0.134 | policy_loss -0.027 | value_loss 0.000 | grad_norm 0.044
U 14 | F 003584 | FPS 0134 | D 27 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.814 | value -0.126 | policy_loss -0.026 | value_loss 0.000 | grad_norm 0.053
U 15 | F 003840 | FPS 0098 | D 30 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.860 | value -0.150 | policy_loss 0.024 | value_loss 0.021 | grad_norm 0.154
U 16 | F 004096 | FPS 0134 | D 32 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.818 | value -0.138 | policy_loss -0.030 | value_loss 0.000 | grad_norm 0.067
U 17 | F 004352 | FPS 0098 | D 34 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.841 | value -0.121 | policy_loss -0.026 | value_loss 0.000 | grad_norm 0.028
U 18 | F 004608 | FPS 0097 | D 37 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.811 | value -0.170 | policy_loss 0.081 | value_loss 0.029 | grad_norm 0.070
U 19 | F 004864 | FPS 0097 | D 40 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.789 | value -0.222 | policy_loss 0.079 | value_loss 0.037 | grad_norm 0.208
U 20 | F 005120 | FPS 0098 | D 42 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.809 | value -0.159 | policy_loss -0.038 | value_loss 0.000 | grad_norm 0.080
Test 10 turns results: Start from 3, reward per episode: OrderedDict({'mean': -0.7, 'std': 0.45825756949558405, 'min': -1.0, 'max': 0.0})
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0132 | D 1 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.854 | value -0.114 | policy_loss -0.025 | value_loss 0.000 | grad_norm 0.030
U 2 | F 000512 | FPS 0134 | D 3 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.795 | value -0.133 | policy_loss -0.030 | value_loss 0.001 | grad_norm 0.063
U 3 | F 000768 | FPS 0134 | D 5 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.821 | value -0.103 | policy_loss -0.020 | value_loss 0.000 | grad_norm 0.030
U 4 | F 001024 | FPS 0098 | D 8 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.859 | value -0.135 | policy_loss 0.032 | value_loss 0.013 | grad_norm 0.045
U 5 | F 001280 | FPS 0098 | D 10 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.829 | value -0.148 | policy_loss 0.033 | value_loss 0.022 | grad_norm 0.061
U 6 | F 001536 | FPS 0134 | D 12 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.845 | value -0.127 | policy_loss -0.034 | value_loss 0.000 | grad_norm 0.051
U 7 | F 001792 | FPS 0133 | D 14 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.859 | value -0.167 | policy_loss 0.026 | value_loss 0.008 | grad_norm 0.065
U 8 | F 002048 | FPS 0134 | D 16 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.863 | value -0.110 | policy_loss -0.026 | value_loss 0.000 | grad_norm 0.030
U 9 | F 002304 | FPS 0134 | D 18 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.865 | value -0.128 | policy_loss 0.036 | value_loss 0.017 | grad_norm 0.046
U 10 | F 002560 | FPS 0097 | D 21 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.779 | value -0.260 | policy_loss 0.061 | value_loss 0.026 | grad_norm 0.105
U 11 | F 002816 | FPS 0133 | D 23 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.745 | value -0.387 | policy_loss 0.076 | value_loss 0.038 | grad_norm 0.245
U 12 | F 003072 | FPS 0133 | D 25 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.670 | value -0.359 | policy_loss -0.025 | value_loss 0.013 | grad_norm 0.200
U 13 | F 003328 | FPS 0133 | D 27 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.687 | value -0.292 | policy_loss -0.051 | value_loss 0.001 | grad_norm 0.079
U 14 | F 003584 | FPS 0133 | D 28 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.711 | value -0.261 | policy_loss -0.048 | value_loss 0.000 | grad_norm 0.058
U 15 | F 003840 | FPS 0133 | D 30 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.743 | value -0.233 | policy_loss 0.001 | value_loss 0.009 | grad_norm 0.065
U 16 | F 004096 | FPS 0133 | D 32 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.682 | value -0.175 | policy_loss -0.034 | value_loss 0.000 | grad_norm 0.038
U 17 | F 004352 | FPS 0133 | D 34 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.686 | value -0.147 | policy_loss -0.031 | value_loss 0.000 | grad_norm 0.036
U 18 | F 004608 | FPS 0098 | D 37 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.803 | value -0.163 | policy_loss -0.032 | value_loss 0.000 | grad_norm 0.042
U 19 | F 004864 | FPS 0132 | D 39 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.778 | value -0.117 | policy_loss -0.025 | value_loss 0.000 | grad_norm 0.033
U 20 | F 005120 | FPS 0132 | D 41 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.810 | value -0.130 | policy_loss 0.029 | value_loss 0.012 | grad_norm 0.063
Test 10 turns results: Start from 3, reward per episode: OrderedDict({'mean': -0.7, 'std': 0.45825756949558394, 'min': -1.0, 'max': 0.0})
Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': -0.5, 'std': 0.5, 'min': -1.0, 'max': 0.0})
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0122 | D 2 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.841 | value -0.101 | policy_loss -0.006 | value_loss 0.001 | grad_norm 0.024
U 2 | F 000512 | FPS 0116 | D 4 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.851 | value -0.082 | policy_loss -0.018 | value_loss 0.000 | grad_norm 0.042
U 3 | F 000768 | FPS 0127 | D 6 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.863 | value -0.183 | policy_loss 0.103 | value_loss 0.041 | grad_norm 0.103
U 4 | F 001024 | FPS 0085 | D 9 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.795 | value -0.314 | policy_loss 0.141 | value_loss 0.053 | grad_norm 0.148
U 5 | F 001280 | FPS 0080 | D 12 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.835 | value -0.273 | policy_loss 0.037 | value_loss 0.019 | grad_norm 0.080
U 6 | F 001536 | FPS 0124 | D 14 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.810 | value -0.180 | policy_loss -0.029 | value_loss 0.001 | grad_norm 0.088
U 7 | F 001792 | FPS 0119 | D 16 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.807 | value -0.297 | policy_loss 0.131 | value_loss 0.047 | grad_norm 0.103
U 8 | F 002048 | FPS 0110 | D 19 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.791 | value -0.378 | policy_loss 0.063 | value_loss 0.024 | grad_norm 0.094
U 9 | F 002304 | FPS 0121 | D 21 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.725 | value -0.294 | policy_loss -0.004 | value_loss 0.004 | grad_norm 0.045
discover.py --task-config task2 --discover 1 --algo ppo --env MiniGrid-ConfigWorld-v0 --lr 0.0001 --AnomalyNN test_8 --model 20240725-seed1 --discount 0.99

Namespace(task_config='task2', discover=1, algo='ppo', env='MiniGrid-ConfigWorld-v0', model='20240725-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_8', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.0001, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Optimizer loaded

Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': -0.5, 'std': 0.5, 'min': -1.0, 'max': 0.0})
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0115 | D 2 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.844 | value 0.387 | policy_loss 0.462 | value_loss 0.349 | grad_norm 1.875
U 2 | F 000512 | FPS 0110 | D 4 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.820 | value 0.222 | policy_loss 0.205 | value_loss 0.085 | grad_norm 0.546
U 3 | F 000768 | FPS 0097 | D 7 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.820 | value 0.157 | policy_loss 0.136 | value_loss 0.064 | grad_norm 0.304
U 4 | F 001024 | FPS 0131 | D 9 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.798 | value -0.025 | policy_loss 0.123 | value_loss 0.061 | grad_norm 0.312
U 5 | F 001280 | FPS 0132 | D 11 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.805 | value -0.132 | policy_loss 0.159 | value_loss 0.075 | grad_norm 0.232
U 6 | F 001536 | FPS 0133 | D 13 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.744 | value -0.229 | policy_loss 0.134 | value_loss 0.055 | grad_norm 0.353
U 7 | F 001792 | FPS 0132 | D 14 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.768 | value -0.285 | policy_loss 0.038 | value_loss 0.029 | grad_norm 0.204
U 8 | F 002048 | FPS 0133 | D 16 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.808 | value -0.230 | policy_loss -0.051 | value_loss 0.000 | grad_norm 0.134
U 9 | F 002304 | FPS 0132 | D 18 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.808 | value -0.202 | policy_loss -0.037 | value_loss 0.000 | grad_norm 0.059
U 10 | F 002560 | FPS 0122 | D 20 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.859 | value -0.208 | policy_loss 0.012 | value_loss 0.008 | grad_norm 0.088
U 11 | F 002816 | FPS 0130 | D 22 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.830 | value -0.235 | policy_loss 0.008 | value_loss 0.011 | grad_norm 0.078
U 12 | F 003072 | FPS 0131 | D 24 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.767 | value -0.391 | policy_loss 0.119 | value_loss 0.048 | grad_norm 0.314
U 13 | F 003328 | FPS 0107 | D 27 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.794 | value -0.367 | policy_loss 0.023 | value_loss 0.020 | grad_norm 0.379
U 14 | F 003584 | FPS 0088 | D 30 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.786 | value -0.247 | policy_loss -0.054 | value_loss 0.001 | grad_norm 0.107
U 15 | F 003840 | FPS 0075 | D 33 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.762 | value -0.216 | policy_loss -0.043 | value_loss 0.000 | grad_norm 0.061
U 16 | F 004096 | FPS 0091 | D 36 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.761 | value -0.224 | policy_loss 0.011 | value_loss 0.008 | grad_norm 0.050
U 17 | F 004352 | FPS 0069 | D 40 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.786 | value -0.211 | policy_loss 0.019 | value_loss 0.014 | grad_norm 0.136
U 18 | F 004608 | FPS 0081 | D 43 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.740 | value -0.189 | policy_loss -0.041 | value_loss 0.000 | grad_norm 0.108
U 19 | F 004864 | FPS 0070 | D 46 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.777 | value -0.146 | policy_loss -0.032 | value_loss 0.000 | grad_norm 0.033
U 20 | F 005120 | FPS 0095 | D 49 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.814 | value -0.246 | policy_loss 0.089 | value_loss 0.036 | grad_norm 0.161
discover.py --task-config task2 --discover 1 --algo ppo --env MiniGrid-ConfigWorld-v0 --lr 0.0001 --AnomalyNN test_8 --model 20240725-seed1 --discount 0.99

Namespace(task_config='task2', discover=1, algo='ppo', env='MiniGrid-ConfigWorld-v0', model='20240725-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_8', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.0001, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Optimizer loaded

Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': -0.5, 'std': 0.5, 'min': -1.0, 'max': 0.0})
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0115 | D 2 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.825 | value 0.390 | policy_loss 0.455 | value_loss 0.308 | grad_norm 2.086
U 2 | F 000512 | FPS 0134 | D 4 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.769 | value 0.207 | policy_loss 0.401 | value_loss 0.230 | grad_norm 1.506
U 3 | F 000768 | FPS 0134 | D 6 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.834 | value 0.191 | policy_loss 0.101 | value_loss 0.034 | grad_norm 0.405
U 4 | F 001024 | FPS 0133 | D 7 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.812 | value -0.037 | policy_loss 0.273 | value_loss 0.149 | grad_norm 0.651
U 5 | F 001280 | FPS 0134 | D 9 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.839 | value -0.164 | policy_loss 0.152 | value_loss 0.061 | grad_norm 0.392
U 6 | F 001536 | FPS 0098 | D 12 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.792 | value -0.165 | policy_loss 0.055 | value_loss 0.038 | grad_norm 0.234
U 7 | F 001792 | FPS 0098 | D 15 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.798 | value -0.154 | policy_loss 0.014 | value_loss 0.021 | grad_norm 0.186
U 8 | F 002048 | FPS 0131 | D 17 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.820 | value -0.161 | policy_loss 0.023 | value_loss 0.010 | grad_norm 0.097
U 9 | F 002304 | FPS 0131 | D 19 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.799 | value -0.244 | policy_loss 0.114 | value_loss 0.052 | grad_norm 0.191
U 10 | F 002560 | FPS 0133 | D 20 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.835 | value -0.245 | policy_loss 0.008 | value_loss 0.019 | grad_norm 0.217
U 11 | F 002816 | FPS 0132 | D 22 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.838 | value -0.374 | policy_loss 0.167 | value_loss 0.061 | grad_norm 0.197
U 12 | F 003072 | FPS 0133 | D 24 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.808 | value -0.423 | policy_loss 0.030 | value_loss 0.027 | grad_norm 0.163
U 13 | F 003328 | FPS 0133 | D 26 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.706 | value -0.402 | policy_loss -0.081 | value_loss 0.000 | grad_norm 0.180
U 14 | F 003584 | FPS 0131 | D 28 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.633 | value -0.335 | policy_loss -0.068 | value_loss 0.000 | grad_norm 0.126
U 15 | F 003840 | FPS 0096 | D 31 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.640 | value -0.283 | policy_loss -0.056 | value_loss 0.000 | grad_norm 0.087
U 16 | F 004096 | FPS 0098 | D 33 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.407 | value -0.233 | policy_loss -0.047 | value_loss 0.000 | grad_norm 0.068
U 17 | F 004352 | FPS 0125 | D 36 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.381 | value -0.198 | policy_loss -0.088 | value_loss 0.048 | grad_norm 0.318
U 18 | F 004608 | FPS 0133 | D 37 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.616 | value -0.254 | policy_loss 0.028 | value_loss 0.014 | grad_norm 0.061
U 19 | F 004864 | FPS 0132 | D 39 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.694 | value -0.252 | policy_loss -0.068 | value_loss 0.004 | grad_norm 0.214
U 20 | F 005120 | FPS 0133 | D 41 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.722 | value -0.164 | policy_loss -0.045 | value_loss 0.000 | grad_norm 0.133
Test 10 turns results: Start from 3, reward per episode: OrderedDict({'mean': -0.7, 'std': 0.45825756949558405, 'min': -1.0, 'max': 0.0})
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0114 | D 2 | Reward:μσmM -0.33 0.94 -1.00 1.00 |  entropy 1.744 | value -0.175 | policy_loss -0.053 | value_loss 0.039 | grad_norm 0.199
U 2 | F 000512 | FPS 0128 | D 4 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.745 | value -0.095 | policy_loss -0.112 | value_loss 0.016 | grad_norm 0.239
U 3 | F 000768 | FPS 0129 | D 6 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.699 | value -0.056 | policy_loss -0.104 | value_loss 0.012 | grad_norm 0.262
U 4 | F 001024 | FPS 0133 | D 8 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.772 | value -0.030 | policy_loss -0.006 | value_loss 0.000 | grad_norm 0.039
U 5 | F 001280 | FPS 0098 | D 10 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.716 | value -0.070 | policy_loss 0.051 | value_loss 0.024 | grad_norm 0.110
U 6 | F 001536 | FPS 0097 | D 13 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.778 | value -0.218 | policy_loss 0.177 | value_loss 0.059 | grad_norm 0.182
U 7 | F 001792 | FPS 0098 | D 16 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.770 | value -0.213 | policy_loss 0.009 | value_loss 0.011 | grad_norm 0.168
U 8 | F 002048 | FPS 0134 | D 17 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.801 | value -0.137 | policy_loss -0.033 | value_loss 0.000 | grad_norm 0.054
U 9 | F 002304 | FPS 0133 | D 19 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.803 | value -0.109 | policy_loss -0.022 | value_loss 0.000 | grad_norm 0.023
U 10 | F 002560 | FPS 0133 | D 21 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.848 | value -0.093 | policy_loss -0.018 | value_loss 0.000 | grad_norm 0.020
U 11 | F 002816 | FPS 0131 | D 23 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.839 | value -0.078 | policy_loss -0.015 | value_loss 0.000 | grad_norm 0.013
U 12 | F 003072 | FPS 0129 | D 25 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.819 | value -0.167 | policy_loss 0.092 | value_loss 0.031 | grad_norm 0.063
U 13 | F 003328 | FPS 0132 | D 27 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.861 | value -0.115 | policy_loss -0.020 | value_loss 0.000 | grad_norm 0.040
U 14 | F 003584 | FPS 0132 | D 29 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.857 | value -0.134 | policy_loss 0.025 | value_loss 0.018 | grad_norm 0.086
U 15 | F 003840 | FPS 0132 | D 31 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.832 | value -0.151 | policy_loss 0.026 | value_loss 0.022 | grad_norm 0.081
U 16 | F 004096 | FPS 0131 | D 33 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.825 | value -0.122 | policy_loss -0.031 | value_loss 0.003 | grad_norm 0.059
U 17 | F 004352 | FPS 0132 | D 35 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.836 | value -0.100 | policy_loss -0.021 | value_loss 0.000 | grad_norm 0.029
U 18 | F 004608 | FPS 0133 | D 37 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.878 | value -0.095 | policy_loss -0.009 | value_loss 0.000 | grad_norm 0.015
U 19 | F 004864 | FPS 0133 | D 39 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.878 | value -0.075 | policy_loss -0.015 | value_loss 0.000 | grad_norm 0.017
U 20 | F 005120 | FPS 0133 | D 41 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.899 | value -0.062 | policy_loss -0.012 | value_loss 0.000 | grad_norm 0.008
Test 10 turns results: Start from 3, reward per episode: OrderedDict({'mean': -0.6, 'std': 0.48989794855663565, 'min': -1.0, 'max': 0.0})
Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': -0.5, 'std': 0.5, 'min': -1.0, 'max': 0.0})
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0130 | D 1 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.843 | value -0.157 | policy_loss 0.092 | value_loss 0.032 | grad_norm 0.084
U 2 | F 000512 | FPS 0097 | D 4 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.802 | value -0.264 | policy_loss 0.168 | value_loss 0.061 | grad_norm 0.295
U 3 | F 000768 | FPS 0098 | D 7 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.805 | value -0.260 | policy_loss -0.018 | value_loss 0.011 | grad_norm 0.096
U 4 | F 001024 | FPS 0132 | D 9 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.812 | value -0.233 | policy_loss 0.003 | value_loss 0.007 | grad_norm 0.091
U 5 | F 001280 | FPS 0133 | D 11 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.773 | value -0.172 | policy_loss -0.035 | value_loss 0.000 | grad_norm 0.042
U 6 | F 001536 | FPS 0133 | D 13 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.781 | value -0.145 | policy_loss -0.030 | value_loss 0.000 | grad_norm 0.034
U 7 | F 001792 | FPS 0133 | D 14 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.788 | value -0.127 | policy_loss -0.021 | value_loss 0.003 | grad_norm 0.031
U 8 | F 002048 | FPS 0133 | D 16 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.822 | value -0.163 | policy_loss 0.028 | value_loss 0.021 | grad_norm 0.101
U 9 | F 002304 | FPS 0133 | D 18 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.798 | value -0.173 | policy_loss 0.029 | value_loss 0.019 | grad_norm 0.123
U 10 | F 002560 | FPS 0131 | D 20 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.769 | value -0.362 | policy_loss 0.171 | value_loss 0.056 | grad_norm 0.202
U 11 | F 002816 | FPS 0132 | D 22 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.741 | value -0.321 | policy_loss -0.061 | value_loss 0.001 | grad_norm 0.103
U 12 | F 003072 | FPS 0131 | D 24 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.706 | value -0.315 | policy_loss -0.011 | value_loss 0.012 | grad_norm 0.085
U 13 | F 003328 | FPS 0131 | D 26 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.748 | value -0.251 | policy_loss -0.051 | value_loss 0.000 | grad_norm 0.056
U 14 | F 003584 | FPS 0131 | D 28 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.712 | value -0.210 | policy_loss -0.043 | value_loss 0.000 | grad_norm 0.049
U 15 | F 003840 | FPS 0098 | D 31 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.608 | value -0.178 | policy_loss -0.038 | value_loss 0.000 | grad_norm 0.036
U 16 | F 004096 | FPS 0097 | D 33 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.596 | value -0.161 | policy_loss -0.034 | value_loss 0.000 | grad_norm 0.055
U 17 | F 004352 | FPS 0132 | D 35 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.611 | value -0.130 | policy_loss -0.025 | value_loss 0.000 | grad_norm 0.023
U 18 | F 004608 | FPS 0132 | D 37 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.772 | value -0.113 | policy_loss -0.023 | value_loss 0.000 | grad_norm 0.018
U 19 | F 004864 | FPS 0132 | D 39 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.833 | value -0.188 | policy_loss 0.070 | value_loss 0.035 | grad_norm 0.084
U 20 | F 005120 | FPS 0126 | D 41 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.814 | value -0.146 | policy_loss -0.068 | value_loss 0.036 | grad_norm 0.169
Test 10 turns results: Start from 3, reward per episode: OrderedDict({'mean': -0.8, 'std': 0.4, 'min': -1.0, 'max': 0.0})
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0129 | D 1 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.839 | value -0.102 | policy_loss -0.026 | value_loss 0.000 | grad_norm 0.042
U 2 | F 000512 | FPS 0095 | D 4 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.828 | value -0.022 | policy_loss -0.040 | value_loss 0.057 | grad_norm 0.128
U 3 | F 000768 | FPS 0094 | D 7 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.800 | value -0.042 | policy_loss -0.024 | value_loss 0.035 | grad_norm 0.128
U 4 | F 001024 | FPS 0127 | D 9 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.731 | value -0.139 | policy_loss -0.102 | value_loss 0.011 | grad_norm 0.118
U 5 | F 001280 | FPS 0127 | D 11 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.734 | value -0.073 | policy_loss -0.069 | value_loss 0.003 | grad_norm 0.109
U 6 | F 001536 | FPS 0128 | D 13 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.745 | value -0.031 | policy_loss -0.046 | value_loss 0.003 | grad_norm 0.059
U 7 | F 001792 | FPS 0127 | D 15 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.742 | value -0.042 | policy_loss -0.041 | value_loss 0.003 | grad_norm 0.042
U 8 | F 002048 | FPS 0127 | D 17 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.782 | value -0.021 | policy_loss -0.029 | value_loss 0.003 | grad_norm 0.038
U 9 | F 002304 | FPS 0123 | D 19 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.784 | value 0.007 | policy_loss 0.039 | value_loss 0.017 | grad_norm 0.036
U 10 | F 002560 | FPS 0126 | D 21 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.778 | value 0.289 | policy_loss 0.118 | value_loss 0.061 | grad_norm 0.090
U 11 | F 002816 | FPS 0125 | D 23 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.748 | value 0.539 | policy_loss 0.078 | value_loss 0.014 | grad_norm 0.065
U 12 | F 003072 | FPS 0127 | D 25 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.767 | value 0.435 | policy_loss 0.039 | value_loss 0.010 | grad_norm 0.138
U 13 | F 003328 | FPS 0127 | D 27 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.736 | value 0.118 | policy_loss 0.050 | value_loss 0.029 | grad_norm 0.173
U 14 | F 003584 | FPS 0126 | D 29 | Reward:μσmM -0.80 0.60 -1.00 1.00 |  entropy 1.721 | value 0.199 | policy_loss 0.333 | value_loss 0.297 | grad_norm 0.264
U 15 | F 003840 | FPS 0128 | D 31 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.643 | value -0.009 | policy_loss -0.027 | value_loss 0.026 | grad_norm 0.049
U 16 | F 004096 | FPS 0127 | D 33 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.638 | value 0.021 | policy_loss -0.019 | value_loss 0.036 | grad_norm 0.135
U 17 | F 004352 | FPS 0125 | D 35 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.710 | value -0.070 | policy_loss -0.037 | value_loss 0.003 | grad_norm 0.069
U 18 | F 004608 | FPS 0126 | D 37 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.562 | value 0.464 | policy_loss 0.115 | value_loss 0.068 | grad_norm 0.307
U 19 | F 004864 | FPS 0127 | D 39 | Reward:μσmM -0.33 0.94 -1.00 1.00 |  entropy 1.738 | value 0.311 | policy_loss 0.157 | value_loss 0.096 | grad_norm 0.214
U 20 | F 005120 | FPS 0105 | D 42 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.791 | value 0.307 | policy_loss 0.081 | value_loss 0.066 | grad_norm 0.191
Test 10 turns results: Start from 3, reward per episode: OrderedDict({'mean': -0.7, 'std': 0.45825756949558394, 'min': -1.0, 'max': 0.0})
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0126 | D 2 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.736 | value 0.115 | policy_loss -0.026 | value_loss 0.002 | grad_norm 0.050
U 2 | F 000512 | FPS 0128 | D 4 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.750 | value 0.048 | policy_loss -0.053 | value_loss 0.024 | grad_norm 0.083
U 3 | F 000768 | FPS 0121 | D 6 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.762 | value -0.064 | policy_loss -0.049 | value_loss 0.017 | grad_norm 0.115
U 4 | F 001024 | FPS 0127 | D 8 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.791 | value 0.029 | policy_loss -0.049 | value_loss 0.003 | grad_norm 0.062
U 5 | F 001280 | FPS 0125 | D 10 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.787 | value 0.141 | policy_loss 0.055 | value_loss 0.097 | grad_norm 0.180
U 6 | F 001536 | FPS 0109 | D 12 | Reward:μσmM -0.33 0.94 -1.00 1.00 |  entropy 1.755 | value 0.260 | policy_loss 0.135 | value_loss 0.101 | grad_norm 0.162
U 7 | F 001792 | FPS 0127 | D 14 | Reward:μσmM -0.33 0.94 -1.00 1.00 |  entropy 1.755 | value 0.373 | policy_loss 0.027 | value_loss 0.029 | grad_norm 0.077
U 8 | F 002048 | FPS 0104 | D 17 | Reward:μσmM 0.33 0.94 -1.00 1.00 |  entropy 1.792 | value 0.099 | policy_loss -0.031 | value_loss 0.020 | grad_norm 0.060
U 9 | F 002304 | FPS 0118 | D 19 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.738 | value 0.257 | policy_loss 0.067 | value_loss 0.093 | grad_norm 0.163
U 10 | F 002560 | FPS 0122 | D 21 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.715 | value -0.017 | policy_loss -0.004 | value_loss 0.068 | grad_norm 0.196
U 11 | F 002816 | FPS 0118 | D 23 | Reward:μσmM 0.42 0.83 -1.00 1.00 |  entropy 1.720 | value 0.087 | policy_loss -0.163 | value_loss 0.049 | grad_norm 0.311
U 12 | F 003072 | FPS 0126 | D 25 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.720 | value -0.011 | policy_loss -0.038 | value_loss 0.003 | grad_norm 0.096
U 13 | F 003328 | FPS 0121 | D 27 | Reward:μσmM 0.33 0.94 -1.00 1.00 |  entropy 1.678 | value 0.154 | policy_loss 0.010 | value_loss 0.016 | grad_norm 0.048
U 14 | F 003584 | FPS 0126 | D 29 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.793 | value 0.056 | policy_loss -0.029 | value_loss 0.003 | grad_norm 0.036
U 15 | F 003840 | FPS 0124 | D 31 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.806 | value 0.094 | policy_loss 0.028 | value_loss 0.091 | grad_norm 0.115
U 16 | F 004096 | FPS 0121 | D 33 | Reward:μσmM 0.33 0.94 -1.00 1.00 |  entropy 1.725 | value 0.314 | policy_loss 0.000 | value_loss 0.018 | grad_norm 0.101
U 17 | F 004352 | FPS 0126 | D 35 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.781 | value 0.115 | policy_loss -0.034 | value_loss 0.002 | grad_norm 0.055
U 18 | F 004608 | FPS 0126 | D 37 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.758 | value 0.087 | policy_loss -0.038 | value_loss 0.001 | grad_norm 0.052
U 19 | F 004864 | FPS 0115 | D 40 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.749 | value 0.199 | policy_loss 0.000 | value_loss 0.003 | grad_norm 0.040
U 20 | F 005120 | FPS 0117 | D 42 | Reward:μσmM 0.33 0.94 -1.00 1.00 |  entropy 1.727 | value 0.329 | policy_loss 0.045 | value_loss 0.022 | grad_norm 0.151
Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': -0.4, 'std': 0.4898979485566356, 'min': -1.0, 'max': 0.0})
Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': -0.1, 'std': 0.30000000000000004, 'min': -1.0, 'max': 0.0})
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0124 | D 2 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.755 | value 0.192 | policy_loss 0.081 | value_loss 0.084 | grad_norm 0.142
U 2 | F 000512 | FPS 0125 | D 4 | Reward:μσmM 0.75 0.25 0.50 1.00 |  entropy 1.731 | value 0.210 | policy_loss -0.081 | value_loss 0.018 | grad_norm 0.234
U 3 | F 000768 | FPS 0098 | D 6 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.755 | value 0.258 | policy_loss 0.001 | value_loss 0.001 | grad_norm 0.071
U 4 | F 001024 | FPS 0087 | D 9 | Reward:μσmM 0.33 0.94 -1.00 1.00 |  entropy 1.713 | value 0.230 | policy_loss 0.007 | value_loss 0.016 | grad_norm 0.105
U 5 | F 001280 | FPS 0079 | D 12 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.648 | value 0.253 | policy_loss 0.073 | value_loss 0.053 | grad_norm 0.157
U 6 | F 001536 | FPS 0093 | D 15 | Reward:μσmM 0.33 0.94 -1.00 1.00 |  entropy 1.662 | value 0.337 | policy_loss -0.015 | value_loss 0.019 | grad_norm 0.186
U 7 | F 001792 | FPS 0079 | D 18 | Reward:μσmM 0.33 0.94 -1.00 1.00 |  entropy 1.708 | value 0.377 | policy_loss 0.035 | value_loss 0.028 | grad_norm 0.119
U 8 | F 002048 | FPS 0082 | D 21 | Reward:μσmM 0.20 0.98 -1.00 1.00 |  entropy 1.728 | value 0.447 | policy_loss 0.007 | value_loss 0.027 | grad_norm 0.163
U 9 | F 002304 | FPS 0065 | D 25 | Reward:μσmM 0.54 0.78 -1.00 1.00 |  entropy 1.695 | value 0.219 | policy_loss -0.128 | value_loss 0.032 | grad_norm 0.212
U 10 | F 002560 | FPS 0086 | D 28 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.664 | value 0.348 | policy_loss 0.118 | value_loss 0.082 | grad_norm 0.155
U 11 | F 002816 | FPS 0084 | D 31 | Reward:μσmM 0.96 0.05 0.89 1.00 |  entropy 1.718 | value -0.063 | policy_loss -0.150 | value_loss 0.027 | grad_norm 0.139
U 12 | F 003072 | FPS 0070 | D 35 | Reward:μσmM 0.90 0.10 0.78 1.00 |  entropy 1.517 | value 0.509 | policy_loss -0.214 | value_loss 0.029 | grad_norm 0.248
U 13 | F 003328 | FPS 0064 | D 39 | Reward:μσmM 0.94 0.08 0.83 1.00 |  entropy 1.538 | value 0.545 | policy_loss -0.141 | value_loss 0.017 | grad_norm 0.161
U 14 | F 003584 | FPS 0072 | D 43 | Reward:μσmM 0.92 0.08 0.81 1.00 |  entropy 1.536 | value 0.784 | policy_loss -0.027 | value_loss 0.008 | grad_norm 0.095
U 15 | F 003840 | FPS 0073 | D 46 | Reward:μσmM 0.70 0.65 -1.00 1.00 |  entropy 1.567 | value 0.746 | policy_loss -0.013 | value_loss 0.021 | grad_norm 0.125
U 16 | F 004096 | FPS 0066 | D 50 | Reward:μσmM 0.38 0.87 -1.00 1.00 |  entropy 1.627 | value 0.657 | policy_loss 0.118 | value_loss 0.142 | grad_norm 0.346
U 17 | F 004352 | FPS 0076 | D 53 | Reward:μσmM 0.72 0.61 -1.00 1.00 |  entropy 1.553 | value 0.658 | policy_loss 0.006 | value_loss 0.071 | grad_norm 0.230
U 18 | F 004608 | FPS 0069 | D 57 | Reward:μσmM 0.29 0.92 -1.00 1.00 |  entropy 1.653 | value 0.584 | policy_loss 0.088 | value_loss 0.128 | grad_norm 0.109
U 19 | F 004864 | FPS 0078 | D 60 | Reward:μσmM 0.94 0.06 0.82 1.00 |  entropy 1.564 | value 0.783 | policy_loss -0.083 | value_loss 0.005 | grad_norm 0.203
U 20 | F 005120 | FPS 0094 | D 63 | Reward:μσmM 0.36 0.89 -1.00 1.00 |  entropy 1.650 | value 0.714 | policy_loss 0.067 | value_loss 0.125 | grad_norm 0.160
U 241 | F 061696 | FPS 0062 | D 4 | Reward:μσmM 0.00 0.63 -1.00 1.00 | policy_loss ['None', 'None', 'None', '0.299', '-0.336'] | value_loss ['None', 'None', 'None', '0.144', '0.075']
U 242 | F 061952 | FPS 0091 | D 6 | Reward:μσmM 0.57 1.13 0.00 2.84 | policy_loss ['None', 'None', '0.019', '0.065', '-0.094'] | value_loss ['None', 'None', '0.000', '0.186', '0.019']
U 243 | F 062208 | FPS 0086 | D 9 | Reward:μσmM 1.58 0.82 1.00 2.73 | policy_loss ['None', 'None', '0.282', '-0.769', '-0.299'] | value_loss ['None', 'None', '0.155', '0.337', '0.061']
U 244 | F 062464 | FPS 0096 | D 12 | Reward:μσmM 1.00 0.00 1.00 1.00 | policy_loss ['None', 'None', '0.564', '-0.079', '-0.273'] | value_loss ['None', 'None', '0.377', '0.063', '0.021']
U 245 | F 062720 | FPS 0092 | D 15 | Reward:μσmM 1.91 1.19 0.00 2.86 | policy_loss ['None', 'None', '-0.001', '-0.251', '-0.263'] | value_loss ['None', 'None', '0.135', '0.010', '0.007']
U 246 | F 062976 | FPS 0106 | D 17 | Reward:μσmM 2.49 0.36 2.00 2.83 | policy_loss ['None', 'None', '-0.052', '-0.177', '-0.286'] | value_loss ['None', 'None', '0.006', '0.005', '0.023']
U 247 | F 063232 | FPS 0106 | D 20 | Reward:μσmM 1.52 1.11 0.00 2.85 | policy_loss ['None', 'None', '0.077', '0.121', '-0.039'] | value_loss ['None', 'None', '0.102', '0.086', '0.021']
U 248 | F 063488 | FPS 0110 | D 22 | Reward:μσmM 1.96 0.85 1.00 2.92 | policy_loss ['None', 'None', '-0.025', '0.034', '-0.024'] | value_loss ['None', 'None', '0.163', '0.067', '0.020']
U 249 | F 063744 | FPS 0113 | D 24 | Reward:μσmM 2.49 0.75 1.00 2.87 | policy_loss ['None', 'None', '-0.258', '-0.134', '-0.004'] | value_loss ['None', 'None', '0.053', '0.009', '0.004']
U 250 | F 064000 | FPS 0105 | D 27 | Reward:μσmM 1.91 1.47 -1.00 2.91 | policy_loss ['None', 'None', '-0.119', '-0.131', '0.074'] | value_loss ['None', 'None', '0.007', '0.012', '0.539']
U 10 | Test reward:μσmM 0.00 0.00 0.00 0.00 | Test num frames:μσmM 256.00 0.00 256.00 256.00
Status saved
U 251 | F 064256 | FPS 0110 | D 87 | Reward:μσmM 2.76 0.34 2.00 2.94 | policy_loss ['None', 'None', '-0.086', '-0.244', '-0.261'] | value_loss ['None', 'None', '0.006', '0.010', '0.022']
U 252 | F 064512 | FPS 0111 | D 90 | Reward:μσmM 2.93 0.02 2.89 2.95 | policy_loss ['None', 'None', '-0.063', '-0.215', '-0.319'] | value_loss ['None', 'None', '0.003', '0.006', '0.027']
U 253 | F 064768 | FPS 0105 | D 92 | Reward:μσmM 2.68 0.63 1.00 2.95 | policy_loss ['None', 'None', '-0.012', '-0.044', '-0.085'] | value_loss ['None', 'None', '0.002', '0.003', '0.006']
U 254 | F 065024 | FPS 0110 | D 94 | Reward:μσmM 2.67 0.85 0.00 2.95 | policy_loss ['None', 'None', '-0.043', '-0.104', '-0.108'] | value_loss ['None', 'None', '0.000', '0.000', '0.002']
U 255 | F 065280 | FPS 0109 | D 97 | Reward:μσmM 2.70 0.81 0.00 2.95 | policy_loss ['None', 'None', '-0.011', '-0.025', '-0.080'] | value_loss ['None', 'None', '0.000', '0.000', '0.001']
U 256 | F 065536 | FPS 0107 | D 99 | Reward:μσmM 2.75 0.58 1.00 2.95 | policy_loss ['None', 'None', '0.029', '0.020', '-0.013'] | value_loss ['None', 'None', '0.001', '0.001', '0.003']
U 257 | F 065792 | FPS 0105 | D 102 | Reward:μσmM 2.71 0.61 1.00 2.94 | policy_loss ['None', 'None', '0.016', '0.032', '0.018'] | value_loss ['None', 'None', '0.002', '0.001', '0.001']
U 258 | F 066048 | FPS 0111 | D 104 | Reward:μσmM 2.50 0.69 1.00 2.91 | policy_loss ['None', 'None', '0.057', '0.148', '0.190'] | value_loss ['None', 'None', '0.075', '0.095', '0.064']
U 259 | F 066304 | FPS 0119 | D 106 | Reward:μσmM 2.40 1.08 0.00 2.92 | policy_loss ['None', 'None', '-0.009', '0.032', '0.014'] | value_loss ['None', 'None', '0.003', '0.008', '0.006']
U 260 | F 066560 | FPS 0107 | D 109 | Reward:μσmM 1.38 1.38 0.00 2.83 | policy_loss ['None', 'None', '0.073', '0.255', '0.267'] | value_loss ['None', 'None', '0.001', '0.230', '0.197']
U 10 | Test reward:μσmM -0.30 0.46 -1.00 0.00 | Test num frames:μσmM 188.60 103.93 2.00 256.00
Status saved
U 261 | F 066816 | FPS 0109 | D 154 | Reward:μσmM 2.18 0.84 1.00 2.79 | policy_loss ['None', 'None', '0.025', '-0.126', '0.124'] | value_loss ['None', 'None', '0.001', '0.006', '0.010']
U 262 | F 067072 | FPS 0113 | D 157 | Reward:μσmM 1.87 1.32 0.00 2.81 | policy_loss ['None', 'None', '-0.053', '0.085', '0.076'] | value_loss ['None', 'None', '0.012', '0.003', '0.015']
U 263 | F 067328 | FPS 0110 | D 159 | Reward:μσmM 2.54 0.38 2.00 2.83 | policy_loss ['None', 'None', '-0.068', '0.112', '-0.116'] | value_loss ['None', 'None', '0.025', '0.013', '0.010']
U 264 | F 067584 | FPS 0095 | D 162 | Reward:μσmM 1.87 0.87 1.00 2.74 | policy_loss ['None', 'None', '-0.024', '0.166', '0.138'] | value_loss ['None', 'None', '0.001', '0.032', '0.011']
U 265 | F 067840 | FPS 0112 | D 164 | Reward:μσmM -0.40 0.80 -1.00 1.00 | policy_loss ['None', 'None', 'None', '0.123', '0.365'] | value_loss ['None', 'None', 'None', '0.102', '0.468']
U 266 | F 068096 | FPS 0111 | D 166 | Reward:μσmM -0.25 0.83 -1.00 1.00 | policy_loss ['None', 'None', '0.496', '-0.059', '0.531'] | value_loss ['None', 'None', '0.286', '0.010', '0.674']
U 267 | F 068352 | FPS 0111 | D 169 | Reward:μσmM 0.88 1.49 -1.00 2.65 | policy_loss ['None', 'None', '-0.030', '0.004', '0.138'] | value_loss ['None', 'None', '0.005', '0.014', '0.363']
U 268 | F 068608 | FPS 0112 | D 171 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', 'None', '0.854', '0.184'] | value_loss ['None', 'None', 'None', '0.875', '0.083']
U 269 | F 068864 | FPS 0110 | D 173 | Reward:μσmM 0.91 1.28 0.00 2.73 | policy_loss ['None', 'None', '-0.155', '-0.137', '0.092'] | value_loss ['None', 'None', '0.001', '0.395', '0.036']
U 270 | F 069120 | FPS 0112 | D 176 | Reward:μσmM 0.00 1.00 -1.00 1.00 | policy_loss ['None', 'None', 'None', '0.037', '0.231'] | value_loss ['None', 'None', 'None', '0.000', '0.167']
U 10 | Test reward:μσmM -0.80 0.40 -1.00 0.00 | Test num frames:μσmM 111.00 105.81 1.00 256.00
Status saved
U 271 | F 069376 | FPS 0108 | D 203 | Reward:μσmM 0.67 0.94 0.00 2.00 | policy_loss ['None', 'None', '-0.040', '0.237', '0.081'] | value_loss ['None', 'None', '0.001', '0.291', '0.089']
U 272 | F 069632 | FPS 0110 | D 205 | Reward:μσmM 1.11 1.65 -1.00 2.85 | policy_loss ['None', 'None', '-0.069', '-0.441', '-0.022'] | value_loss ['None', 'None', '0.006', '0.197', '0.174']
U 273 | F 069888 | FPS 0110 | D 207 | Reward:μσmM 1.21 1.58 -1.00 2.62 | policy_loss ['None', 'None', '0.001', '-0.477', '-0.215'] | value_loss ['None', 'None', '0.006', '0.344', '0.204']
U 274 | F 070144 | FPS 0105 | D 210 | Reward:μσmM 2.38 0.80 1.00 2.87 | policy_loss ['None', 'None', '-0.128', '-0.379', '-0.493'] | value_loss ['None', 'None', '0.007', '0.115', '0.121']
U 275 | F 070400 | FPS 0100 | D 213 | Reward:μσmM 2.63 0.37 2.00 2.87 | policy_loss ['None', 'None', '-0.059', '-0.038', '-0.615'] | value_loss ['None', 'None', '0.007', '0.005', '0.171']
U 276 | F 070656 | FPS 0088 | D 215 | Reward:μσmM 2.58 0.71 1.00 2.92 | policy_loss ['None', 'None', '-0.137', '-0.060', '-0.506'] | value_loss ['None', 'None', '0.002', '0.004', '0.123']
U 277 | F 070912 | FPS 0082 | D 219 | Reward:μσmM 2.75 0.34 2.00 2.94 | policy_loss ['None', 'None', '-0.057', '-0.111', '-0.318'] | value_loss ['None', 'None', '0.002', '0.009', '0.033']
U 278 | F 071168 | FPS 0086 | D 221 | Reward:μσmM 2.60 0.92 0.00 2.95 | policy_loss ['None', 'None', '-0.092', '-0.084', '-0.300'] | value_loss ['None', 'None', '0.001', '0.004', '0.025']
U 279 | F 071424 | FPS 0083 | D 225 | Reward:μσmM 2.83 0.29 2.00 2.95 | policy_loss ['None', 'None', '-0.012', '-0.047', '-0.162'] | value_loss ['None', 'None', '0.002', '0.001', '0.007']
U 280 | F 071680 | FPS 0075 | D 228 | Reward:μσmM 2.94 0.01 2.93 2.95 | policy_loss ['None', 'None', '-0.032', '-0.031', '-0.067'] | value_loss ['None', 'None', '0.000', '0.001', '0.001']
U 10 | Test reward:μσmM -0.10 0.30 -1.00 0.00 | Test num frames:μσmM 230.50 76.50 1.00 256.00
Status saved
U 281 | F 071936 | FPS 0072 | D 301 | Reward:μσmM 2.94 0.01 2.93 2.96 | policy_loss ['None', 'None', '-0.010', '-0.013', '-0.017'] | value_loss ['None', 'None', '0.000', '0.002', '0.001']
U 282 | F 072192 | FPS 0077 | D 304 | Reward:μσmM 2.81 0.31 2.00 2.95 | policy_loss ['None', 'None', '-0.002', '0.047', '0.018'] | value_loss ['None', 'None', '0.003', '0.005', '0.003']
U 283 | F 072448 | FPS 0080 | D 308 | Reward:μσmM 2.21 1.12 0.00 2.94 | policy_loss ['None', 'None', '0.189', '0.092', '0.175'] | value_loss ['None', 'None', '0.241', '0.051', '0.017']
U 284 | F 072704 | FPS 0084 | D 311 | Reward:μσmM 2.40 1.07 0.00 2.91 | policy_loss ['None', 'None', '-0.052', '0.004', '0.117'] | value_loss ['None', 'None', '0.002', '0.002', '0.019']
U 285 | F 072960 | FPS 0080 | D 314 | Reward:μσmM 2.75 0.34 2.00 2.91 | policy_loss ['None', 'None', '-0.027', '-0.023', '-0.033'] | value_loss ['None', 'None', '0.002', '0.004', '0.004']
U 286 | F 073216 | FPS 0064 | D 318 | Reward:μσmM 1.56 1.75 -1.00 2.92 | policy_loss ['None', 'None', '-0.026', '-0.189', '0.172'] | value_loss ['None', 'None', '0.000', '0.060', '0.542']
U 287 | F 073472 | FPS 0072 | D 321 | Reward:μσmM 1.92 1.36 0.00 2.90 | policy_loss ['None', 'None', '0.025', '0.242', '0.103'] | value_loss ['None', 'None', '0.001', '0.600', '0.156']
U 288 | F 073728 | FPS 0079 | D 325 | Reward:μσmM 2.75 0.33 2.00 2.91 | policy_loss ['None', 'None', '0.001', '-0.080', '-0.104'] | value_loss ['None', 'None', '0.001', '0.003', '0.007']
U 289 | F 073984 | FPS 0074 | D 328 | Reward:μσmM 2.68 0.39 2.00 2.92 | policy_loss ['None', 'None', '0.036', '-0.010', '-0.051'] | value_loss ['None', 'None', '0.002', '0.018', '0.009']
U 290 | F 074240 | FPS 0080 | D 331 | Reward:μσmM 1.95 1.38 -1.00 2.92 | policy_loss ['None', 'None', '0.110', '0.144', '0.282'] | value_loss ['None', 'None', '0.192', '0.115', '0.693']
U 10 | Test reward:μσmM -0.30 0.46 -1.00 0.00 | Test num frames:μσmM 181.60 113.73 2.00 256.00
Status saved
U 291 | F 074496 | FPS 0109 | D 381 | Reward:μσmM 2.58 0.71 1.00 2.92 | policy_loss ['None', 'None', '-0.038', '-0.002', '-0.131'] | value_loss ['None', 'None', '0.002', '0.008', '0.006']
U 292 | F 074752 | FPS 0115 | D 383 | Reward:μσmM 2.58 0.71 1.00 2.95 | policy_loss ['None', 'None', '-0.030', '-0.029', '0.027'] | value_loss ['None', 'None', '0.001', '0.007', '0.006']
U 293 | F 075008 | FPS 0104 | D 386 | Reward:μσmM 2.41 1.08 0.00 2.92 | policy_loss ['None', 'None', '0.017', '-0.054', '-0.042'] | value_loss ['None', 'None', '0.001', '0.004', '0.009']
U 294 | F 075264 | FPS 0102 | D 388 | Reward:μσmM 2.57 0.70 1.00 2.90 | policy_loss ['None', 'None', '0.002', '-0.001', '-0.032'] | value_loss ['None', 'None', '0.000', '0.003', '0.005']
U 295 | F 075520 | FPS 0099 | D 391 | Reward:μσmM 2.75 0.34 2.00 2.92 | policy_loss ['None', 'None', '-0.006', '-0.038', '-0.037'] | value_loss ['None', 'None', '0.000', '0.001', '0.008']
U 296 | F 075776 | FPS 0104 | D 393 | Reward:μσmM 2.37 0.86 1.00 2.94 | policy_loss ['None', 'None', '0.203', '0.008', '-0.056'] | value_loss ['None', 'None', '0.195', '0.014', '0.006']
U 297 | F 076032 | FPS 0109 | D 396 | Reward:μσmM 2.07 1.41 -1.00 2.92 | policy_loss ['None', 'None', '-0.019', '0.020', '0.305'] | value_loss ['None', 'None', '0.002', '0.002', '0.899']
U 298 | F 076288 | FPS 0110 | D 398 | Reward:μσmM 2.78 0.32 2.00 2.93 | policy_loss ['None', 'None', '-0.047', '-0.062', '-0.050'] | value_loss ['None', 'None', '0.001', '0.003', '0.004']
U 299 | F 076544 | FPS 0103 | D 400 | Reward:μσmM 2.31 1.08 0.00 2.95 | policy_loss ['None', 'None', '0.047', '0.034', '0.027'] | value_loss ['None', 'None', '0.097', '0.094', '0.049']
U 300 | F 076800 | FPS 0106 | D 403 | Reward:μσmM 1.72 1.40 0.00 2.92 | policy_loss ['None', 'None', '0.001', '0.321', '0.158'] | value_loss ['None', 'None', '0.002', '0.295', '0.037']
U 10 | Test reward:μσmM -0.30 0.46 -1.00 0.00 | Test num frames:μσmM 183.30 111.07 10.00 256.00
Status saved
U 301 | F 077056 | FPS 0085 | D 446 | Reward:μσmM 2.58 0.70 1.00 2.91 | policy_loss ['None', 'None', '-0.002', '-0.126', '-0.081'] | value_loss ['None', 'None', '0.001', '0.005', '0.006']
U 302 | F 077312 | FPS 0100 | D 449 | Reward:μσmM 2.70 0.35 2.00 2.90 | policy_loss ['None', 'None', '0.007', '0.005', '-0.047'] | value_loss ['None', 'None', '0.001', '0.004', '0.007']
U 303 | F 077568 | FPS 0107 | D 451 | Reward:μσmM 2.40 1.08 0.00 2.92 | policy_loss ['None', 'None', '-0.034', '0.001', '-0.018'] | value_loss ['None', 'None', '0.003', '0.009', '0.014']
U 304 | F 077824 | FPS 0106 | D 454 | Reward:μσmM 2.44 0.83 1.00 2.94 | policy_loss ['None', 'None', '0.043', '0.053', '-0.040'] | value_loss ['None', 'None', '0.126', '0.141', '0.025']
U 305 | F 078080 | FPS 0108 | D 456 | Reward:μσmM 2.36 1.01 0.00 2.93 | policy_loss ['None', 'None', '-0.089', '0.049', '0.188'] | value_loss ['None', 'None', '0.001', '0.377', '0.170']
U 306 | F 078336 | FPS 0111 | D 458 | Reward:μσmM 2.07 1.41 -1.00 2.92 | policy_loss ['None', 'None', '0.002', '-0.211', '-0.024'] | value_loss ['None', 'None', '0.002', '0.003', '0.068']
U 307 | F 078592 | FPS 0113 | D 461 | Reward:μσmM 2.08 1.31 0.00 2.93 | policy_loss ['None', 'None', '-0.001', '0.237', '-0.053'] | value_loss ['None', 'None', '0.001', '0.340', '0.084']
U 308 | F 078848 | FPS 0095 | D 463 | Reward:μσmM 2.38 1.23 -1.00 2.93 | policy_loss ['None', 'None', '-0.035', '-0.115', '-0.072'] | value_loss ['None', 'None', '0.000', '0.017', '0.416']
U 309 | F 079104 | FPS 0110 | D 466 | Reward:μσmM 2.55 0.96 0.00 2.94 | policy_loss ['None', 'None', '0.015', '-0.035', '-0.155'] | value_loss ['None', 'None', '0.002', '0.002', '0.007']
U 310 | F 079360 | FPS 0087 | D 469 | Reward:μσmM 2.68 0.64 1.00 2.93 | policy_loss ['None', 'None', '-0.013', '0.001', '-0.058'] | value_loss ['None', 'None', '0.001', '0.001', '0.021']
U 10 | Test reward:μσmM -0.20 0.40 -1.00 0.00 | Test num frames:μσmM 207.80 96.53 4.00 256.00
Status saved
U 311 | F 079616 | FPS 0104 | D 517 | Reward:μσmM 2.55 0.97 0.00 2.94 | policy_loss ['None', 'None', '-0.006', '0.019', '0.005'] | value_loss ['None', 'None', '0.001', '0.003', '0.006']
U 312 | F 079872 | FPS 0105 | D 519 | Reward:μσmM 2.18 1.35 -1.00 2.94 | policy_loss ['None', 'None', '0.007', '0.000', '0.108'] | value_loss ['None', 'None', '0.001', '0.005', '0.334']
U 313 | F 080128 | FPS 0106 | D 522 | Reward:μσmM 2.36 0.86 1.00 2.92 | policy_loss ['None', 'None', '0.116', '0.078', '0.040'] | value_loss ['None', 'None', '0.174', '0.062', '0.026']
U 314 | F 080384 | FPS 0111 | D 524 | Reward:μσmM 1.36 1.31 0.00 2.91 | policy_loss ['None', 'None', '0.237', '0.443', '0.204'] | value_loss ['None', 'None', '0.195', '0.359', '0.037']
U 315 | F 080640 | FPS 0088 | D 527 | Reward:μσmM 1.46 0.79 1.00 2.83 | policy_loss ['None', 'None', '0.354', '0.109', '0.170'] | value_loss ['None', 'None', '0.280', '0.060', '0.015']
U 316 | F 080896 | FPS 0072 | D 531 | Reward:μσmM 2.28 1.14 0.00 2.88 | policy_loss ['None', 'None', '-0.182', '-0.108', '0.044'] | value_loss ['None', 'None', '0.008', '0.009', '0.007']
U 317 | F 081152 | FPS 0090 | D 534 | Reward:μσmM 1.74 1.61 -1.00 2.90 | policy_loss ['None', 'None', '-0.019', '-0.145', '0.246'] | value_loss ['None', 'None', '0.004', '0.005', '0.874']
U 318 | F 081408 | FPS 0085 | D 536 | Reward:μσmM 2.63 0.36 2.00 2.87 | policy_loss ['None', 'None', '-0.018', '0.020', '-0.217'] | value_loss ['None', 'None', '0.002', '0.007', '0.014']
U 319 | F 081664 | FPS 0103 | D 539 | Reward:μσmM 2.74 0.33 2.00 2.92 | policy_loss ['None', 'None', '-0.063', '-0.102', '-0.205'] | value_loss ['None', 'None', '0.003', '0.005', '0.009']
U 320 | F 081920 | FPS 0099 | D 542 | Reward:μσmM 2.27 1.30 -1.00 2.94 | policy_loss ['None', 'None', '-0.074', '-0.123', '-0.065'] | value_loss ['None', 'None', '0.001', '0.004', '0.389']
U 10 | Test reward:μσmM -0.10 0.30 -1.00 0.00 | Test num frames:μσmM 231.90 72.30 15.00 256.00
Status saved
U 321 | F 082176 | FPS 0106 | D 600 | Reward:μσmM 2.83 0.29 2.00 2.95 | policy_loss ['None', 'None', '-0.020', '-0.090', '-0.184'] | value_loss ['None', 'None', '0.000', '0.002', '0.018']
discover.py --task-config task3 --discover 0 --algo ppo --env MiniGrid-ConfigWorld-v0 --lr 0.0001 --AnomalyNN test_8 --model 20240725-seed1 --discount 0.99

Namespace(task_config='task3', discover=0, algo='ppo', env='MiniGrid-ConfigWorld-v0', model='20240725-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_8', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.0001, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Optimizer loaded

Optimizer loaded

U 321 | F 082176 | FPS 0078 | D 3 | Reward:μσmM 2.83 0.29 2.00 2.95 | policy_loss ['None', 'None', '-0.034', '-0.097', '-0.146'] | value_loss ['None', 'None', '0.001', '0.002', '0.018']
U 322 | F 082432 | FPS 0109 | D 5 | Reward:μσmM 2.85 0.28 2.00 2.95 | policy_loss ['None', 'None', '-0.020', '-0.045', '-0.077'] | value_loss ['None', 'None', '0.000', '0.001', '0.002']
U 323 | F 082688 | FPS 0102 | D 8 | Reward:μσmM 2.34 1.25 -1.00 2.96 | policy_loss ['None', 'None', '0.015', '0.005', '0.144'] | value_loss ['None', 'None', '0.001', '0.001', '0.438']
U 324 | F 082944 | FPS 0107 | D 10 | Reward:μσmM 2.88 0.02 2.85 2.91 | policy_loss ['None', 'None', '0.051', '0.074', '0.010'] | value_loss ['None', 'None', '0.001', '0.002', '0.003']
U 325 | F 083200 | FPS 0106 | D 12 | Reward:μσmM 2.70 0.35 2.00 2.90 | policy_loss ['None', 'None', '0.006', '0.021', '0.142'] | value_loss ['None', 'None', '0.001', '0.002', '0.004']
U 326 | F 083456 | FPS 0090 | D 15 | Reward:μσmM 2.62 0.36 2.00 2.86 | policy_loss ['None', 'None', '0.059', '0.050', '0.022'] | value_loss ['None', 'None', '0.002', '0.005', '0.009']
U 327 | F 083712 | FPS 0092 | D 18 | Reward:μσmM 1.54 1.45 -1.00 2.88 | policy_loss ['None', 'None', '0.170', '0.050', '0.505'] | value_loss ['None', 'None', '0.095', '0.005', '1.184']
U 328 | F 083968 | FPS 0089 | D 21 | Reward:μσmM 1.16 1.11 0.00 2.81 | policy_loss ['None', 'None', '0.254', '0.343', '0.336'] | value_loss ['None', 'None', '0.273', '0.492', '0.333']
U 329 | F 084224 | FPS 0085 | D 24 | Reward:μσmM 0.40 0.49 0.00 1.00 | policy_loss ['None', 'None', '0.577', '0.459', '0.308'] | value_loss ['None', 'None', '0.309', '0.531', '0.220']
U 330 | F 084480 | FPS 0088 | D 27 | Reward:μσmM 1.36 1.36 0.00 2.71 | policy_loss ['None', 'None', '-0.264', '0.046', '0.121'] | value_loss ['None', 'None', '0.005', '0.001', '0.009']
U 10 | Test reward:μσmM -0.70 0.46 -1.00 0.00 | Test num frames:μσmM 142.50 88.07 14.00 256.00
Status saved
U 331 | F 084736 | FPS 0115 | D 61 | Reward:μσmM 2.15 0.82 1.00 2.73 | policy_loss ['None', 'None', '-0.078', '-0.001', '-0.017'] | value_loss ['None', 'None', '0.006', '0.025', '0.013']
U 332 | F 084992 | FPS 0111 | D 64 | Reward:μσmM 2.12 1.22 0.00 2.85 | policy_loss ['None', 'None', '-0.177', '-0.268', '-0.020'] | value_loss ['None', 'None', '0.004', '0.012', '0.023']
U 333 | F 085248 | FPS 0078 | D 67 | Reward:μσmM -0.50 0.76 -1.00 1.00 | policy_loss ['None', 'None', '0.980', '0.340', '0.660'] | value_loss ['None', 'None', '1.105', '0.079', '0.815']
U 334 | F 085504 | FPS 0096 | D 70 | Reward:μσmM 0.69 1.70 -1.00 2.74 | policy_loss ['None', 'None', '-0.300', '-0.001', '0.227'] | value_loss ['None', 'None', '0.013', '0.023', '0.572']
U 335 | F 085760 | FPS 0105 | D 72 | Reward:μσmM 2.12 1.23 0.00 2.90 | policy_loss ['None', 'None', '-0.144', '-0.153', '-0.197'] | value_loss ['None', 'None', '0.003', '0.025', '0.057']
U 336 | F 086016 | FPS 0099 | D 75 | Reward:μσmM 2.37 0.79 1.00 2.87 | policy_loss ['None', 'None', '-0.063', '-0.097', '-0.277'] | value_loss ['None', 'None', '0.001', '0.016', '0.055']
U 337 | F 086272 | FPS 0097 | D 77 | Reward:μσmM 2.58 0.71 1.00 2.92 | policy_loss ['None', 'None', '-0.047', '-0.133', '-0.385'] | value_loss ['None', 'None', '0.001', '0.125', '0.072']
U 338 | F 086528 | FPS 0107 | D 80 | Reward:μσmM 2.22 1.12 0.00 2.93 | policy_loss ['None', 'None', '-0.047', '0.113', '-0.309'] | value_loss ['None', 'None', '0.000', '0.350', '0.062']
U 339 | F 086784 | FPS 0107 | D 82 | Reward:μσmM 2.27 1.22 0.00 2.93 | policy_loss ['None', 'None', '-0.024', '-0.171', '-0.202'] | value_loss ['None', 'None', '0.000', '0.337', '0.136']
U 340 | F 087040 | FPS 0104 | D 85 | Reward:μσmM 2.55 0.96 0.00 2.94 | policy_loss ['None', 'None', '0.001', '-0.216', '-0.101'] | value_loss ['None', 'None', '0.000', '0.023', '0.007']
U 10 | Test reward:μσmM -0.10 0.30 -1.00 0.00 | Test num frames:μσmM 230.50 76.50 1.00 256.00
Status saved
U 341 | F 087296 | FPS 0101 | D 138 | Reward:μσmM 2.64 0.67 1.00 2.93 | policy_loss ['None', 'None', '0.008', '-0.050', '-0.118'] | value_loss ['None', 'None', '0.000', '0.004', '0.007']
U 342 | F 087552 | FPS 0105 | D 140 | Reward:μσmM 2.57 0.71 1.00 2.94 | policy_loss ['None', 'None', '0.010', '0.020', '0.002'] | value_loss ['None', 'None', '0.009', '0.004', '0.006']
U 343 | F 087808 | FPS 0105 | D 143 | Reward:μσmM 2.78 0.32 2.00 2.94 | policy_loss ['None', 'None', '-0.020', '0.003', '-0.026'] | value_loss ['None', 'None', '0.002', '0.002', '0.005']
U 344 | F 088064 | FPS 0095 | D 146 | Reward:μσmM 2.55 0.97 0.00 2.93 | policy_loss ['None', 'None', '-0.026', '-0.018', '-0.018'] | value_loss ['None', 'None', '0.001', '0.002', '0.003']
discover.py --task-config task3 --discover 0 --algo ppo --env MiniGrid-ConfigWorld-v0 --lr 0.0001 --AnomalyNN test_8 --model 20240725-seed1 --discount 0.99

Namespace(task_config='task3', discover=0, algo='ppo', env='MiniGrid-ConfigWorld-v0', model='20240725-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_8', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.0001, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Optimizer loaded

Optimizer loaded

U 341 | F 087296 | FPS 0058 | D 4 | Reward:μσmM 2.64 0.67 1.00 2.94 | policy_loss ['None', 'None', '0.001', '-0.039', '-0.073'] | value_loss ['None', 'None', '0.000', '0.006', '0.005']
U 342 | F 087552 | FPS 0077 | D 7 | Reward:μσmM 2.78 0.32 2.00 2.92 | policy_loss ['None', 'None', '0.009', '-0.047', '-0.143'] | value_loss ['None', 'None', '0.000', '0.002', '0.007']
U 343 | F 087808 | FPS 0068 | D 11 | Reward:μσmM 2.55 0.96 0.00 2.93 | policy_loss ['None', 'None', '0.009', '-0.006', '-0.026'] | value_loss ['None', 'None', '0.003', '0.002', '0.002']
U 344 | F 088064 | FPS 0069 | D 15 | Reward:μσmM 2.90 0.02 2.87 2.95 | policy_loss ['None', 'None', '0.029', '0.024', '-0.016'] | value_loss ['None', 'None', '0.002', '0.003', '0.002']
U 345 | F 088320 | FPS 0074 | D 18 | Reward:μσmM 2.58 0.71 1.00 2.93 | policy_loss ['None', 'None', '-0.005', '0.049', '0.083'] | value_loss ['None', 'None', '0.001', '0.006', '0.005']
U 346 | F 088576 | FPS 0079 | D 21 | Reward:μσmM 1.91 1.35 0.00 2.91 | policy_loss ['None', 'None', '0.011', '0.113', '0.255'] | value_loss ['None', 'None', '0.003', '0.325', '0.262']
U 347 | F 088832 | FPS 0077 | D 25 | Reward:μσmM 1.86 1.32 0.00 2.90 | policy_loss ['None', 'None', '0.005', '-0.108', '0.199'] | value_loss ['None', 'None', '0.006', '0.006', '0.031']
U 348 | F 089088 | FPS 0081 | D 28 | Reward:μσmM 1.51 1.67 -1.00 2.88 | policy_loss ['None', 'None', '-0.060', '-0.030', '0.242'] | value_loss ['None', 'None', '0.001', '0.002', '0.486']
U 349 | F 089344 | FPS 0083 | D 31 | Reward:μσmM 1.43 1.75 -1.00 2.91 | policy_loss ['None', 'None', '-0.076', '-0.001', '0.364'] | value_loss ['None', 'None', '0.007', '0.005', '0.920']
U 350 | F 089600 | FPS 0085 | D 34 | Reward:μσmM 1.27 1.83 -1.00 2.91 | policy_loss ['None', 'None', '-0.027', '-0.072', '0.013'] | value_loss ['None', 'None', '0.001', '0.004', '0.663']
U 10 | Test reward:μσmM -0.82 0.54 -1.00 0.80 | Test num frames:μσmM 85.50 66.62 1.00 230.00
Status saved
U 351 | F 089856 | FPS 0089 | D 57 | Reward:μσmM 2.06 1.41 -1.00 2.92 | policy_loss ['None', 'None', '0.004', '-0.040', '-0.216'] | value_loss ['None', 'None', '0.001', '0.003', '0.299']
U 352 | F 090112 | FPS 0100 | D 60 | Reward:μσmM 2.78 0.32 2.00 2.94 | policy_loss ['None', 'None', '-0.037', '-0.068', '-0.222'] | value_loss ['None', 'None', '0.001', '0.001', '0.055']
U 353 | F 090368 | FPS 0096 | D 62 | Reward:μσmM 2.81 0.31 2.00 2.94 | policy_loss ['None', 'None', '-0.014', '-0.053', '-0.131'] | value_loss ['None', 'None', '0.001', '0.001', '0.013']
U 354 | F 090624 | FPS 0104 | D 65 | Reward:μσmM 2.83 0.29 2.00 2.95 | policy_loss ['None', 'None', '-0.017', '-0.039', '-0.122'] | value_loss ['None', 'None', '0.001', '0.001', '0.009']
U 355 | F 090880 | FPS 0111 | D 67 | Reward:μσmM 2.64 0.88 0.00 2.96 | policy_loss ['None', 'None', '0.012', '-0.025', '-0.058'] | value_loss ['None', 'None', '0.001', '0.001', '0.002']
U 356 | F 091136 | FPS 0104 | D 69 | Reward:μσmM 2.71 0.61 1.00 2.95 | policy_loss ['None', 'None', '0.004', '0.014', '0.002'] | value_loss ['None', 'None', '0.002', '0.001', '0.002']
U 357 | F 091392 | FPS 0112 | D 72 | Reward:μσmM 2.81 0.31 2.00 2.94 | policy_loss ['None', 'None', '0.008', '0.027', '0.033'] | value_loss ['None', 'None', '0.001', '0.002', '0.001']
U 358 | F 091648 | FPS 0103 | D 74 | Reward:μσmM 1.54 0.85 1.00 2.90 | policy_loss ['None', 'None', '0.470', '0.427', '0.280'] | value_loss ['None', 'None', '0.592', '0.273', '0.086']
U 359 | F 091904 | FPS 0114 | D 76 | Reward:μσmM 2.53 0.38 2.00 2.81 | policy_loss ['None', 'None', '0.029', '0.020', '0.207'] | value_loss ['None', 'None', '0.001', '0.004', '0.022']
U 360 | F 092160 | FPS 0105 | D 79 | Reward:μσmM 2.11 1.22 0.00 2.87 | policy_loss ['None', 'None', '-0.024', '-0.048', '0.102'] | value_loss ['None', 'None', '0.004', '0.001', '0.015']
U 10 | Test reward:μσmM -0.60 0.49 -1.00 0.00 | Test num frames:μσmM 156.60 95.45 31.00 256.00
Status saved
U 361 | F 092416 | FPS 0076 | D 117 | Reward:μσmM 2.16 0.75 1.00 2.87 | policy_loss ['None', 'None', '0.092', '0.177', '-0.007'] | value_loss ['None', 'None', '0.136', '0.111', '0.019']
U 362 | F 092672 | FPS 0068 | D 121 | Reward:μσmM 1.80 1.28 0.00 2.78 | policy_loss ['None', 'None', '0.031', '0.014', '0.079'] | value_loss ['None', 'None', '0.011', '0.004', '0.004']
U 363 | F 092928 | FPS 0066 | D 125 | Reward:μσmM 2.08 1.42 -1.00 2.89 | policy_loss ['None', 'None', '-0.097', '-0.121', '0.015'] | value_loss ['None', 'None', '0.004', '0.006', '0.489']
U 364 | F 093184 | FPS 0075 | D 128 | Reward:μσmM 2.24 1.45 -1.00 2.91 | policy_loss ['None', 'None', '-0.091', '-0.136', '0.106'] | value_loss ['None', 'None', '0.003', '0.007', '0.627']
U 365 | F 093440 | FPS 0076 | D 131 | Reward:μσmM 1.70 1.35 -1.00 2.93 | policy_loss ['None', 'None', '0.183', '0.154', '-0.126'] | value_loss ['None', 'None', '0.265', '0.175', '0.209']
U 366 | F 093696 | FPS 0063 | D 135 | Reward:μσmM 2.37 0.79 1.00 2.91 | policy_loss ['None', 'None', '0.033', '-0.062', '0.033'] | value_loss ['None', 'None', '0.004', '0.010', '0.020']
U 367 | F 093952 | FPS 0067 | D 139 | Reward:μσmM 1.12 1.66 -1.00 2.81 | policy_loss ['None', 'None', '0.034', '-0.105', '0.240'] | value_loss ['None', 'None', '0.001', '0.002', '0.131']
U 368 | F 094208 | FPS 0068 | D 143 | Reward:μσmM 1.82 1.29 0.00 2.79 | policy_loss ['None', 'None', '0.010', '-0.346', '0.167'] | value_loss ['None', 'None', '0.001', '0.402', '0.019']
U 369 | F 094464 | FPS 0077 | D 146 | Reward:μσmM 1.64 1.19 0.00 2.82 | policy_loss ['None', 'None', '0.193', '-0.109', '0.068'] | value_loss ['None', 'None', '0.190', '0.019', '0.009']
U 370 | F 094720 | FPS 0071 | D 150 | Reward:μσmM 1.91 1.49 -1.00 2.89 | policy_loss ['None', 'None', '-0.142', '-0.148', '-0.020'] | value_loss ['None', 'None', '0.003', '0.003', '0.248']
U 10 | Test reward:μσmM -1.00 0.00 -1.00 -1.00 | Test num frames:μσmM 73.40 47.88 20.00 183.00
Status saved
U 371 | F 094976 | FPS 0070 | D 173 | Reward:μσmM 2.85 0.03 2.83 2.90 | policy_loss ['None', 'None', '-0.043', '-0.101', '-0.177'] | value_loss ['None', 'None', '0.002', '0.002', '0.031']
U 372 | F 095232 | FPS 0073 | D 176 | Reward:μσmM 2.57 0.70 1.00 2.91 | policy_loss ['None', 'None', '-0.064', '-0.055', '-0.263'] | value_loss ['None', 'None', '0.002', '0.002', '0.023']
U 373 | F 095488 | FPS 0077 | D 180 | Reward:μσmM 2.75 0.33 2.00 2.91 | policy_loss ['None', 'None', '-0.042', '-0.020', '-0.096'] | value_loss ['None', 'None', '0.000', '0.002', '0.007']
U 374 | F 095744 | FPS 0071 | D 183 | Reward:μσmM 2.79 0.32 2.00 2.93 | policy_loss ['None', 'None', '0.006', '-0.088', '-0.156'] | value_loss ['None', 'None', '0.003', '0.002', '0.003']
U 375 | F 096000 | FPS 0073 | D 187 | Reward:μσmM 2.58 0.71 1.00 2.94 | policy_loss ['None', 'None', '0.036', '0.002', '-0.039'] | value_loss ['None', 'None', '0.004', '0.004', '0.005']
U 376 | F 096256 | FPS 0073 | D 190 | Reward:μσmM 2.79 0.32 2.00 2.93 | policy_loss ['None', 'None', '-0.033', '0.024', '-0.051'] | value_loss ['None', 'None', '0.002', '0.002', '0.005']
U 377 | F 096512 | FPS 0072 | D 194 | Reward:μσmM 2.81 0.31 2.00 2.95 | policy_loss ['None', 'None', '-0.040', '-0.015', '-0.030'] | value_loss ['None', 'None', '0.001', '0.003', '0.003']
U 378 | F 096768 | FPS 0072 | D 197 | Reward:μσmM 2.81 0.31 2.00 2.95 | policy_loss ['None', 'None', '-0.011', '-0.157', '-0.003'] | value_loss ['None', 'None', '0.000', '0.080', '0.018']
U 379 | F 097024 | FPS 0074 | D 201 | Reward:μσmM 2.78 0.32 2.00 2.93 | policy_loss ['None', 'None', '0.022', '-0.098', '0.029'] | value_loss ['None', 'None', '0.001', '0.001', '0.005']
U 380 | F 097280 | FPS 0065 | D 205 | Reward:μσmM 2.31 1.07 0.00 2.93 | policy_loss ['None', 'None', '0.071', '0.142', '0.033'] | value_loss ['None', 'None', '0.095', '0.148', '0.045']
U 10 | Test reward:μσmM -1.00 0.00 -1.00 -1.00 | Test num frames:μσmM 76.70 56.59 15.00 195.00
Status saved
U 381 | F 097536 | FPS 0069 | D 228 | Reward:μσmM 1.54 1.13 0.00 2.87 | policy_loss ['None', 'None', '0.231', '0.132', '0.159'] | value_loss ['None', 'None', '0.212', '0.094', '0.031']
U 382 | F 097792 | FPS 0068 | D 232 | Reward:μσmM 0.64 1.31 -1.00 2.55 | policy_loss ['None', 'None', '0.091', '0.296', '0.205'] | value_loss ['None', 'None', '0.085', '0.054', '0.167']
U 383 | F 098048 | FPS 0064 | D 236 | Reward:μσmM 1.25 1.62 -1.00 2.75 | policy_loss ['None', 'None', '-0.037', '-0.100', '0.376'] | value_loss ['None', 'None', '0.002', '0.002', '0.337']
U 384 | F 098304 | FPS 0071 | D 240 | Reward:μσmM 1.33 1.55 -1.00 2.82 | policy_loss ['None', 'None', '-0.130', '0.057', '0.246'] | value_loss ['None', 'None', '0.002', '0.258', '0.385']
U 385 | F 098560 | FPS 0063 | D 244 | Reward:μσmM 0.19 1.54 -1.00 2.76 | policy_loss ['None', 'None', '0.032', '-0.136', '0.436'] | value_loss ['None', 'None', '0.000', '0.001', '0.532']
U 386 | F 098816 | FPS 0072 | D 247 | Reward:μσmM 1.92 0.92 1.00 2.87 | policy_loss ['None', 'None', '0.190', '-0.005', '-0.395'] | value_loss ['None', 'None', '0.106', '0.011', '0.084']
U 387 | F 099072 | FPS 0066 | D 251 | Reward:μσmM 1.58 1.83 -1.00 2.92 | policy_loss ['None', 'None', '-0.135', '-0.095', '-0.107'] | value_loss ['None', 'None', '0.002', '0.006', '0.350']
U 388 | F 099328 | FPS 0069 | D 255 | Reward:μσmM 2.41 1.08 0.00 2.92 | policy_loss ['None', 'None', '-0.029', '-0.105', '-0.263'] | value_loss ['None', 'None', '0.001', '0.003', '0.065']
U 389 | F 099584 | FPS 0069 | D 259 | Reward:μσmM 2.79 0.32 2.00 2.94 | policy_loss ['None', 'None', '-0.015', '-0.055', '-0.352'] | value_loss ['None', 'None', '0.001', '0.002', '0.052']
U 390 | F 099840 | FPS 0074 | D 262 | Reward:μσmM 2.55 0.97 0.00 2.95 | policy_loss ['None', 'None', '-0.016', '-0.028', '-0.237'] | value_loss ['None', 'None', '0.001', '0.002', '0.028']
U 10 | Test reward:μσmM -0.70 0.46 -1.00 0.00 | Test num frames:μσmM 166.50 76.01 32.00 256.00
Status saved
U 391 | F 100096 | FPS 0070 | D 310 | Reward:μσmM 2.39 1.23 -1.00 2.95 | policy_loss ['None', 'None', '-0.012', '-0.043', '0.285'] | value_loss ['None', 'None', '0.001', '0.001', '1.078']
U 392 | F 100352 | FPS 0072 | D 314 | Reward:μσmM 2.81 0.31 2.00 2.94 | policy_loss ['None', 'None', '0.011', '0.013', '-0.275'] | value_loss ['None', 'None', '0.001', '0.001', '0.024']
U 393 | F 100608 | FPS 0069 | D 318 | Reward:μσmM 2.55 0.97 0.00 2.95 | policy_loss ['None', 'None', '-0.055', '0.071', '-0.057'] | value_loss ['None', 'None', '0.010', '0.008', '0.006']
U 394 | F 100864 | FPS 0066 | D 322 | Reward:μσmM 2.75 0.33 2.00 2.93 | policy_loss ['None', 'None', '0.004', '0.022', '-0.001'] | value_loss ['None', 'None', '0.002', '0.004', '0.002']
U 395 | F 101120 | FPS 0058 | D 326 | Reward:μσmM 2.19 0.77 1.00 2.90 | policy_loss ['None', 'None', '0.176', '0.037', '0.105'] | value_loss ['None', 'None', '0.083', '0.006', '0.006']
U 396 | F 101376 | FPS 0063 | D 330 | Reward:μσmM 2.49 0.75 1.00 2.90 | policy_loss ['None', 'None', '-0.019', '0.056', '0.063'] | value_loss ['None', 'None', '0.001', '0.007', '0.018']
U 397 | F 101632 | FPS 0065 | D 334 | Reward:μσmM 1.81 1.28 0.00 2.73 | policy_loss ['None', 'None', '0.052', '-0.007', '0.219'] | value_loss ['None', 'None', '0.000', '0.001', '0.022']
U 398 | F 101888 | FPS 0068 | D 338 | Reward:μσmM 2.64 0.37 2.00 2.89 | policy_loss ['None', 'None', '0.021', '-0.008', '-0.104'] | value_loss ['None', 'None', '0.001', '0.003', '0.015']
U 399 | F 102144 | FPS 0067 | D 342 | Reward:μσmM 2.15 0.81 1.00 2.74 | policy_loss ['None', 'None', '0.054', '0.090', '0.020'] | value_loss ['None', 'None', '0.000', '0.005', '0.008']
U 400 | F 102400 | FPS 0066 | D 345 | Reward:μσmM 1.95 0.75 1.00 2.84 | policy_loss ['None', 'None', '0.150', '0.045', '0.040'] | value_loss ['None', 'None', '0.089', '0.001', '0.001']
U 10 | Test reward:μσmM -1.00 0.00 -1.00 -1.00 | Test num frames:μσmM 91.30 66.38 15.00 235.00
Status saved
U 401 | F 102656 | FPS 0067 | D 375 | Reward:μσmM 0.50 1.12 -1.00 2.00 | policy_loss ['None', 'None', '0.282', '0.401', '0.497'] | value_loss ['None', 'None', '0.190', '0.426', '0.757']
U 402 | F 102912 | FPS 0075 | D 378 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', 'None', '0.303', '0.182'] | value_loss ['None', 'None', 'None', '0.164', '0.010']
U 403 | F 103168 | FPS 0066 | D 382 | Reward:μσmM 0.50 0.87 -1.00 1.00 | policy_loss ['None', 'None', '0.530', '0.188', '0.278'] | value_loss ['None', 'None', '0.314', '0.019', '0.357']
U 404 | F 103424 | FPS 0065 | D 386 | Reward:μσmM -0.43 0.73 -1.00 1.00 | policy_loss ['None', 'None', '0.203', '0.434', '0.683'] | value_loss ['None', 'None', '0.063', '0.585', '0.959']
U 405 | F 103680 | FPS 0060 | D 390 | Reward:μσmM 0.67 1.25 -1.00 2.00 | policy_loss ['None', 'None', '0.169', '0.039', '0.187'] | value_loss ['None', 'None', '0.039', '0.001', '0.188']
U 406 | F 103936 | FPS 0060 | D 395 | Reward:μσmM 0.45 1.41 -1.00 2.78 | policy_loss ['None', 'None', '-0.473', '0.531', '0.143'] | value_loss ['None', 'None', '0.093', '0.664', '0.212']
U 407 | F 104192 | FPS 0056 | D 399 | Reward:μσmM 1.00 0.00 1.00 1.00 | policy_loss ['None', 'None', '0.133', '-0.096', '0.016'] | value_loss ['None', 'None', '0.037', '0.007', '0.038']
U 408 | F 104448 | FPS 0071 | D 403 | Reward:μσmM 1.50 1.66 -1.00 2.87 | policy_loss ['None', 'None', '-0.548', '-0.194', '-0.001'] | value_loss ['None', 'None', '0.124', '0.005', '0.258']
U 409 | F 104704 | FPS 0070 | D 406 | Reward:μσmM 1.75 1.62 -1.00 2.91 | policy_loss ['None', 'None', '-0.300', '-0.351', '-0.211'] | value_loss ['None', 'None', '0.043', '0.033', '0.534']
U 410 | F 104960 | FPS 0068 | D 410 | Reward:μσmM 2.88 0.02 2.85 2.91 | policy_loss ['None', 'None', '-0.249', '-0.204', '-0.639'] | value_loss ['None', 'None', '0.018', '0.014', '0.229']
U 10 | Test reward:μσmM -0.90 0.30 -1.00 0.00 | Test num frames:μσmM 115.10 74.05 26.00 256.00
Status saved
U 411 | F 105216 | FPS 0067 | D 447 | Reward:μσmM 2.57 0.70 1.00 2.92 | policy_loss ['None', 'None', '-0.117', '-0.209', '-0.440'] | value_loss ['None', 'None', '0.007', '0.004', '0.075']
U 412 | F 105472 | FPS 0065 | D 451 | Reward:μσmM 2.60 0.92 0.00 2.94 | policy_loss ['None', 'None', '-0.078', '-0.187', '-0.484'] | value_loss ['None', 'None', '0.002', '0.002', '0.071']
U 413 | F 105728 | FPS 0067 | D 454 | Reward:μσmM 2.68 0.63 1.00 2.93 | policy_loss ['None', 'None', '-0.022', '-0.041', '-0.060'] | value_loss ['None', 'None', '0.001', '0.001', '0.007']
U 414 | F 105984 | FPS 0067 | D 458 | Reward:μσmM 2.81 0.31 2.00 2.94 | policy_loss ['None', 'None', '-0.010', '-0.059', '-0.042'] | value_loss ['None', 'None', '0.001', '0.001', '0.004']
U 415 | F 106240 | FPS 0068 | D 462 | Reward:μσmM 2.56 0.97 0.00 2.94 | policy_loss ['None', 'None', '-0.001', '-0.024', '-0.008'] | value_loss ['None', 'None', '0.001', '0.000', '0.006']
U 416 | F 106496 | FPS 0067 | D 466 | Reward:μσmM 2.40 0.80 1.00 2.94 | policy_loss ['None', 'None', '0.263', '0.102', '0.025'] | value_loss ['None', 'None', '0.288', '0.104', '0.050']
U 417 | F 106752 | FPS 0071 | D 469 | Reward:μσmM 2.74 0.33 2.00 2.93 | policy_loss ['None', 'None', '-0.056', '-0.032', '0.110'] | value_loss ['None', 'None', '0.003', '0.017', '0.009']
U 418 | F 107008 | FPS 0068 | D 473 | Reward:μσmM 2.75 0.33 2.00 2.93 | policy_loss ['None', 'None', '-0.033', '-0.116', '0.030'] | value_loss ['None', 'None', '0.002', '0.007', '0.011']
U 419 | F 107264 | FPS 0070 | D 477 | Reward:μσmM 2.31 1.08 0.00 2.95 | policy_loss ['None', 'None', '0.108', '0.083', '-0.042'] | value_loss ['None', 'None', '0.233', '0.095', '0.026']
U 420 | F 107520 | FPS 0069 | D 480 | Reward:μσmM 2.63 0.67 1.00 2.93 | policy_loss ['None', 'None', '-0.119', '0.022', '-0.021'] | value_loss ['None', 'None', '0.002', '0.014', '0.003']
discover.py --task-config task3 --discover 0 --algo ppo --env MiniGrid-ConfigWorld-v0 --lr 0.0001 --AnomalyNN test_8 --model 20240725-seed1 --discount 0.99

Namespace(task_config='task3', discover=0, algo='ppo', env='MiniGrid-ConfigWorld-v0', model='20240725-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_8', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.0001, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Optimizer loaded

Optimizer loaded

U 411 | F 105216 | FPS 0050 | D 5 | Reward:μσmM 2.68 0.64 1.00 2.94 | policy_loss ['None', 'None', '-0.197', '-0.275', '-0.584'] | value_loss ['None', 'None', '0.003', '0.009', '0.160']
U 412 | F 105472 | FPS 0060 | D 9 | Reward:μσmM 2.55 0.96 0.00 2.94 | policy_loss ['None', 'None', '-0.050', '-0.129', '-0.378'] | value_loss ['None', 'None', '0.001', '0.004', '0.063']
U 413 | F 105728 | FPS 0067 | D 13 | Reward:μσmM 2.93 0.01 2.91 2.95 | policy_loss ['None', 'None', '-0.023', '-0.053', '-0.229'] | value_loss ['None', 'None', '0.002', '0.003', '0.014']
U 414 | F 105984 | FPS 0066 | D 17 | Reward:μσmM 2.83 0.29 2.00 2.95 | policy_loss ['None', 'None', '-0.026', '-0.055', '-0.143'] | value_loss ['None', 'None', '0.001', '0.001', '0.008']
U 415 | F 106240 | FPS 0069 | D 20 | Reward:μσmM 2.93 0.01 2.91 2.95 | policy_loss ['None', 'None', '0.004', '-0.020', '-0.038'] | value_loss ['None', 'None', '0.001', '0.000', '0.002']
U 416 | F 106496 | FPS 0068 | D 24 | Reward:μσmM 2.64 0.88 0.00 2.95 | policy_loss ['None', 'None', '-0.010', '0.005', '0.001'] | value_loss ['None', 'None', '0.001', '0.001', '0.001']
U 417 | F 106752 | FPS 0070 | D 28 | Reward:μσmM 2.61 0.64 1.00 2.94 | policy_loss ['None', 'None', '0.069', '0.159', '0.054'] | value_loss ['None', 'None', '0.142', '0.139', '0.025']
U 418 | F 107008 | FPS 0062 | D 32 | Reward:μσmM 2.41 0.82 1.00 2.91 | policy_loss ['None', 'None', '-0.042', '0.029', '0.235'] | value_loss ['None', 'None', '0.001', '0.005', '0.054']
U 419 | F 107264 | FPS 0062 | D 36 | Reward:μσmM 2.65 0.38 2.00 2.88 | policy_loss ['None', 'None', '0.050', '-0.067', '-0.123'] | value_loss ['None', 'None', '0.000', '0.001', '0.036']
U 420 | F 107520 | FPS 0062 | D 40 | Reward:μσmM 2.65 0.37 2.00 2.89 | policy_loss ['None', 'None', '0.055', '0.035', '-0.129'] | value_loss ['None', 'None', '0.001', '0.001', '0.010']
U 10 | Test reward:μσmM -0.90 0.30 -1.00 0.00 | Test num frames:μσmM 41.20 77.64 1.00 256.00
Status saved
U 421 | F 107776 | FPS 0069 | D 52 | Reward:μσmM 1.32 0.71 1.00 2.91 | policy_loss ['None', 'None', '0.618', '0.530', '0.219'] | value_loss ['None', 'None', '0.636', '0.400', '0.047']
