discover.py --task-config task1 --discover 0 --algo ppo --env MiniGrid-ConfigWorld-v0-havekey --lr 0.0001 --AnomalyNN module1016 --model 20240725-seed1 --discount 0.99

Namespace(task_config='task1', discover=0, algo='ppo', env='MiniGrid-ConfigWorld-v0-havekey', model='20240725-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='module1016', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.0001, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

discover.py --task-config task1 --discover 0 --algo ppo --env MiniGrid-ConfigWorld-v0-havekey --lr 0.0001 --AnomalyNN module1016 --model 20240725-seed1 --discount 0.99

Namespace(task_config='task1', discover=0, algo='ppo', env='MiniGrid-ConfigWorld-v0-havekey', model='20240725-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='module1016', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.0001, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

discover.py --task-config task1 --discover 0 --algo ppo --env MiniGrid-ConfigWorld-v0-havekey --lr 0.0001 --AnomalyNN module1016 --model 20240725-seed1 --discount 0.99

Namespace(task_config='task1', discover=0, algo='ppo', env='MiniGrid-ConfigWorld-v0-havekey', model='20240725-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='module1016', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.0001, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

discover.py --task-config task1 --discover 0 --algo ppo --env MiniGrid-ConfigWorld-v0-havekey --lr 0.0001 --AnomalyNN module1016 --model 20240725-seed1 --discount 0.99

Namespace(task_config='task1', discover=0, algo='ppo', env='MiniGrid-ConfigWorld-v0-havekey', model='20240725-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='module1016', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.0001, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

discover.py --task-config task1 --discover 0 --algo ppo --env MiniGrid-ConfigWorld-v0-havekey --lr 0.0001 --AnomalyNN module1016 --model 20240725-seed1 --discount 0.99

Namespace(task_config='task1', discover=0, algo='ppo', env='MiniGrid-ConfigWorld-v0-havekey', model='20240725-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='module1016', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.0001, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

U 1 | F 000256 | FPS 0062 | D 4 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.126'] | value_loss ['None', 'None', '0.052']
U 2 | F 000512 | FPS 0067 | D 7 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.129'] | value_loss ['None', 'None', '0.050']
U 3 | F 000768 | FPS 0082 | D 11 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '0.042'] | value_loss ['None', 'None', '0.000']
U 4 | F 001024 | FPS 0083 | D 14 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.097'] | value_loss ['None', 'None', '0.027']
U 5 | F 001280 | FPS 0080 | D 17 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.073'] | value_loss ['None', 'None', '0.025']
discover.py --task-config task1 --discover 0 --algo ppo --env MiniGrid-ConfigWorld-v0-havekey --lr 0.0001 --AnomalyNN module1016 --model 20240725-seed1 --discount 0.99

Namespace(task_config='task1', discover=0, algo='ppo', env='MiniGrid-ConfigWorld-v0-havekey', model='20240725-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='module1016', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.0001, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.0..255.0].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.0..255.0].
discover.py --task-config task1 --discover 0 --algo ppo --env MiniGrid-ConfigWorld-v0-havekey --lr 0.0001 --AnomalyNN module1016 --model 20240725-seed1 --discount 0.99

Namespace(task_config='task1', discover=0, algo='ppo', env='MiniGrid-ConfigWorld-v0-havekey', model='20240725-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='module1016', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.0001, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

discover.py --task-config task1 --discover 0 --algo ppo --env MiniGrid-ConfigWorld-v0-havekey --lr 0.0001 --AnomalyNN module1016 --model 20240725-seed1 --discount 0.99

Namespace(task_config='task1', discover=0, algo='ppo', env='MiniGrid-ConfigWorld-v0-havekey', model='20240725-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='module1016', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.0001, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

discover.py --task-config task1 --discover 0 --algo ppo --env MiniGrid-ConfigWorld-v0-havekey --lr 0.0001 --AnomalyNN module1016 --model 20240725-seed1 --discount 0.99

Namespace(task_config='task1', discover=0, algo='ppo', env='MiniGrid-ConfigWorld-v0-havekey', model='20240725-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='module1016', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.0001, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

discover.py --task-config task1 --discover 0 --algo ppo --env MiniGrid-ConfigWorld-v0-havekey --lr 0.0001 --AnomalyNN module1016 --model 20240725-seed1 --discount 0.99

Namespace(task_config='task1', discover=0, algo='ppo', env='MiniGrid-ConfigWorld-v0-havekey', model='20240725-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='module1016', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.0001, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

U 1 | F 000256 | FPS 0015 | D 16 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.126'] | value_loss ['None', 'None', '0.052']
U 2 | F 000512 | FPS 0012 | D 37 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.129'] | value_loss ['None', 'None', '0.050']
U 3 | F 000768 | FPS 0011 | D 59 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '0.042'] | value_loss ['None', 'None', '0.000']
U 4 | F 001024 | FPS 0014 | D 78 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.097'] | value_loss ['None', 'None', '0.027']
discover.py --task-config task1 --discover 0 --algo ppo --env MiniGrid-ConfigWorld-v0-havekey --lr 0.0001 --AnomalyNN module1016 --model 20240725-seed1 --discount 0.99

Namespace(task_config='task1', discover=0, algo='ppo', env='MiniGrid-ConfigWorld-v0-havekey', model='20240725-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='module1016', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.0001, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

U 1 | F 000256 | FPS 0067 | D 3 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.126'] | value_loss ['None', 'None', '0.052']
U 2 | F 000512 | FPS 0077 | D 7 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.129'] | value_loss ['None', 'None', '0.050']
U 3 | F 000768 | FPS 0096 | D 9 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '0.042'] | value_loss ['None', 'None', '0.000']
U 4 | F 001024 | FPS 0078 | D 13 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.097'] | value_loss ['None', 'None', '0.027']
U 5 | F 001280 | FPS 0093 | D 15 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.073'] | value_loss ['None', 'None', '0.025']
U 6 | F 001536 | FPS 0077 | D 19 | Reward:μσmM -0.93 0.26 -1.00 0.00 | policy_loss ['None', 'None', '0.454'] | value_loss ['None', 'None', '0.242']
U 7 | F 001792 | FPS 0095 | D 21 | Reward:μσmM -0.75 0.43 -1.00 0.00 | policy_loss ['None', 'None', '0.127'] | value_loss ['None', 'None', '0.047']
U 8 | F 002048 | FPS 0078 | D 25 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.002'] | value_loss ['None', 'None', '0.016']
U 9 | F 002304 | FPS 0078 | D 28 | Reward:μσmM -0.67 0.47 -1.00 0.00 | policy_loss ['None', 'None', '0.020'] | value_loss ['None', 'None', '0.018']
U 10 | F 002560 | FPS 0075 | D 31 | Reward:μσmM -0.67 0.47 -1.00 0.00 | policy_loss ['None', 'None', '0.037'] | value_loss ['None', 'None', '0.028']
U 10 | Test reward:μσmM -0.50 0.50 -1.00 0.00 | Test num frames:μσmM 168.70 98.87 2.00 256.00
Status saved
U 11 | F 002816 | FPS 0093 | D 38 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.087'] | value_loss ['None', 'None', '0.003']
U 12 | F 003072 | FPS 0078 | D 42 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.052'] | value_loss ['None', 'None', '0.000']
U 13 | F 003328 | FPS 0077 | D 45 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.044'] | value_loss ['None', 'None', '0.000']
U 14 | F 003584 | FPS 0077 | D 48 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.038'] | value_loss ['None', 'None', '0.000']
U 15 | F 003840 | FPS 0076 | D 52 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.016'] | value_loss ['None', 'None', '0.008']
U 16 | F 004096 | FPS 0077 | D 55 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.034'] | value_loss ['None', 'None', '0.001']
U 17 | F 004352 | FPS 0094 | D 58 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.030'] | value_loss ['None', 'None', '0.000']
U 18 | F 004608 | FPS 0095 | D 60 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.028'] | value_loss ['None', 'None', '0.021']
U 19 | F 004864 | FPS 0095 | D 63 | Reward:μσmM -0.75 0.43 -1.00 0.00 | policy_loss ['None', 'None', '0.078'] | value_loss ['None', 'None', '0.036']
U 20 | F 005120 | FPS 0094 | D 66 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.043'] | value_loss ['None', 'None', '0.000']
U 10 | Test reward:μσmM -0.60 0.49 -1.00 0.00 | Test num frames:μσmM 162.80 83.58 40.00 256.00
Status saved
U 21 | F 005376 | FPS 0092 | D 73 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.040'] | value_loss ['None', 'None', '0.000']
U 22 | F 005632 | FPS 0094 | D 76 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.030'] | value_loss ['None', 'None', '0.000']
U 23 | F 005888 | FPS 0093 | D 78 | Reward:μσmM -0.75 0.43 -1.00 0.00 | policy_loss ['None', 'None', '0.098'] | value_loss ['None', 'None', '0.036']
U 24 | F 006144 | FPS 0075 | D 82 | Reward:μσmM -0.67 0.47 -1.00 0.00 | policy_loss ['None', 'None', '0.034'] | value_loss ['None', 'None', '0.030']
U 25 | F 006400 | FPS 0079 | D 85 | Reward:μσmM -0.80 0.40 -1.00 0.00 | policy_loss ['None', 'None', '0.110'] | value_loss ['None', 'None', '0.038']
U 26 | F 006656 | FPS 0077 | D 88 | Reward:μσmM -0.67 0.47 -1.00 0.00 | policy_loss ['None', 'None', '0.019'] | value_loss ['None', 'None', '0.023']
U 27 | F 006912 | FPS 0078 | D 91 | Reward:μσmM -0.75 0.43 -1.00 0.00 | policy_loss ['None', 'None', '0.027'] | value_loss ['None', 'None', '0.023']
U 28 | F 007168 | FPS 0072 | D 95 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '-0.041'] | value_loss ['None', 'None', '0.008']
U 29 | F 007424 | FPS 0082 | D 98 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.069'] | value_loss ['None', 'None', '0.001']
U 30 | F 007680 | FPS 0080 | D 101 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '-0.010'] | value_loss ['None', 'None', '0.009']
U 10 | Test reward:μσmM -0.66 0.54 -1.00 0.45 | Test num frames:μσmM 195.40 72.49 50.00 256.00
Status saved
U 31 | F 007936 | FPS 0076 | D 110 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.052'] | value_loss ['None', 'None', '0.000']
U 32 | F 008192 | FPS 0092 | D 113 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.006'] | value_loss ['None', 'None', '0.016']
U 33 | F 008448 | FPS 0092 | D 116 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.004'] | value_loss ['None', 'None', '0.008']
U 34 | F 008704 | FPS 0093 | D 119 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '-0.006'] | value_loss ['None', 'None', '0.017']
U 35 | F 008960 | FPS 0094 | D 122 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '-0.021'] | value_loss ['None', 'None', '0.007']
U 36 | F 009216 | FPS 0077 | D 125 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.044'] | value_loss ['None', 'None', '0.000']
U 37 | F 009472 | FPS 0094 | D 128 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.032'] | value_loss ['None', 'None', '0.000']
U 38 | F 009728 | FPS 0093 | D 130 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.027'] | value_loss ['None', 'None', '0.000']
U 39 | F 009984 | FPS 0093 | D 133 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.022'] | value_loss ['None', 'None', '0.000']
U 40 | F 010240 | FPS 0094 | D 136 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.021'] | value_loss ['None', 'None', '0.000']
U 10 | Test reward:μσmM -0.70 0.46 -1.00 0.00 | Test num frames:μσmM 156.60 90.55 22.00 256.00
Status saved
U 41 | F 010496 | FPS 0092 | D 143 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.034'] | value_loss ['None', 'None', '0.016']
U 42 | F 010752 | FPS 0094 | D 145 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.031'] | value_loss ['None', 'None', '0.000']
U 43 | F 011008 | FPS 0094 | D 148 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.018'] | value_loss ['None', 'None', '0.000']
U 44 | F 011264 | FPS 0094 | D 151 | Reward:μσmM -0.67 0.47 -1.00 0.00 | policy_loss ['None', 'None', '0.090'] | value_loss ['None', 'None', '0.033']
U 45 | F 011520 | FPS 0095 | D 154 | Reward:μσmM -0.75 0.43 -1.00 0.00 | policy_loss ['None', 'None', '0.109'] | value_loss ['None', 'None', '0.041']
U 46 | F 011776 | FPS 0094 | D 156 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '-0.004'] | value_loss ['None', 'None', '0.008']
U 47 | F 012032 | FPS 0094 | D 159 | Reward:μσmM -0.67 0.47 -1.00 0.00 | policy_loss ['None', 'None', '0.022'] | value_loss ['None', 'None', '0.018']
U 48 | F 012288 | FPS 0094 | D 162 | Reward:μσmM -0.67 0.47 -1.00 0.00 | policy_loss ['None', 'None', '0.003'] | value_loss ['None', 'None', '0.025']
U 49 | F 012544 | FPS 0094 | D 164 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.060'] | value_loss ['None', 'None', '0.001']
U 50 | F 012800 | FPS 0092 | D 167 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.060'] | value_loss ['None', 'None', '0.001']
U 10 | Test reward:μσmM -0.24 0.53 -1.00 0.63 | Test num frames:μσmM 178.90 101.75 1.00 256.00
Status saved
U 51 | F 013056 | FPS 0091 | D 175 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.007'] | value_loss ['None', 'None', '0.008']
U 52 | F 013312 | FPS 0076 | D 178 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.034'] | value_loss ['None', 'None', '0.000']
U 53 | F 013568 | FPS 0076 | D 182 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.029'] | value_loss ['None', 'None', '0.000']
U 54 | F 013824 | FPS 0076 | D 185 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.024'] | value_loss ['None', 'None', '0.000']
U 55 | F 014080 | FPS 0076 | D 188 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.019'] | value_loss ['None', 'None', '0.000']
U 56 | F 014336 | FPS 0093 | D 191 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.033'] | value_loss ['None', 'None', '0.017']
U 57 | F 014592 | FPS 0091 | D 194 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.023'] | value_loss ['None', 'None', '0.000']
U 58 | F 014848 | FPS 0092 | D 197 | Reward:μσmM 0.27 0.27 0.00 0.53 | policy_loss ['None', 'None', '-0.075'] | value_loss ['None', 'None', '0.005']
U 59 | F 015104 | FPS 0092 | D 199 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.020'] | value_loss ['None', 'None', '0.000']
U 60 | F 015360 | FPS 0086 | D 202 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.012'] | value_loss ['None', 'None', '0.000']
U 10 | Test reward:μσmM -0.55 0.57 -1.00 0.53 | Test num frames:μσmM 186.70 71.50 64.00 256.00
Status saved
U 61 | F 015616 | FPS 0091 | D 210 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.045'] | value_loss ['None', 'None', '0.019']
U 62 | F 015872 | FPS 0093 | D 213 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.041'] | value_loss ['None', 'None', '0.021']
U 63 | F 016128 | FPS 0092 | D 216 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.045'] | value_loss ['None', 'None', '0.016']
U 64 | F 016384 | FPS 0091 | D 219 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.018'] | value_loss ['None', 'None', '0.001']
U 65 | F 016640 | FPS 0093 | D 221 | Reward:μσmM -0.67 0.47 -1.00 0.00 | policy_loss ['None', 'None', '0.071'] | value_loss ['None', 'None', '0.032']
U 66 | F 016896 | FPS 0093 | D 224 | Reward:μσmM -0.67 0.47 -1.00 0.00 | policy_loss ['None', 'None', '0.072'] | value_loss ['None', 'None', '0.020']
U 67 | F 017152 | FPS 0076 | D 227 | Reward:μσmM -0.67 0.47 -1.00 0.00 | policy_loss ['None', 'None', '0.034'] | value_loss ['None', 'None', '0.023']
U 68 | F 017408 | FPS 0093 | D 230 | Reward:μσmM -0.67 0.47 -1.00 0.00 | policy_loss ['None', 'None', '0.029'] | value_loss ['None', 'None', '0.013']
U 69 | F 017664 | FPS 0093 | D 233 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.005'] | value_loss ['None', 'None', '0.011']
U 70 | F 017920 | FPS 0093 | D 236 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '-0.014'] | value_loss ['None', 'None', '0.011']
U 10 | Test reward:μσmM -0.80 0.40 -1.00 0.00 | Test num frames:μσmM 132.10 77.75 43.00 256.00
Status saved
U 71 | F 018176 | FPS 0076 | D 245 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '-0.028'] | value_loss ['None', 'None', '0.004']
U 72 | F 018432 | FPS 0076 | D 249 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.062'] | value_loss ['None', 'None', '0.002']
U 73 | F 018688 | FPS 0077 | D 252 | Reward:μσmM -0.67 0.47 -1.00 0.00 | policy_loss ['None', 'None', '0.025'] | value_loss ['None', 'None', '0.009']
U 74 | F 018944 | FPS 0089 | D 255 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.057'] | value_loss ['None', 'None', '0.001']
U 75 | F 019200 | FPS 0076 | D 258 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.047'] | value_loss ['None', 'None', '0.000']
U 76 | F 019456 | FPS 0091 | D 261 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.038'] | value_loss ['None', 'None', '0.000']
U 77 | F 019712 | FPS 0093 | D 264 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.031'] | value_loss ['None', 'None', '0.000']
U 78 | F 019968 | FPS 0077 | D 267 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.026'] | value_loss ['None', 'None', '0.000']
U 79 | F 020224 | FPS 0093 | D 270 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.022'] | value_loss ['None', 'None', '0.000']
U 80 | F 020480 | FPS 0092 | D 273 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.025'] | value_loss ['None', 'None', '0.000']
U 10 | Test reward:μσmM -0.80 0.40 -1.00 0.00 | Test num frames:μσmM 163.00 86.59 5.00 256.00
Status saved
U 81 | F 020736 | FPS 0092 | D 280 | Reward:μσmM -0.75 0.43 -1.00 0.00 | policy_loss ['None', 'None', '0.133'] | value_loss ['None', 'None', '0.053']
U 82 | F 020992 | FPS 0077 | D 283 | Reward:μσmM -0.67 0.47 -1.00 0.00 | policy_loss ['None', 'None', '0.001'] | value_loss ['None', 'None', '0.013']
U 83 | F 021248 | FPS 0093 | D 286 | Reward:μσmM -0.67 0.47 -1.00 0.00 | policy_loss ['None', 'None', '0.039'] | value_loss ['None', 'None', '0.017']
U 84 | F 021504 | FPS 0076 | D 289 | Reward:μσmM -0.67 0.47 -1.00 0.00 | policy_loss ['None', 'None', '0.002'] | value_loss ['None', 'None', '0.014']
U 85 | F 021760 | FPS 0093 | D 292 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.068'] | value_loss ['None', 'None', '0.000']
U 86 | F 022016 | FPS 0093 | D 295 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.056'] | value_loss ['None', 'None', '0.000']
U 87 | F 022272 | FPS 0077 | D 298 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.047'] | value_loss ['None', 'None', '0.000']
U 88 | F 022528 | FPS 0092 | D 301 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.040'] | value_loss ['None', 'None', '0.000']
U 89 | F 022784 | FPS 0076 | D 304 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.015'] | value_loss ['None', 'None', '0.017']
U 90 | F 023040 | FPS 0076 | D 307 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.041'] | value_loss ['None', 'None', '0.000']
U 10 | Test reward:μσmM -0.20 0.40 -1.00 0.00 | Test num frames:μσmM 220.30 72.86 45.00 256.00
Status saved
U 91 | F 023296 | FPS 0090 | D 316 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.033'] | value_loss ['None', 'None', '0.000']
U 92 | F 023552 | FPS 0092 | D 319 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.024'] | value_loss ['None', 'None', '0.021']
U 93 | F 023808 | FPS 0093 | D 322 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.016'] | value_loss ['None', 'None', '0.011']
U 94 | F 024064 | FPS 0077 | D 325 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.040'] | value_loss ['None', 'None', '0.001']
U 95 | F 024320 | FPS 0092 | D 328 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.028'] | value_loss ['None', 'None', '0.000']
U 96 | F 024576 | FPS 0076 | D 331 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.025'] | value_loss ['None', 'None', '0.000']
U 97 | F 024832 | FPS 0092 | D 334 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.030'] | value_loss ['None', 'None', '0.022']
U 98 | F 025088 | FPS 0077 | D 337 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.033'] | value_loss ['None', 'None', '0.000']
U 99 | F 025344 | FPS 0076 | D 341 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.022'] | value_loss ['None', 'None', '0.016']
U 100 | F 025600 | FPS 0076 | D 344 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.021'] | value_loss ['None', 'None', '0.000']
U 10 | Test reward:μσmM -0.60 0.49 -1.00 0.00 | Test num frames:μσmM 149.50 94.58 10.00 256.00
Status saved
U 101 | F 025856 | FPS 0091 | D 353 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.032'] | value_loss ['None', 'None', '0.024']
U 102 | F 026112 | FPS 0091 | D 356 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.024'] | value_loss ['None', 'None', '0.019']
U 103 | F 026368 | FPS 0093 | D 358 | Reward:μσmM -0.67 0.47 -1.00 0.00 | policy_loss ['None', 'None', '0.064'] | value_loss ['None', 'None', '0.031']
U 104 | F 026624 | FPS 0092 | D 361 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.001'] | value_loss ['None', 'None', '0.007']
U 105 | F 026880 | FPS 0076 | D 364 | Reward:μσmM -0.83 0.37 -1.00 0.00 | policy_loss ['None', 'None', '0.102'] | value_loss ['None', 'None', '0.042']
U 106 | F 027136 | FPS 0093 | D 367 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.046'] | value_loss ['None', 'None', '0.000']
U 107 | F 027392 | FPS 0093 | D 370 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.036'] | value_loss ['None', 'None', '0.000']
U 108 | F 027648 | FPS 0093 | D 373 | Reward:μσmM -0.67 0.47 -1.00 0.00 | policy_loss ['None', 'None', '0.048'] | value_loss ['None', 'None', '0.022']
U 109 | F 027904 | FPS 0077 | D 376 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.060'] | value_loss ['None', 'None', '0.002']
U 110 | F 028160 | FPS 0093 | D 379 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.039'] | value_loss ['None', 'None', '0.000']
U 10 | Test reward:μσmM -0.65 0.55 -1.00 0.49 | Test num frames:μσmM 127.40 96.04 7.00 256.00
Status saved
U 111 | F 028416 | FPS 0092 | D 385 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.026'] | value_loss ['None', 'None', '0.000']
U 112 | F 028672 | FPS 0093 | D 388 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.022'] | value_loss ['None', 'None', '0.000']
U 113 | F 028928 | FPS 0093 | D 391 | Reward:μσmM -0.75 0.43 -1.00 0.00 | policy_loss ['None', 'None', '0.146'] | value_loss ['None', 'None', '0.047']
U 114 | F 029184 | FPS 0078 | D 394 | Reward:μσmM -0.80 0.40 -1.00 0.00 | policy_loss ['None', 'None', '0.129'] | value_loss ['None', 'None', '0.040']
U 115 | F 029440 | FPS 0090 | D 397 | Reward:μσmM -0.67 0.47 -1.00 0.00 | policy_loss ['None', 'None', '0.040'] | value_loss ['None', 'None', '0.020']
U 116 | F 029696 | FPS 0076 | D 400 | Reward:μσmM 0.41 0.41 0.00 0.83 | policy_loss ['None', 'None', '-0.151'] | value_loss ['None', 'None', '0.024']
U 117 | F 029952 | FPS 0077 | D 403 | Reward:μσmM -0.67 0.47 -1.00 0.00 | policy_loss ['None', 'None', '0.049'] | value_loss ['None', 'None', '0.030']
U 118 | F 030208 | FPS 0077 | D 407 | Reward:μσmM -0.67 0.47 -1.00 0.00 | policy_loss ['None', 'None', '-0.001'] | value_loss ['None', 'None', '0.019']
U 119 | F 030464 | FPS 0077 | D 410 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.058'] | value_loss ['None', 'None', '0.000']
U 120 | F 030720 | FPS 0093 | D 413 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.004'] | value_loss ['None', 'None', '0.011']
U 10 | Test reward:μσmM -0.50 0.50 -1.00 0.00 | Test num frames:μσmM 186.20 79.28 14.00 256.00
Status saved
U 121 | F 030976 | FPS 0091 | D 420 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.044'] | value_loss ['None', 'None', '0.000']
U 122 | F 031232 | FPS 0077 | D 424 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.036'] | value_loss ['None', 'None', '0.000']
U 123 | F 031488 | FPS 0092 | D 426 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.027'] | value_loss ['None', 'None', '0.000']
U 124 | F 031744 | FPS 0092 | D 429 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.023'] | value_loss ['None', 'None', '0.000']
U 125 | F 032000 | FPS 0077 | D 433 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '0.003'] | value_loss ['None', 'None', '0.012']
U 126 | F 032256 | FPS 0076 | D 436 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.032'] | value_loss ['None', 'None', '0.000']
U 127 | F 032512 | FPS 0093 | D 439 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.030'] | value_loss ['None', 'None', '0.000']
U 128 | F 032768 | FPS 0092 | D 442 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.015'] | value_loss ['None', 'None', '0.000']
U 129 | F 033024 | FPS 0093 | D 444 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.013'] | value_loss ['None', 'None', '0.000']
U 130 | F 033280 | FPS 0077 | D 448 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.012'] | value_loss ['None', 'None', '0.000']
U 10 | Test reward:μσmM -0.60 0.49 -1.00 0.00 | Test num frames:μσmM 156.40 97.86 14.00 256.00
Status saved
U 131 | F 033536 | FPS 0090 | D 455 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.010'] | value_loss ['None', 'None', '0.000']
U 132 | F 033792 | FPS 0093 | D 457 | Reward:μσmM -0.67 0.47 -1.00 0.00 | policy_loss ['None', 'None', '0.086'] | value_loss ['None', 'None', '0.037']
U 133 | F 034048 | FPS 0092 | D 460 | Reward:μσmM -0.75 0.43 -1.00 0.00 | policy_loss ['None', 'None', '0.112'] | value_loss ['None', 'None', '0.047']
U 134 | F 034304 | FPS 0077 | D 463 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '-0.013'] | value_loss ['None', 'None', '0.014']
U 135 | F 034560 | FPS 0092 | D 466 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.044'] | value_loss ['None', 'None', '0.000']
U 136 | F 034816 | FPS 0076 | D 470 | Reward:μσmM -0.50 0.50 -1.00 0.00 | policy_loss ['None', 'None', '-0.007'] | value_loss ['None', 'None', '0.012']
U 137 | F 035072 | FPS 0077 | D 473 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.031'] | value_loss ['None', 'None', '0.000']
U 138 | F 035328 | FPS 0091 | D 476 | Reward:μσmM -0.67 0.47 -1.00 0.00 | policy_loss ['None', 'None', '0.023'] | value_loss ['None', 'None', '0.017']
U 139 | F 035584 | FPS 0077 | D 479 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.030'] | value_loss ['None', 'None', '0.000']
U 140 | F 035840 | FPS 0092 | D 482 | Reward:μσmM 0.29 0.29 0.00 0.57 | policy_loss ['None', 'None', '-0.074'] | value_loss ['None', 'None', '0.005']
U 10 | Test reward:μσmM -0.18 0.59 -1.00 0.71 | Test num frames:μσmM 205.00 67.00 89.00 256.00
Status saved
U 141 | F 036096 | FPS 0076 | D 495 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.013'] | value_loss ['None', 'None', '0.000']
U 142 | F 036352 | FPS 0093 | D 497 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.012'] | value_loss ['None', 'None', '0.000']
U 143 | F 036608 | FPS 0091 | D 500 | Reward:μσmM 0.23 0.23 0.00 0.46 | policy_loss ['None', 'None', '-0.044'] | value_loss ['None', 'None', '0.002']
U 144 | F 036864 | FPS 0073 | D 504 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.011'] | value_loss ['None', 'None', '0.000']
U 145 | F 037120 | FPS 0093 | D 506 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.020'] | value_loss ['None', 'None', '0.001']
U 146 | F 037376 | FPS 0087 | D 509 | Reward:μσmM 0.38 0.38 0.00 0.76 | policy_loss ['None', 'None', '-0.057'] | value_loss ['None', 'None', '0.009']
U 147 | F 037632 | FPS 0077 | D 513 | Reward:μσmM 0.68 0.34 0.00 0.91 | policy_loss ['None', 'None', '-0.223'] | value_loss ['None', 'None', '0.014']
U 148 | F 037888 | FPS 0078 | D 516 | Reward:μσmM 0.49 0.68 -1.00 0.90 | policy_loss ['None', 'None', '-0.222'] | value_loss ['None', 'None', '0.021']
U 149 | F 038144 | FPS 0093 | D 519 | Reward:μσmM 0.78 0.32 0.00 0.95 | policy_loss ['None', 'None', '-0.188'] | value_loss ['None', 'None', '0.008']
U 150 | F 038400 | FPS 0093 | D 522 | Reward:μσmM 0.78 0.32 0.00 0.95 | policy_loss ['None', 'None', '-0.106'] | value_loss ['None', 'None', '0.006']
U 10 | Test reward:μσmM 0.92 0.03 0.86 0.96 | Test num frames:μσmM 32.00 13.86 17.00 58.00
Status saved
U 151 | F 038656 | FPS 0077 | D 526 | Reward:μσmM 0.86 0.27 0.00 0.95 | policy_loss ['None', 'None', '-0.083'] | value_loss ['None', 'None', '0.001']
U 152 | F 038912 | FPS 0094 | D 529 | Reward:μσmM 0.86 0.27 0.00 0.96 | policy_loss ['None', 'None', '-0.025'] | value_loss ['None', 'None', '0.004']
U 153 | F 039168 | FPS 0094 | D 532 | Reward:μσmM 0.82 0.29 0.00 0.94 | policy_loss ['None', 'None', '0.011'] | value_loss ['None', 'None', '0.001']
U 154 | F 039424 | FPS 0094 | D 535 | Reward:μσmM 0.82 0.29 0.00 0.95 | policy_loss ['None', 'None', '-0.001'] | value_loss ['None', 'None', '0.001']
U 155 | F 039680 | FPS 0090 | D 537 | Reward:μσmM 0.51 0.69 -1.00 0.94 | policy_loss ['None', 'None', '0.110'] | value_loss ['None', 'None', '0.091']
U 156 | F 039936 | FPS 0093 | D 540 | Reward:μσmM 0.68 0.34 0.00 0.90 | policy_loss ['None', 'None', '0.011'] | value_loss ['None', 'None', '0.001']
U 157 | F 040192 | FPS 0093 | D 543 | Reward:μσmM -0.27 0.86 -1.00 0.88 | policy_loss ['None', 'None', '0.235'] | value_loss ['None', 'None', '0.232']
U 158 | F 040448 | FPS 0077 | D 546 | Reward:μσmM -0.32 0.84 -1.00 0.92 | policy_loss ['None', 'None', '0.221'] | value_loss ['None', 'None', '0.196']
U 159 | F 040704 | FPS 0093 | D 549 | Reward:μσmM -0.08 0.72 -1.00 0.75 | policy_loss ['None', 'None', '0.075'] | value_loss ['None', 'None', '0.053']
U 160 | F 040960 | FPS 0077 | D 552 | Reward:μσmM -0.28 0.78 -1.00 0.88 | policy_loss ['None', 'None', '0.085'] | value_loss ['None', 'None', '0.073']
U 10 | Test reward:μσmM 0.09 0.90 -1.00 0.93 | Test num frames:μσmM 67.80 35.05 30.00 155.00
Status saved
U 161 | F 041216 | FPS 0091 | D 557 | Reward:μσmM 0.31 0.73 -1.00 0.89 | policy_loss ['None', 'None', '-0.089'] | value_loss ['None', 'None', '0.022']
U 162 | F 041472 | FPS 0093 | D 560 | Reward:μσmM 0.63 0.37 0.00 0.90 | policy_loss ['None', 'None', '-0.101'] | value_loss ['None', 'None', '0.014']
U 163 | F 041728 | FPS 0093 | D 562 | Reward:μσmM 0.71 0.36 0.00 0.94 | policy_loss ['None', 'None', '-0.079'] | value_loss ['None', 'None', '0.008']
U 164 | F 041984 | FPS 0093 | D 565 | Reward:μσmM 0.76 0.34 0.00 0.95 | policy_loss ['None', 'None', '-0.093'] | value_loss ['None', 'None', '0.004']
U 165 | F 042240 | FPS 0077 | D 569 | Reward:μσmM 0.78 0.32 0.00 0.96 | policy_loss ['None', 'None', '-0.064'] | value_loss ['None', 'None', '0.006']
U 166 | F 042496 | FPS 0078 | D 572 | Reward:μσmM 0.81 0.31 0.00 0.96 | policy_loss ['None', 'None', '-0.036'] | value_loss ['None', 'None', '0.003']
U 167 | F 042752 | FPS 0094 | D 575 | Reward:μσmM 0.70 0.58 -1.00 0.97 | policy_loss ['None', 'None', '0.003'] | value_loss ['None', 'None', '0.082']
U 168 | F 043008 | FPS 0094 | D 577 | Reward:μσmM 0.84 0.28 0.00 0.96 | policy_loss ['None', 'None', '-0.050'] | value_loss ['None', 'None', '0.004']
U 169 | F 043264 | FPS 0094 | D 580 | Reward:μσmM 0.78 0.32 0.00 0.94 | policy_loss ['None', 'None', '0.005'] | value_loss ['None', 'None', '0.001']
U 170 | F 043520 | FPS 0093 | D 583 | Reward:μσmM 0.81 0.31 0.00 0.94 | policy_loss ['None', 'None', '0.007'] | value_loss ['None', 'None', '0.001']
U 10 | Test reward:μσmM 0.90 0.03 0.85 0.95 | Test num frames:μσmM 44.30 14.06 19.00 63.00
Status saved
discover.py --task-config task1 --discover 1 --algo ppo --env MiniGrid-ConfigWorld-v0-havekey --lr 0.0001 --AnomalyNN module1016 --model 20240725-seed1 --discount 0.99

Namespace(task_config='task1', discover=1, algo='ppo', env='MiniGrid-ConfigWorld-v0-havekey', model='20240725-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='module1016', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.0001, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': np.float64(-0.26418420672416687), 'std': np.float64(0.9014351746899866), 'min': np.float64(-1.0), 'max': np.float64(0.8910526037216187)})
Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': np.float64(0.21544737219810486), 'std': np.float64(0.8362711427919834), 'min': np.float64(-1.0), 'max': np.float64(0.9100000262260437)})
Optimizer loaded

Start discovering in 5000 steps.

discover.py --task-config task1 --discover 1 --algo ppo --env MiniGrid-ConfigWorld-v0-havekey --lr 0.0001 --AnomalyNN module1016 --model 20240725-seed1 --discount 0.99

Namespace(task_config='task1', discover=1, algo='ppo', env='MiniGrid-ConfigWorld-v0-havekey', model='20240725-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='module1016', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.0001, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': np.float64(-0.2599210500717163), 'std': np.float64(0.9068610244154854), 'min': np.float64(-1.0), 'max': np.float64(0.9123684167861938)})
Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': np.float64(-0.16228947639465333), 'std': np.float64(0.8723488579186717), 'min': np.float64(-1.0), 'max': np.float64(0.9455263018608093)})
Optimizer loaded

Start discovering in 5000 steps.

discover.py --task-config task1 --discover 1 --algo ppo --env MiniGrid-ConfigWorld-v0-havekey --lr 0.0001 --AnomalyNN module1016 --model 20240725-seed1 --discount 0.99

Namespace(task_config='task1', discover=1, algo='ppo', env='MiniGrid-ConfigWorld-v0-havekey', model='20240725-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='module1016', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.0001, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': np.float64(0.18347368836402894), 'std': np.float64(0.8113482342750802), 'min': np.float64(-1.0), 'max': np.float64(0.8792105317115784)})
Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': np.float64(0.11639474034309387), 'std': np.float64(0.9122444652319774), 'min': np.float64(-1.0), 'max': np.float64(0.921842098236084)})
Optimizer loaded

Start discovering in 5000 steps.

discover.py --task-config task1 --discover 1 --algo ppo --env MiniGrid-ConfigWorld-v0-havekey --lr 0.0001 --AnomalyNN module1016 --model 20240725-seed1 --discount 0.99

Namespace(task_config='task1', discover=1, algo='ppo', env='MiniGrid-ConfigWorld-v0-havekey', model='20240725-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='module1016', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.0001, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': np.float64(0.10857895016670227), 'std': np.float64(0.9062704804028464), 'min': np.float64(-1.0), 'max': np.float64(0.9549999833106995)})
Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': np.float64(-0.2601578950881958), 'std': np.float64(0.9080265431258857), 'min': np.float64(-1.0), 'max': np.float64(0.9455263018608093)})
Optimizer loaded

Start discovering in 5000 steps.

discover.py --task-config task1 --discover 1 --algo ppo --env MiniGrid-ConfigWorld-v0-havekey --lr 0.0001 --AnomalyNN module1016 --model 20240725-seed1 --discount 0.99

Namespace(task_config='task1', discover=1, algo='ppo', env='MiniGrid-ConfigWorld-v0-havekey', model='20240725-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='module1016', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.0001, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': np.float64(-0.25684210658073425), 'std': np.float64(0.9111317331134303), 'min': np.float64(-1.0), 'max': np.float64(0.9549999833106995)})
Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': np.float64(-0.09615789651870728), 'std': np.float64(0.9054824008674671), 'min': np.float64(-1.0), 'max': np.float64(0.9313157796859741)})
Optimizer loaded

Start discovering in 5000 steps.

discover.py --task-config task1 --discover 1 --algo ppo --env MiniGrid-ConfigWorld-v0-havekey --lr 0.0001 --AnomalyNN module1016 --model 20240725-seed1 --discount 0.99

Namespace(task_config='task1', discover=1, algo='ppo', env='MiniGrid-ConfigWorld-v0-havekey', model='20240725-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='module1016', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.0001, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': np.float64(-0.43765789866447447), 'std': np.float64(0.8590172161192489), 'min': np.float64(-1.0), 'max': np.float64(0.8910526037216187)})
Optimizer loaded

Start discovering in 5000 steps.

discover.py --task-config task1 --discover 1 --algo ppo --env MiniGrid-ConfigWorld-v0-havekey --lr 0.0001 --AnomalyNN module1016 --model 20240725-seed1 --discount 0.99

Namespace(task_config='task1', discover=1, algo='ppo', env='MiniGrid-ConfigWorld-v0-havekey', model='20240725-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='module1016', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.0001, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Test 10 turns results: Start from 2, reward per episode: OrderedDict({'mean': np.float64(-0.438368421792984), 'std': np.float64(0.858558047166051), 'min': np.float64(-1.0), 'max': np.float64(0.926578938961029)})
Optimizer loaded

Start discovering in 5000 steps.

