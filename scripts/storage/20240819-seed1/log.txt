discover.py --algo ppo --env MiniGrid-ConfigWorld-v0-havekey --lr 0.00025 --AnomalyNN test_8 --model 20240819-seed1 --test-interval 10 --task-config task1 --discover 0

Namespace(task_config='task1', discover=0, algo='ppo', env='MiniGrid-ConfigWorld-v0-havekey', model='20240819-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_8', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.00025, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

U 1 | F 000256 | FPS 0042 | D 6 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '0.030'] | value_loss ['None', 'None', '0.000']
U 2 | F 000512 | FPS 0048 | D 11 | Reward:μσmM -0.80 0.40 -1.00 0.00 | policy_loss ['None', 'None', '0.206'] | value_loss ['None', 'None', '0.097']
U 3 | F 000768 | FPS 0051 | D 16 | Reward:μσmM -0.92 0.28 -1.00 0.00 | policy_loss ['None', 'None', '0.277'] | value_loss ['None', 'None', '0.120']
U 4 | F 001024 | FPS 0062 | D 20 | Reward:μσmM 0.00 1.00 -1.00 1.00 | policy_loss ['None', 'None', '-0.132'] | value_loss ['None', 'None', '0.033']
U 5 | F 001280 | FPS 0079 | D 23 | Reward:μσmM 1.00 0.00 1.00 1.00 | policy_loss ['None', 'None', '-0.461'] | value_loss ['None', 'None', '0.176']
U 6 | F 001536 | FPS 0065 | D 27 | Reward:μσmM 0.00 1.00 -1.00 1.00 | policy_loss ['None', 'None', '-0.133'] | value_loss ['None', 'None', '0.107']
U 7 | F 001792 | FPS 0087 | D 30 | Reward:μσmM 1.00 0.00 1.00 1.00 | policy_loss ['None', 'None', '-1.227'] | value_loss ['None', 'None', '0.984']
U 8 | F 002048 | FPS 0085 | D 33 | Reward:μσmM 1.00 0.00 1.00 1.00 | policy_loss ['None', 'None', '-1.197'] | value_loss ['None', 'None', '0.822']
U 9 | F 002304 | FPS 0087 | D 36 | Reward:μσmM 1.00 0.00 1.00 1.00 | policy_loss ['None', 'None', '-0.736'] | value_loss ['None', 'None', '0.268']
U 10 | F 002560 | FPS 0081 | D 39 | Reward:μσmM 1.00 0.00 1.00 1.00 | policy_loss ['None', 'None', '-0.614'] | value_loss ['None', 'None', '0.170']
U 10 | Test reward:μσmM 0.00 0.00 0.00 0.00 | Test num frames:μσmM 242.00 104.74 128.00 380.00
Status saved
U 11 | F 002816 | FPS 0081 | D 53 | Reward:μσmM 1.00 0.00 1.00 1.00 | policy_loss ['None', 'None', '-0.810'] | value_loss ['None', 'None', '1.031']
U 12 | F 003072 | FPS 0088 | D 56 | Reward:μσmM 1.00 0.00 1.00 1.00 | policy_loss ['None', 'None', '-0.450'] | value_loss ['None', 'None', '0.066']
U 13 | F 003328 | FPS 0084 | D 59 | Reward:μσmM 1.00 0.00 1.00 1.00 | policy_loss ['None', 'None', '-0.902'] | value_loss ['None', 'None', '0.807']
U 14 | F 003584 | FPS 0083 | D 62 | Reward:μσmM 1.00 0.00 1.00 1.00 | policy_loss ['None', 'None', '-0.178'] | value_loss ['None', 'None', '0.185']
U 15 | F 003840 | FPS 0087 | D 65 | Reward:μσmM 1.00 0.00 1.00 1.00 | policy_loss ['None', 'None', '-0.138'] | value_loss ['None', 'None', '0.015']
U 16 | F 004096 | FPS 0088 | D 68 | Reward:μσmM 1.00 0.00 1.00 1.00 | policy_loss ['None', 'None', '0.040'] | value_loss ['None', 'None', '0.008']
discover.py --algo ppo --env MiniGrid-ConfigWorld-v0-havekey --lr 0.00025 --AnomalyNN test_8 --model 20240819-seed1 --test-interval 10 --task-config task1 --discover 0

Namespace(task_config='task1', discover=0, algo='ppo', env='MiniGrid-ConfigWorld-v0-havekey', model='20240819-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_8', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.00025, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

U 11 | F 002816 | FPS 0040 | D 6 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '0.493'] | value_loss ['None', 'None', '0.186']
U 12 | F 003072 | FPS 0049 | D 11 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '0.579'] | value_loss ['None', 'None', '0.180']
U 13 | F 003328 | FPS 0053 | D 16 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '0.511'] | value_loss ['None', 'None', '0.192']
U 14 | F 003584 | FPS 0048 | D 21 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '0.363'] | value_loss ['None', 'None', '0.095']
U 15 | F 003840 | FPS 0051 | D 26 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '0.428'] | value_loss ['None', 'None', '0.124']
U 16 | F 004096 | FPS 0053 | D 31 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '0.256'] | value_loss ['None', 'None', '0.206']
U 17 | F 004352 | FPS 0049 | D 36 | Reward:μσmM -0.67 0.47 -1.00 0.00 | policy_loss ['None', 'None', '0.336'] | value_loss ['None', 'None', '0.266']
U 18 | F 004608 | FPS 0052 | D 41 | Reward:μσmM -0.80 0.40 -1.00 0.00 | policy_loss ['None', 'None', '0.470'] | value_loss ['None', 'None', '0.404']
U 19 | F 004864 | FPS 0055 | D 46 | Reward:μσmM -0.88 0.33 -1.00 0.00 | policy_loss ['None', 'None', '0.528'] | value_loss ['None', 'None', '0.443']
U 20 | F 005120 | FPS 0050 | D 51 | Reward:μσmM -0.80 0.40 -1.00 0.00 | policy_loss ['None', 'None', '0.282'] | value_loss ['None', 'None', '0.179']
U 10 | Test reward:μσmM -0.60 0.49 -1.00 0.00 | Test num frames:μσmM 110.10 68.27 8.00 220.00
Status saved
U 21 | F 005376 | FPS 0055 | D 61 | Reward:μσmM -0.75 0.43 -1.00 0.00 | policy_loss ['None', 'None', '0.226'] | value_loss ['None', 'None', '0.108']
U 22 | F 005632 | FPS 0057 | D 65 | Reward:μσmM -0.67 0.47 -1.00 0.00 | policy_loss ['None', 'None', '0.136'] | value_loss ['None', 'None', '0.049']
U 23 | F 005888 | FPS 0058 | D 70 | Reward:μσmM -1.00 0.00 -1.00 -1.00 | policy_loss ['None', 'None', '0.177'] | value_loss ['None', 'None', '0.062']
U 24 | F 006144 | FPS 0057 | D 74 | Reward:μσmM -0.75 0.43 -1.00 0.00 | policy_loss ['None', 'None', '0.100'] | value_loss ['None', 'None', '0.046']
U 25 | F 006400 | FPS 0057 | D 79 | Reward:μσmM -0.80 0.40 -1.00 0.00 | policy_loss ['None', 'None', '0.149'] | value_loss ['None', 'None', '0.054']
U 26 | F 006656 | FPS 0053 | D 84 | Reward:μσmM -0.67 0.47 -1.00 0.00 | policy_loss ['None', 'None', '0.029'] | value_loss ['None', 'None', '0.021']
U 27 | F 006912 | FPS 0052 | D 89 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', '-0.065'] | value_loss ['None', 'None', '0.001']
U 28 | F 007168 | FPS 0054 | D 93 | Reward:μσmM -0.10 0.70 -1.00 0.69 | policy_loss ['None', 'None', '-0.075'] | value_loss ['None', 'None', '0.021']
U 29 | F 007424 | FPS 0055 | D 98 | Reward:μσmM -0.67 0.47 -1.00 0.00 | policy_loss ['None', 'None', '0.043'] | value_loss ['None', 'None', '0.013']
U 30 | F 007680 | FPS 0053 | D 103 | Reward:μσmM 0.32 0.32 0.00 0.63 | policy_loss ['None', 'None', '-0.118'] | value_loss ['None', 'None', '0.010']
U 10 | Test reward:μσmM -0.24 0.61 -1.00 0.93 | Test num frames:μσmM 130.60 79.32 8.00 299.00
Status saved
U 31 | F 007936 | FPS 0055 | D 114 | Reward:μσmM 0.45 0.45 0.00 0.90 | policy_loss ['None', 'None', '-0.109'] | value_loss ['None', 'None', '0.016']
U 32 | F 008192 | FPS 0053 | D 119 | Reward:μσmM 0.43 0.72 -1.00 0.96 | policy_loss ['None', 'None', '-0.165'] | value_loss ['None', 'None', '0.045']
U 33 | F 008448 | FPS 0056 | D 123 | Reward:μσmM 0.92 0.04 0.86 0.97 | policy_loss ['None', 'None', '-0.236'] | value_loss ['None', 'None', '0.023']
U 34 | F 008704 | FPS 0055 | D 128 | Reward:μσmM 0.84 0.28 0.00 0.96 | policy_loss ['None', 'None', '-0.188'] | value_loss ['None', 'None', '0.010']
U 35 | F 008960 | FPS 0056 | D 133 | Reward:μσmM 0.87 0.26 0.00 0.98 | policy_loss ['None', 'None', '-0.093'] | value_loss ['None', 'None', '0.004']
U 36 | F 009216 | FPS 0054 | D 137 | Reward:μσmM 0.87 0.26 0.00 0.97 | policy_loss ['None', 'None', '-0.015'] | value_loss ['None', 'None', '0.003']
U 37 | F 009472 | FPS 0054 | D 142 | Reward:μσmM 0.86 0.27 0.00 0.96 | policy_loss ['None', 'None', '-0.025'] | value_loss ['None', 'None', '0.010']
U 38 | F 009728 | FPS 0055 | D 147 | Reward:μσmM 0.83 0.30 0.00 0.97 | policy_loss ['None', 'None', '0.004'] | value_loss ['None', 'None', '0.003']
U 39 | F 009984 | FPS 0055 | D 151 | Reward:μσmM 0.73 0.56 -1.00 0.98 | policy_loss ['None', 'None', '0.069'] | value_loss ['None', 'None', '0.115']
U 40 | F 010240 | FPS 0055 | D 156 | Reward:μσmM 0.75 0.54 -1.00 0.98 | policy_loss ['None', 'None', '0.011'] | value_loss ['None', 'None', '0.089']
U 10 | Test reward:μσmM 0.16 0.75 -1.00 0.97 | Test num frames:μσmM 94.87 82.24 8.00 299.00
Status saved
U 41 | F 010496 | FPS 0057 | D 162 | Reward:μσmM 0.88 0.25 0.00 0.98 | policy_loss ['None', 'None', '-0.092'] | value_loss ['None', 'None', '0.003']
U 42 | F 010752 | FPS 0055 | D 167 | Reward:μσmM 0.71 0.58 -1.00 0.97 | policy_loss ['None', 'None', '0.037'] | value_loss ['None', 'None', '0.060']
U 43 | F 011008 | FPS 0054 | D 172 | Reward:μσmM 0.90 0.24 0.00 0.97 | policy_loss ['None', 'None', '-0.039'] | value_loss ['None', 'None', '0.001']
discover.py --algo ppo --env MiniGrid-ConfigWorld-v0-havekey --lr 0.00025 --AnomalyNN test_8 --model 20240819-seed1 --test-interval 10 --task-config task1 --discover 0

Namespace(task_config='task1', discover=0, algo='ppo', env='MiniGrid-ConfigWorld-v0-havekey', model='20240819-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_8', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.00025, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

U 41 | F 010496 | FPS 0041 | D 6 | Reward:μσmM 0.83 0.29 0.00 0.97 | policy_loss ['None', 'None', '-0.046'] | value_loss ['None', 'None', '0.005']
U 42 | F 010752 | FPS 0048 | D 11 | Reward:μσmM 0.86 0.29 0.00 0.97 | policy_loss ['None', 'None', '-0.073'] | value_loss ['None', 'None', '0.004']
U 43 | F 011008 | FPS 0057 | D 16 | Reward:μσmM 0.89 0.25 0.00 0.98 | policy_loss ['None', 'None', '-0.060'] | value_loss ['None', 'None', '0.002']
U 44 | F 011264 | FPS 0054 | D 20 | Reward:μσmM 0.78 0.51 -1.00 0.98 | policy_loss ['None', 'None', '0.011'] | value_loss ['None', 'None', '0.047']
U 45 | F 011520 | FPS 0056 | D 25 | Reward:μσmM 0.69 0.60 -1.00 0.97 | policy_loss ['None', 'None', '0.046'] | value_loss ['None', 'None', '0.071']
U 46 | F 011776 | FPS 0057 | D 29 | Reward:μσmM 0.89 0.25 0.00 0.98 | policy_loss ['None', 'None', '-0.056'] | value_loss ['None', 'None', '0.002']
U 47 | F 012032 | FPS 0055 | D 34 | Reward:μσmM 0.88 0.25 0.00 0.98 | policy_loss ['None', 'None', '-0.012'] | value_loss ['None', 'None', '0.002']
U 48 | F 012288 | FPS 0056 | D 39 | Reward:μσmM 0.88 0.26 0.00 0.98 | policy_loss ['None', 'None', '0.005'] | value_loss ['None', 'None', '0.002']
U 49 | F 012544 | FPS 0056 | D 43 | Reward:μσmM 0.88 0.25 0.00 0.97 | policy_loss ['None', 'None', '0.002'] | value_loss ['None', 'None', '0.002']
U 50 | F 012800 | FPS 0057 | D 48 | Reward:μσmM 0.84 0.48 -1.00 0.98 | policy_loss ['None', 'None', '0.031'] | value_loss ['None', 'None', '0.088']
U 10 | Test reward:μσmM 0.95 0.03 0.88 0.97 | Test num frames:μσmM 23.10 11.90 12.00 50.00
Status saved
U 51 | F 013056 | FPS 0054 | D 55 | Reward:μσmM 0.95 0.04 0.83 0.98 | policy_loss ['None', 'None', '0.001'] | value_loss ['None', 'None', '0.004']
U 52 | F 013312 | FPS 0053 | D 59 | Reward:μσmM 0.75 0.58 -1.00 0.98 | policy_loss ['None', 'None', '0.061'] | value_loss ['None', 'None', '0.066']
U 53 | F 013568 | FPS 0056 | D 64 | Reward:μσmM 0.70 0.36 0.00 0.97 | policy_loss ['None', 'None', '0.043'] | value_loss ['None', 'None', '0.005']
U 54 | F 013824 | FPS 0056 | D 68 | Reward:μσmM 0.68 0.36 0.00 0.94 | policy_loss ['None', 'None', '0.021'] | value_loss ['None', 'None', '0.004']
U 55 | F 014080 | FPS 0058 | D 73 | Reward:μσmM 0.84 0.28 0.00 0.97 | policy_loss ['None', 'None', '-0.056'] | value_loss ['None', 'None', '0.002']
U 56 | F 014336 | FPS 0056 | D 77 | Reward:μσmM 0.55 0.74 -1.00 0.98 | policy_loss ['None', 'None', '0.119'] | value_loss ['None', 'None', '0.174']
U 57 | F 014592 | FPS 0053 | D 82 | Reward:μσmM 0.81 0.31 0.00 0.97 | policy_loss ['None', 'None', '-0.047'] | value_loss ['None', 'None', '0.005']
U 58 | F 014848 | FPS 0055 | D 87 | Reward:μσmM 0.83 0.29 0.00 0.97 | policy_loss ['None', 'None', '-0.030'] | value_loss ['None', 'None', '0.003']
U 59 | F 015104 | FPS 0056 | D 91 | Reward:μσmM 0.61 0.70 -1.00 0.97 | policy_loss ['None', 'None', '0.014'] | value_loss ['None', 'None', '0.084']
U 60 | F 015360 | FPS 0056 | D 96 | Reward:μσmM 0.88 0.25 0.00 0.98 | policy_loss ['None', 'None', '-0.046'] | value_loss ['None', 'None', '0.004']
U 10 | Test reward:μσmM 0.95 0.02 0.88 0.98 | Test num frames:μσmM 19.95 9.98 9.00 50.00
Status saved
U 61 | F 015616 | FPS 0056 | D 102 | Reward:μσmM 0.76 0.53 -1.00 0.98 | policy_loss ['None', 'None', '-0.004'] | value_loss ['None', 'None', '0.062']
U 62 | F 015872 | FPS 0055 | D 107 | Reward:μσmM 0.90 0.23 0.00 0.98 | policy_loss ['None', 'None', '-0.075'] | value_loss ['None', 'None', '0.003']
U 63 | F 016128 | FPS 0054 | D 112 | Reward:μσmM 0.91 0.22 0.00 0.98 | policy_loss ['None', 'None', '-0.007'] | value_loss ['None', 'None', '0.001']
U 64 | F 016384 | FPS 0058 | D 116 | Reward:μσmM 0.88 0.25 0.00 0.98 | policy_loss ['None', 'None', '0.024'] | value_loss ['None', 'None', '0.001']
U 65 | F 016640 | FPS 0055 | D 121 | Reward:μσmM 0.15 0.91 -1.00 0.97 | policy_loss ['None', 'None', '0.249'] | value_loss ['None', 'None', '0.251']
U 66 | F 016896 | FPS 0054 | D 126 | Reward:μσmM 0.81 0.31 0.00 0.96 | policy_loss ['None', 'None', '-0.033'] | value_loss ['None', 'None', '0.003']
U 67 | F 017152 | FPS 0054 | D 130 | Reward:μσmM 0.92 0.04 0.82 0.96 | policy_loss ['None', 'None', '-0.001'] | value_loss ['None', 'None', '0.003']
U 68 | F 017408 | FPS 0054 | D 135 | Reward:μσmM 0.68 0.60 -1.00 0.98 | policy_loss ['None', 'None', '0.026'] | value_loss ['None', 'None', '0.076']
U 69 | F 017664 | FPS 0053 | D 140 | Reward:μσmM 0.88 0.25 0.00 0.98 | policy_loss ['None', 'None', '-0.020'] | value_loss ['None', 'None', '0.002']
U 70 | F 017920 | FPS 0052 | D 145 | Reward:μσmM 0.89 0.25 0.00 0.98 | policy_loss ['None', 'None', '-0.027'] | value_loss ['None', 'None', '0.002']
U 10 | Test reward:μσmM 0.96 0.02 0.88 0.98 | Test num frames:μσmM 17.20 9.09 9.00 50.00
Status saved
U 71 | F 018176 | FPS 0051 | D 151 | Reward:μσmM 0.85 0.46 -1.00 0.98 | policy_loss ['None', 'None', '0.034'] | value_loss ['None', 'None', '0.061']
U 72 | F 018432 | FPS 0052 | D 156 | Reward:μσmM 0.82 0.48 -1.00 0.98 | policy_loss ['None', 'None', '0.027'] | value_loss ['None', 'None', '0.022']
U 73 | F 018688 | FPS 0055 | D 161 | Reward:μσmM 0.97 0.01 0.93 0.98 | policy_loss ['None', 'None', '0.012'] | value_loss ['None', 'None', '0.001']
U 74 | F 018944 | FPS 0055 | D 166 | Reward:μσmM 0.64 0.61 -1.00 0.97 | policy_loss ['None', 'None', '0.082'] | value_loss ['None', 'None', '0.032']
U 75 | F 019200 | FPS 0056 | D 170 | Reward:μσmM 0.64 0.67 -1.00 0.95 | policy_loss ['None', 'None', '0.113'] | value_loss ['None', 'None', '0.092']
U 76 | F 019456 | FPS 0055 | D 175 | Reward:μσmM 0.74 0.34 0.00 0.96 | policy_loss ['None', 'None', '-0.015'] | value_loss ['None', 'None', '0.003']
U 77 | F 019712 | FPS 0055 | D 180 | Reward:μσmM 0.46 0.78 -1.00 0.97 | policy_loss ['None', 'None', '0.071'] | value_loss ['None', 'None', '0.100']
U 78 | F 019968 | FPS 0055 | D 184 | Reward:μσmM 0.81 0.31 0.00 0.97 | policy_loss ['None', 'None', '-0.139'] | value_loss ['None', 'None', '0.050']
U 79 | F 020224 | FPS 0054 | D 189 | Reward:μσmM 0.58 0.72 -1.00 0.97 | policy_loss ['None', 'None', '0.066'] | value_loss ['None', 'None', '0.150']
U 80 | F 020480 | FPS 0053 | D 194 | Reward:μσmM 0.80 0.33 0.00 0.97 | policy_loss ['None', 'None', '-0.016'] | value_loss ['None', 'None', '0.007']
U 10 | Test reward:μσmM 0.91 0.31 -1.00 0.98 | Test num frames:μσmM 18.77 9.74 9.00 50.00
Status saved
U 81 | F 020736 | FPS 0058 | D 200 | Reward:μσmM 0.86 0.27 0.00 0.97 | policy_loss ['None', 'None', '-0.049'] | value_loss ['None', 'None', '0.002']
U 82 | F 020992 | FPS 0057 | D 205 | Reward:μσmM 0.89 0.25 0.00 0.97 | policy_loss ['None', 'None', '-0.049'] | value_loss ['None', 'None', '0.002']
U 83 | F 021248 | FPS 0057 | D 209 | Reward:μσmM 0.91 0.23 0.00 0.98 | policy_loss ['None', 'None', '-0.027'] | value_loss ['None', 'None', '0.001']
U 84 | F 021504 | FPS 0059 | D 214 | Reward:μσmM 0.91 0.23 0.00 0.98 | policy_loss ['None', 'None', '0.002'] | value_loss ['None', 'None', '0.001']
U 85 | F 021760 | FPS 0057 | D 218 | Reward:μσmM 0.73 0.56 -1.00 0.97 | policy_loss ['None', 'None', '0.077'] | value_loss ['None', 'None', '0.082']
U 86 | F 022016 | FPS 0055 | D 223 | Reward:μσmM 0.93 0.03 0.88 0.96 | policy_loss ['None', 'None', '-0.000'] | value_loss ['None', 'None', '0.003']
U 87 | F 022272 | FPS 0057 | D 227 | Reward:μσmM 0.82 0.29 0.00 0.96 | policy_loss ['None', 'None', '-0.007'] | value_loss ['None', 'None', '0.003']
U 88 | F 022528 | FPS 0055 | D 232 | Reward:μσmM 0.83 0.29 0.00 0.97 | policy_loss ['None', 'None', '-0.034'] | value_loss ['None', 'None', '0.003']
U 89 | F 022784 | FPS 0058 | D 236 | Reward:μσmM 0.89 0.25 0.00 0.98 | policy_loss ['None', 'None', '-0.051'] | value_loss ['None', 'None', '0.001']
U 90 | F 023040 | FPS 0057 | D 241 | Reward:μσmM 0.90 0.23 0.00 0.98 | policy_loss ['None', 'None', '-0.026'] | value_loss ['None', 'None', '0.001']
U 10 | Test reward:μσmM 0.92 0.27 -1.00 0.98 | Test num frames:μσmM 18.14 8.94 9.00 50.00
Status saved
U 91 | F 023296 | FPS 0056 | D 247 | Reward:μσmM 0.90 0.24 0.00 0.98 | policy_loss ['None', 'None', '0.001'] | value_loss ['None', 'None', '0.001']
U 92 | F 023552 | FPS 0056 | D 252 | Reward:μσmM 0.73 0.56 -1.00 0.97 | policy_loss ['None', 'None', '0.053'] | value_loss ['None', 'None', '0.060']
U 93 | F 023808 | FPS 0054 | D 257 | Reward:μσmM 0.39 0.80 -1.00 0.97 | policy_loss ['None', 'None', '0.159'] | value_loss ['None', 'None', '0.171']
U 94 | F 024064 | FPS 0051 | D 262 | Reward:μσmM 0.30 0.73 -1.00 0.91 | policy_loss ['None', 'None', '0.104'] | value_loss ['None', 'None', '0.066']
U 95 | F 024320 | FPS 0053 | D 267 | Reward:μσmM -0.04 0.88 -1.00 0.95 | policy_loss ['None', 'None', '0.220'] | value_loss ['None', 'None', '0.170']
U 96 | F 024576 | FPS 0055 | D 271 | Reward:μσmM 0.74 0.34 0.00 0.96 | policy_loss ['None', 'None', '-0.064'] | value_loss ['None', 'None', '0.007']
U 97 | F 024832 | FPS 0055 | D 276 | Reward:μσmM 0.86 0.27 0.00 0.97 | policy_loss ['None', 'None', '-0.091'] | value_loss ['None', 'None', '0.004']
U 98 | F 025088 | FPS 0055 | D 281 | Reward:μσmM 0.87 0.26 0.00 0.97 | policy_loss ['None', 'None', '-0.031'] | value_loss ['None', 'None', '0.004']
U 99 | F 025344 | FPS 0058 | D 285 | Reward:μσmM 0.89 0.25 0.00 0.98 | policy_loss ['None', 'None', '-0.045'] | value_loss ['None', 'None', '0.002']
U 100 | F 025600 | FPS 0057 | D 289 | Reward:μσmM 0.91 0.23 0.00 0.98 | policy_loss ['None', 'None', '-0.033'] | value_loss ['None', 'None', '0.001']
U 10 | Test reward:μσmM 0.93 0.25 -1.00 0.98 | Test num frames:μσmM 17.35 8.42 9.00 50.00
Status saved
U 101 | F 025856 | FPS 0056 | D 296 | Reward:μσmM 0.97 0.01 0.95 0.98 | policy_loss ['None', 'None', '-0.011'] | value_loss ['None', 'None', '0.000']
U 102 | F 026112 | FPS 0057 | D 300 | Reward:μσmM 0.86 0.27 0.00 0.98 | policy_loss ['None', 'None', '0.043'] | value_loss ['None', 'None', '0.001']
U 103 | F 026368 | FPS 0059 | D 304 | Reward:μσmM 0.65 0.62 -1.00 0.97 | policy_loss ['None', 'None', '0.044'] | value_loss ['None', 'None', '0.056']
U 104 | F 026624 | FPS 0055 | D 309 | Reward:μσmM 0.61 0.64 -1.00 0.96 | policy_loss ['None', 'None', '0.043'] | value_loss ['None', 'None', '0.058']
U 105 | F 026880 | FPS 0051 | D 314 | Reward:μσmM 0.74 0.34 0.00 0.96 | policy_loss ['None', 'None', '0.009'] | value_loss ['None', 'None', '0.006']
discover.py --algo ppo --env MiniGrid-ConfigWorld-v0-havekey --lr 0.00025 --AnomalyNN test_8 --model 20240819-seed1 --test-interval 10 --task-config task1 --discover 1

Namespace(task_config='task1', discover=1, algo='ppo', env='MiniGrid-ConfigWorld-v0-havekey', model='20240819-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_8', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.00025, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', 0.28394737243652346), ('std', 0.5682998559655851), ('min', -1.0), ('max', 0.9242105484008789)])
Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', 0.45635526776313784), ('std', 0.5002234856991661), ('min', -1.0), ('max', 0.928947389125824)])
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0053 | D 4 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.691 | value -0.721 | policy_loss 0.017 | value_loss 0.018 | grad_norm 0.796
U 2 | F 000512 | FPS 0058 | D 9 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.699 | value -0.646 | policy_loss -0.011 | value_loss 0.014 | grad_norm 0.368
U 3 | F 000768 | FPS 0063 | D 13 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.712 | value -0.536 | policy_loss -0.024 | value_loss 0.013 | grad_norm 0.203
U 4 | F 001024 | FPS 0063 | D 17 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.703 | value -0.638 | policy_loss -0.052 | value_loss 0.010 | grad_norm 0.302
U 5 | F 001280 | FPS 0064 | D 21 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.776 | value -0.587 | policy_loss -0.023 | value_loss 0.011 | grad_norm 0.143
U 6 | F 001536 | FPS 0062 | D 25 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.727 | value -0.549 | policy_loss -0.078 | value_loss 0.001 | grad_norm 0.127
U 7 | F 001792 | FPS 0062 | D 29 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.700 | value -0.491 | policy_loss -0.058 | value_loss 0.004 | grad_norm 0.114
U 8 | F 002048 | FPS 0061 | D 33 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.634 | value -0.550 | policy_loss -0.060 | value_loss 0.005 | grad_norm 0.148
U 9 | F 002304 | FPS 0058 | D 38 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.646 | value -0.466 | policy_loss -0.047 | value_loss 0.005 | grad_norm 0.154
U 10 | F 002560 | FPS 0062 | D 42 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.605 | value -0.392 | policy_loss -0.081 | value_loss 0.001 | grad_norm 0.111
U 11 | F 002816 | FPS 0063 | D 46 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.579 | value -0.327 | policy_loss -0.059 | value_loss 0.000 | grad_norm 0.088
U 12 | F 003072 | FPS 0064 | D 50 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.587 | value -0.295 | policy_loss -0.057 | value_loss 0.000 | grad_norm 0.068
U 13 | F 003328 | FPS 0062 | D 54 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.532 | value -0.272 | policy_loss -0.000 | value_loss 0.006 | grad_norm 0.059
U 14 | F 003584 | FPS 0061 | D 58 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.719 | value -0.236 | policy_loss -0.053 | value_loss 0.000 | grad_norm 0.076
U 15 | F 003840 | FPS 0064 | D 62 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.760 | value -0.217 | policy_loss 0.010 | value_loss 0.009 | grad_norm 0.092
U 16 | F 004096 | FPS 0061 | D 67 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.814 | value -0.209 | policy_loss 0.034 | value_loss 0.021 | grad_norm 0.090
U 17 | F 004352 | FPS 0062 | D 71 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.818 | value -0.294 | policy_loss 0.091 | value_loss 0.040 | grad_norm 0.163
U 18 | F 004608 | FPS 0059 | D 75 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.777 | value -0.242 | policy_loss -0.056 | value_loss 0.000 | grad_norm 0.075
U 19 | F 004864 | FPS 0054 | D 80 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.795 | value -0.263 | policy_loss 0.004 | value_loss 0.011 | grad_norm 0.056
U 20 | F 005120 | FPS 0054 | D 85 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.790 | value -0.295 | policy_loss 0.054 | value_loss 0.013 | grad_norm 0.085
Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', 0.5980175495147705), ('std', 0.45582668776795565), ('min', -1.0), ('max', 0.933684229850769)])
Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', 0.6676052674651146), ('std', 0.4140589239363838), ('min', -1.0), ('max', 0.9668421149253845)])
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0058 | D 4 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.774 | value -0.297 | policy_loss 0.003 | value_loss 0.014 | grad_norm 0.099
U 2 | F 000512 | FPS 0062 | D 8 | Reward:μσmM -0.44 0.79 -1.00 0.67 |  entropy 1.833 | value -0.293 | policy_loss -0.036 | value_loss 0.036 | grad_norm 0.145
U 3 | F 000768 | FPS 0061 | D 12 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.794 | value -0.332 | policy_loss 0.037 | value_loss 0.020 | grad_norm 0.169
U 4 | F 001024 | FPS 0064 | D 16 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.810 | value -0.268 | policy_loss -0.000 | value_loss 0.011 | grad_norm 0.095
U 5 | F 001280 | FPS 0057 | D 21 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.840 | value -0.352 | policy_loss 0.021 | value_loss 0.018 | grad_norm 0.104
U 6 | F 001536 | FPS 0060 | D 25 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.795 | value -0.363 | policy_loss -0.067 | value_loss 0.001 | grad_norm 0.112
U 7 | F 001792 | FPS 0064 | D 29 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.775 | value -0.282 | policy_loss -0.060 | value_loss 0.000 | grad_norm 0.052
U 8 | F 002048 | FPS 0063 | D 33 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.717 | value -0.235 | policy_loss -0.050 | value_loss 0.000 | grad_norm 0.052
U 9 | F 002304 | FPS 0062 | D 37 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.761 | value -0.300 | policy_loss 0.080 | value_loss 0.031 | grad_norm 0.173
U 10 | F 002560 | FPS 0063 | D 41 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.775 | value -0.266 | policy_loss -0.044 | value_loss 0.011 | grad_norm 0.089
U 11 | F 002816 | FPS 0063 | D 45 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.757 | value -0.262 | policy_loss -0.009 | value_loss 0.013 | grad_norm 0.088
U 12 | F 003072 | FPS 0062 | D 50 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.796 | value -0.202 | policy_loss 0.016 | value_loss 0.012 | grad_norm 0.106
U 13 | F 003328 | FPS 0064 | D 54 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.799 | value -0.226 | policy_loss 0.006 | value_loss 0.014 | grad_norm 0.109
U 14 | F 003584 | FPS 0066 | D 57 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.807 | value -0.202 | policy_loss -0.039 | value_loss 0.000 | grad_norm 0.044
U 15 | F 003840 | FPS 0062 | D 62 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.860 | value -0.235 | policy_loss 0.071 | value_loss 0.024 | grad_norm 0.075
U 16 | F 004096 | FPS 0058 | D 66 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.770 | value -0.345 | policy_loss 0.067 | value_loss 0.028 | grad_norm 0.077
U 17 | F 004352 | FPS 0061 | D 70 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.792 | value -0.259 | policy_loss -0.055 | value_loss 0.000 | grad_norm 0.055
U 18 | F 004608 | FPS 0056 | D 75 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.791 | value -0.354 | policy_loss 0.110 | value_loss 0.032 | grad_norm 0.094
U 19 | F 004864 | FPS 0063 | D 79 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.769 | value -0.307 | policy_loss -0.035 | value_loss 0.008 | grad_norm 0.073
discover.py --algo ppo --env MiniGrid-ConfigWorld-v0-havekey --lr 0.00025 --AnomalyNN test_8 --model 20240819-seed1 --test-interval 10 --task-config task1 --discover 1

Namespace(task_config='task1', discover=1, algo='ppo', env='MiniGrid-ConfigWorld-v0-havekey', model='20240819-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_8', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.00025, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', 0.28394737243652346), ('std', 0.5682998559655851), ('min', -1.0), ('max', 0.9242105484008789)])
Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', 0.45635526776313784), ('std', 0.5002234856991661), ('min', -1.0), ('max', 0.928947389125824)])
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0048 | D 5 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.684 | value -0.671 | policy_loss -0.014 | value_loss 0.018 | grad_norm 0.989
U 2 | F 000512 | FPS 0056 | D 9 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.525 | value -0.725 | policy_loss -0.019 | value_loss 0.010 | grad_norm 0.240
U 3 | F 000768 | FPS 0060 | D 14 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.540 | value -0.674 | policy_loss -0.069 | value_loss 0.007 | grad_norm 0.203
U 4 | F 001024 | FPS 0062 | D 18 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.437 | value -0.630 | policy_loss -0.097 | value_loss 0.002 | grad_norm 0.134
U 5 | F 001280 | FPS 0061 | D 22 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.299 | value -0.497 | policy_loss -0.100 | value_loss 0.000 | grad_norm 0.122
U 6 | F 001536 | FPS 0062 | D 26 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.356 | value -0.479 | policy_loss -0.059 | value_loss 0.002 | grad_norm 0.122
U 7 | F 001792 | FPS 0061 | D 30 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.264 | value -0.359 | policy_loss -0.077 | value_loss 0.000 | grad_norm 0.123
U 8 | F 002048 | FPS 0056 | D 35 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.193 | value -0.296 | policy_loss -0.055 | value_loss 0.000 | grad_norm 0.081
U 9 | F 002304 | FPS 0053 | D 40 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.180 | value -0.241 | policy_loss -0.047 | value_loss 0.000 | grad_norm 0.071
U 10 | F 002560 | FPS 0059 | D 44 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.278 | value -0.206 | policy_loss -0.039 | value_loss 0.000 | grad_norm 0.049
U 11 | F 002816 | FPS 0058 | D 48 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.464 | value -0.175 | policy_loss -0.031 | value_loss 0.000 | grad_norm 0.046
U 12 | F 003072 | FPS 0059 | D 53 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.558 | value -0.144 | policy_loss -0.028 | value_loss 0.000 | grad_norm 0.025
U 13 | F 003328 | FPS 0061 | D 57 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.705 | value -0.162 | policy_loss 0.027 | value_loss 0.009 | grad_norm 0.022
U 14 | F 003584 | FPS 0058 | D 62 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.808 | value -0.242 | policy_loss 0.067 | value_loss 0.018 | grad_norm 0.097
U 15 | F 003840 | FPS 0058 | D 66 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.793 | value -0.359 | policy_loss 0.178 | value_loss 0.044 | grad_norm 0.137
U 16 | F 004096 | FPS 0058 | D 70 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.768 | value -0.537 | policy_loss 0.267 | value_loss 0.075 | grad_norm 0.194
U 17 | F 004352 | FPS 0061 | D 75 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.718 | value -0.555 | policy_loss -0.000 | value_loss 0.015 | grad_norm 0.182
U 18 | F 004608 | FPS 0064 | D 79 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.692 | value -0.468 | policy_loss -0.097 | value_loss 0.001 | grad_norm 0.111
U 19 | F 004864 | FPS 0064 | D 83 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.657 | value -0.426 | policy_loss -0.045 | value_loss 0.007 | grad_norm 0.135
U 20 | F 005120 | FPS 0066 | D 86 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.703 | value -0.339 | policy_loss -0.071 | value_loss 0.000 | grad_norm 0.093
Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', 0.5980175495147705), ('std', 0.45582668776795565), ('min', -1.0), ('max', 0.933684229850769)])
Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', 0.6651184260845184), ('std', 0.4124978566216716), ('min', -1.0), ('max', 0.933684229850769)])
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0057 | D 4 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.742 | value -0.295 | policy_loss -0.058 | value_loss 0.000 | grad_norm 0.065
U 2 | F 000512 | FPS 0059 | D 8 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.645 | value -0.256 | policy_loss -0.054 | value_loss 0.000 | grad_norm 0.051
U 3 | F 000768 | FPS 0065 | D 12 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.692 | value -0.259 | policy_loss -0.001 | value_loss 0.010 | grad_norm 0.056
U 4 | F 001024 | FPS 0065 | D 16 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.645 | value -0.301 | policy_loss -0.012 | value_loss 0.015 | grad_norm 0.148
U 5 | F 001280 | FPS 0065 | D 20 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.641 | value -0.260 | policy_loss -0.053 | value_loss 0.000 | grad_norm 0.089
U 6 | F 001536 | FPS 0065 | D 24 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.785 | value -0.305 | policy_loss 0.076 | value_loss 0.040 | grad_norm 0.111
U 7 | F 001792 | FPS 0063 | D 28 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.601 | value -0.287 | policy_loss -0.058 | value_loss 0.000 | grad_norm 0.111
U 8 | F 002048 | FPS 0061 | D 32 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.698 | value -0.291 | policy_loss -0.004 | value_loss 0.012 | grad_norm 0.150
U 9 | F 002304 | FPS 0063 | D 36 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.677 | value -0.295 | policy_loss -0.010 | value_loss 0.009 | grad_norm 0.077
U 10 | F 002560 | FPS 0062 | D 41 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.677 | value -0.238 | policy_loss -0.005 | value_loss 0.006 | grad_norm 0.095
U 11 | F 002816 | FPS 0061 | D 45 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.757 | value -0.239 | policy_loss -0.066 | value_loss 0.002 | grad_norm 0.079
U 12 | F 003072 | FPS 0063 | D 49 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.738 | value -0.182 | policy_loss -0.042 | value_loss 0.000 | grad_norm 0.053
U 13 | F 003328 | FPS 0062 | D 53 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.770 | value -0.157 | policy_loss -0.034 | value_loss 0.000 | grad_norm 0.037
U 14 | F 003584 | FPS 0062 | D 57 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.746 | value -0.129 | policy_loss -0.026 | value_loss 0.000 | grad_norm 0.022
U 15 | F 003840 | FPS 0064 | D 61 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.793 | value -0.114 | policy_loss -0.023 | value_loss 0.000 | grad_norm 0.014
U 16 | F 004096 | FPS 0058 | D 66 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.810 | value -0.175 | policy_loss 0.076 | value_loss 0.026 | grad_norm 0.093
U 17 | F 004352 | FPS 0060 | D 70 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.864 | value -0.156 | policy_loss 0.025 | value_loss 0.022 | grad_norm 0.028
U 18 | F 004608 | FPS 0058 | D 74 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.840 | value -0.154 | policy_loss -0.030 | value_loss 0.000 | grad_norm 0.049
U 19 | F 004864 | FPS 0062 | D 78 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.823 | value -0.172 | policy_loss 0.013 | value_loss 0.014 | grad_norm 0.052
U 20 | F 005120 | FPS 0064 | D 82 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.775 | value -0.355 | policy_loss -0.079 | value_loss 0.017 | grad_norm 0.180
Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', 0.666326321363449), ('std', 0.44606708419069946), ('min', -1.0), ('max', 0.9502631425857544)])
Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', 0.6951359699169795), ('std', 0.4130091558630231), ('min', -1.0), ('max', 0.9502631425857544)])
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0058 | D 4 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.817 | value -0.249 | policy_loss 0.006 | value_loss 0.016 | grad_norm 0.074
U 2 | F 000512 | FPS 0060 | D 8 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.831 | value -0.231 | policy_loss -0.005 | value_loss 0.019 | grad_norm 0.090
U 3 | F 000768 | FPS 0057 | D 13 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.840 | value -0.292 | policy_loss 0.073 | value_loss 0.036 | grad_norm 0.111
U 4 | F 001024 | FPS 0059 | D 17 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.778 | value -0.244 | policy_loss -0.009 | value_loss 0.009 | grad_norm 0.100
U 5 | F 001280 | FPS 0061 | D 21 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.824 | value -0.244 | policy_loss 0.008 | value_loss 0.017 | grad_norm 0.043
U 6 | F 001536 | FPS 0059 | D 26 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.847 | value -0.232 | policy_loss 0.020 | value_loss 0.017 | grad_norm 0.104
U 7 | F 001792 | FPS 0059 | D 30 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.810 | value -0.268 | policy_loss 0.024 | value_loss 0.028 | grad_norm 0.117
U 8 | F 002048 | FPS 0061 | D 34 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.777 | value -0.292 | policy_loss 0.005 | value_loss 0.018 | grad_norm 0.081
U 9 | F 002304 | FPS 0059 | D 38 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.767 | value -0.262 | policy_loss -0.064 | value_loss 0.001 | grad_norm 0.117
U 10 | F 002560 | FPS 0059 | D 43 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.784 | value -0.255 | policy_loss -0.001 | value_loss 0.006 | grad_norm 0.055
U 11 | F 002816 | FPS 0060 | D 47 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.776 | value -0.270 | policy_loss 0.002 | value_loss 0.013 | grad_norm 0.090
U 12 | F 003072 | FPS 0062 | D 51 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.779 | value -0.261 | policy_loss 0.063 | value_loss 0.021 | grad_norm 0.029
U 13 | F 003328 | FPS 0062 | D 55 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.809 | value -0.270 | policy_loss 0.019 | value_loss 0.008 | grad_norm 0.075
U 14 | F 003584 | FPS 0063 | D 59 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.813 | value -0.270 | policy_loss 0.001 | value_loss 0.006 | grad_norm 0.072
U 15 | F 003840 | FPS 0056 | D 64 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.801 | value -0.222 | policy_loss -0.052 | value_loss 0.001 | grad_norm 0.033
U 16 | F 004096 | FPS 0061 | D 68 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.830 | value -0.269 | policy_loss 0.065 | value_loss 0.028 | grad_norm 0.087
U 17 | F 004352 | FPS 0065 | D 72 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.828 | value -0.257 | policy_loss 0.028 | value_loss 0.016 | grad_norm 0.039
U 18 | F 004608 | FPS 0062 | D 76 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.775 | value -0.316 | policy_loss 0.097 | value_loss 0.035 | grad_norm 0.086
U 19 | F 004864 | FPS 0062 | D 80 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.734 | value -0.279 | policy_loss -0.067 | value_loss 0.001 | grad_norm 0.082
U 20 | F 005120 | FPS 0059 | D 85 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.704 | value -0.401 | policy_loss -0.040 | value_loss 0.009 | grad_norm 0.119
Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', 0.6842330877270018), ('std', 0.4047566563642013), ('min', -1.0), ('max', 0.9573684334754944)])
Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', 0.6973486881703138), ('std', 0.39205400200125684), ('min', -1.0), ('max', 0.9573684334754944)])
Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', 0.715239769551489), ('std', 0.37355798474985036), ('min', -1.0), ('max', 0.9573684334754944)])
Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', 0.7218973717093468), ('std', 0.3654878246738223), ('min', -1.0), ('max', 0.9573684334754944)])
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0060 | D 4 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.757 | value -0.361 | policy_loss -0.020 | value_loss 0.016 | grad_norm 0.104
U 2 | F 000512 | FPS 0063 | D 8 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.747 | value -0.254 | policy_loss -0.047 | value_loss 0.001 | grad_norm 0.074
U 3 | F 000768 | FPS 0061 | D 12 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.634 | value -0.407 | policy_loss 0.035 | value_loss 0.020 | grad_norm 0.114
U 4 | F 001024 | FPS 0056 | D 17 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.737 | value -0.380 | policy_loss 0.019 | value_loss 0.012 | grad_norm 0.058
U 5 | F 001280 | FPS 0059 | D 21 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.707 | value -0.359 | policy_loss -0.076 | value_loss 0.002 | grad_norm 0.100
U 6 | F 001536 | FPS 0057 | D 25 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.742 | value -0.361 | policy_loss -0.024 | value_loss 0.003 | grad_norm 0.050
U 7 | F 001792 | FPS 0056 | D 30 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.673 | value -0.278 | policy_loss -0.053 | value_loss 0.002 | grad_norm 0.058
U 8 | F 002048 | FPS 0057 | D 35 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.723 | value -0.238 | policy_loss -0.053 | value_loss 0.000 | grad_norm 0.055
U 9 | F 002304 | FPS 0057 | D 39 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.636 | value -0.202 | policy_loss -0.042 | value_loss 0.000 | grad_norm 0.059
U 10 | F 002560 | FPS 0057 | D 44 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.634 | value -0.169 | policy_loss -0.034 | value_loss 0.000 | grad_norm 0.034
U 11 | F 002816 | FPS 0055 | D 48 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.611 | value -0.142 | policy_loss -0.030 | value_loss 0.000 | grad_norm 0.034
U 12 | F 003072 | FPS 0059 | D 53 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.749 | value -0.176 | policy_loss 0.027 | value_loss 0.017 | grad_norm 0.037
U 13 | F 003328 | FPS 0059 | D 57 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.793 | value -0.283 | policy_loss 0.136 | value_loss 0.060 | grad_norm 0.057
U 14 | F 003584 | FPS 0062 | D 61 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.674 | value -0.303 | policy_loss -0.060 | value_loss 0.000 | grad_norm 0.120
U 15 | F 003840 | FPS 0063 | D 65 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.723 | value -0.249 | policy_loss -0.025 | value_loss 0.009 | grad_norm 0.047
U 16 | F 004096 | FPS 0060 | D 69 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.806 | value -0.327 | policy_loss 0.072 | value_loss 0.033 | grad_norm 0.062
U 17 | F 004352 | FPS 0062 | D 73 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.786 | value -0.351 | policy_loss 0.023 | value_loss 0.024 | grad_norm 0.071
U 18 | F 004608 | FPS 0062 | D 78 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.758 | value -0.223 | policy_loss -0.048 | value_loss 0.000 | grad_norm 0.028
U 19 | F 004864 | FPS 0064 | D 82 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.743 | value -0.189 | policy_loss -0.038 | value_loss 0.000 | grad_norm 0.045
U 20 | F 005120 | FPS 0065 | D 86 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.756 | value -0.164 | policy_loss -0.039 | value_loss 0.000 | grad_norm 0.032
Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', 0.729282299767841), ('std', 0.35899601577914875), ('min', -1.0), ('max', 0.9573684334754944)])
Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', 0.7320416701336702), ('std', 0.35235860498129495), ('min', -1.0), ('max', 0.9573684334754944)])
Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', 0.7343947403706037), ('std', 0.3464625199120759), ('min', -1.0), ('max', 0.9573684334754944)])
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0063 | D 4 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.789 | value -0.138 | policy_loss -0.028 | value_loss 0.000 | grad_norm 0.022
U 2 | F 000512 | FPS 0062 | D 8 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.805 | value -0.179 | policy_loss 0.023 | value_loss 0.014 | grad_norm 0.045
U 3 | F 000768 | FPS 0065 | D 12 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.857 | value -0.185 | policy_loss 0.047 | value_loss 0.032 | grad_norm 0.029
U 4 | F 001024 | FPS 0063 | D 16 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.831 | value -0.252 | policy_loss 0.078 | value_loss 0.039 | grad_norm 0.078
U 5 | F 001280 | FPS 0058 | D 20 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.776 | value -0.254 | policy_loss -0.005 | value_loss 0.008 | grad_norm 0.042
U 6 | F 001536 | FPS 0054 | D 25 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.720 | value -0.340 | policy_loss 0.104 | value_loss 0.035 | grad_norm 0.074
U 7 | F 001792 | FPS 0059 | D 29 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.691 | value -0.350 | policy_loss -0.072 | value_loss 0.002 | grad_norm 0.097
U 8 | F 002048 | FPS 0060 | D 33 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.510 | value -0.412 | policy_loss -0.071 | value_loss 0.000 | grad_norm 0.128
U 9 | F 002304 | FPS 0062 | D 38 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.783 | value -0.321 | policy_loss -0.014 | value_loss 0.013 | grad_norm 0.079
U 10 | F 002560 | FPS 0063 | D 42 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.758 | value -0.331 | policy_loss 0.040 | value_loss 0.025 | grad_norm 0.093
U 11 | F 002816 | FPS 0063 | D 46 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.712 | value -0.286 | policy_loss -0.052 | value_loss 0.001 | grad_norm 0.072
U 12 | F 003072 | FPS 0062 | D 50 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.729 | value -0.243 | policy_loss -0.044 | value_loss 0.000 | grad_norm 0.035
U 13 | F 003328 | FPS 0064 | D 54 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.739 | value -0.218 | policy_loss -0.011 | value_loss 0.006 | grad_norm 0.033
U 14 | F 003584 | FPS 0061 | D 58 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.775 | value -0.213 | policy_loss 0.004 | value_loss 0.016 | grad_norm 0.053
U 15 | F 003840 | FPS 0062 | D 62 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.781 | value -0.179 | policy_loss -0.051 | value_loss 0.001 | grad_norm 0.024
U 16 | F 004096 | FPS 0062 | D 66 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.720 | value -0.245 | policy_loss -0.055 | value_loss 0.001 | grad_norm 0.099
U 17 | F 004352 | FPS 0065 | D 70 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.807 | value -0.135 | policy_loss -0.031 | value_loss 0.000 | grad_norm 0.024
U 18 | F 004608 | FPS 0062 | D 74 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.821 | value -0.115 | policy_loss -0.022 | value_loss 0.000 | grad_norm 0.014
U 19 | F 004864 | FPS 0064 | D 78 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.819 | value -0.106 | policy_loss -0.023 | value_loss 0.000 | grad_norm 0.021
U 20 | F 005120 | FPS 0063 | D 82 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.834 | value -0.134 | policy_loss 0.021 | value_loss 0.008 | grad_norm 0.020
Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', 0.7469210562961442), ('std', 0.33709251340013696), ('min', -1.0), ('max', 0.9573684334754944)])
Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', 0.7455526353915533), ('std', 0.33278297324014794), ('min', -1.0), ('max', 0.9573684334754944)])
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0059 | D 4 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.850 | value -0.087 | policy_loss -0.019 | value_loss 0.000 | grad_norm 0.032
U 2 | F 000512 | FPS 0061 | D 8 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.883 | value -0.070 | policy_loss -0.015 | value_loss 0.000 | grad_norm 0.008
U 3 | F 000768 | FPS 0061 | D 12 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.680 | value -0.240 | policy_loss -0.060 | value_loss 0.002 | grad_norm 0.099
U 4 | F 001024 | FPS 0064 | D 16 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.818 | value -0.171 | policy_loss 0.023 | value_loss 0.015 | grad_norm 0.022
U 5 | F 001280 | FPS 0060 | D 20 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.835 | value -0.160 | policy_loss 0.016 | value_loss 0.021 | grad_norm 0.048
U 6 | F 001536 | FPS 0058 | D 25 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.817 | value -0.252 | policy_loss 0.056 | value_loss 0.026 | grad_norm 0.078
U 7 | F 001792 | FPS 0060 | D 29 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.805 | value -0.243 | policy_loss -0.005 | value_loss 0.013 | grad_norm 0.074
U 8 | F 002048 | FPS 0064 | D 33 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.831 | value -0.227 | policy_loss 0.003 | value_loss 0.013 | grad_norm 0.041
discover.py --algo ppo --env MiniGrid-ConfigWorld-v0-havekey --lr 0.00025 --AnomalyNN test_8 --model 20240819-seed1 --test-interval 10 --task-config task1 --discover 1

Namespace(task_config='task1', discover=1, algo='ppo', env='MiniGrid-ConfigWorld-v0-havekey', model='20240819-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_8', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.00025, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

discover.py --algo ppo --env MiniGrid-ConfigWorld-v0-havekey --lr 0.00025 --AnomalyNN test_8 --model 20240819-seed1 --test-interval 10 --task-config task1 --discover 1

Namespace(task_config='task1', discover=1, algo='ppo', env='MiniGrid-ConfigWorld-v0-havekey', model='20240819-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_8', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.00025, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

discover.py --algo ppo --env MiniGrid-ConfigWorld-v0-havekey --lr 0.00025 --AnomalyNN test_8 --model 20240819-seed1 --test-interval 10 --task-config task1 --discover 1

Namespace(task_config='task1', discover=1, algo='ppo', env='MiniGrid-ConfigWorld-v0-havekey', model='20240819-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_8', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.00025, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', 0.28394737243652346), ('std', 0.5682998559655851), ('min', -1.0), ('max', 0.9242105484008789)])
Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', 0.6287631630897522), ('std', 0.34355986415417794), ('min', 0.0), ('max', 0.928947389125824)])
successful test! Start from: 2, reward per episode: OrderedDict([('mean', 0.852921062707901), ('std', 0.06674116668972474), ('min', 0.706315815448761), ('max', 0.9147368669509888)])
discover.py --algo ppo --env MiniGrid-ConfigWorld-v0-havekey --lr 0.00025 --AnomalyNN test_8 --model 20240819-seed1 --test-interval 10 --task-config task1 --discover 1

Namespace(task_config='task1', discover=1, algo='ppo', env='MiniGrid-ConfigWorld-v0-havekey', model='20240819-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_8', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.00025, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', 0.28394737243652346), ('std', 0.5682998559655851), ('min', -1.0), ('max', 0.9242105484008789)])
Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', 0.6287631630897522), ('std', 0.34355986415417794), ('min', 0.0), ('max', 0.928947389125824)])
successful test! Start from: 2, reward per episode: OrderedDict([('mean', 0.852921062707901), ('std', 0.06674116668972474), ('min', 0.706315815448761), ('max', 0.9147368669509888)])
discover.py --algo ppo --env MiniGrid-ConfigWorld-v0-havekey --lr 0.00025 --AnomalyNN test_8 --model 20240819-seed1 --test-interval 10 --task-config task1 --discover 1

Namespace(task_config='task1', discover=1, algo='ppo', env='MiniGrid-ConfigWorld-v0-havekey', model='20240819-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_8', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.00025, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

discover.py --algo ppo --env MiniGrid-ConfigWorld-v0-havekey --lr 0.00025 --AnomalyNN test_8 --model 20240819-seed1 --test-interval 10 --task-config task1 --discover 1

Namespace(task_config='task1', discover=1, algo='ppo', env='MiniGrid-ConfigWorld-v0-havekey', model='20240819-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_8', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.00025, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

discover.py --algo ppo --env MiniGrid-ConfigWorld-v0-havekey --lr 0.00025 --AnomalyNN test_8 --model 20240819-seed1 --test-interval 10 --task-config task1 --discover 1

Namespace(task_config='task1', discover=1, algo='ppo', env='MiniGrid-ConfigWorld-v0-havekey', model='20240819-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_8', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.00025, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', 0.28394737243652346), ('std', 0.5682998559655851), ('min', -1.0), ('max', 0.9242105484008789)])
Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', 0.6287631630897522), ('std', 0.34355986415417794), ('min', 0.0), ('max', 0.928947389125824)])
successful test! Start from: 2, reward per episode: OrderedDict([('mean', 0.852921062707901), ('std', 0.06674116668972474), ('min', 0.706315815448761), ('max', 0.9147368669509888)])
discover.py --algo ppo --env MiniGrid-ConfigWorld-v0-havekey --lr 0.00025 --AnomalyNN test_8 --model 20240819-seed1 --test-interval 10 --task-config task1 --discover 1

Namespace(task_config='task1', discover=1, algo='ppo', env='MiniGrid-ConfigWorld-v0-havekey', model='20240819-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_8', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.00025, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', 0.28394737243652346), ('std', 0.5682998559655851), ('min', -1.0), ('max', 0.9242105484008789)])
Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', 0.6287631630897522), ('std', 0.34355986415417794), ('min', 0.0), ('max', 0.928947389125824)])
successful test! Start from: 2, reward per episode: OrderedDict([('mean', 0.852921062707901), ('std', 0.06674116668972474), ('min', 0.706315815448761), ('max', 0.9147368669509888)])
discover.py --algo ppo --env MiniGrid-ConfigWorld-v0-havekey --lr 0.00025 --AnomalyNN test_8 --model 20240819-seed1 --test-interval 10 --task-config task1 --discover 1

Namespace(task_config='task1', discover=1, algo='ppo', env='MiniGrid-ConfigWorld-v0-havekey', model='20240819-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_8', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.00025, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', -0.37294737100601194), ('std', 0.7970819038947018), ('min', -1.0), ('max', 0.8863157629966736)])
Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', -0.2878684222698212), ('std', 0.7547078494280738), ('min', -1.0), ('max', 0.8105263113975525)])
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0051 | D 5 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.717 | value -0.695 | policy_loss 0.068 | value_loss 0.023 | grad_norm 0.669
U 2 | F 000512 | FPS 0064 | D 9 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.653 | value -0.613 | policy_loss -0.077 | value_loss 0.004 | grad_norm 0.260
U 3 | F 000768 | FPS 0061 | D 13 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.577 | value -0.621 | policy_loss -0.074 | value_loss 0.007 | grad_norm 0.217
U 4 | F 001024 | FPS 0061 | D 17 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.459 | value -0.472 | policy_loss -0.065 | value_loss 0.010 | grad_norm 0.163
U 5 | F 001280 | FPS 0063 | D 21 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.391 | value -0.419 | policy_loss -0.050 | value_loss 0.006 | grad_norm 0.129
U 6 | F 001536 | FPS 0064 | D 25 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.425 | value -0.333 | policy_loss -0.070 | value_loss 0.001 | grad_norm 0.087
U 7 | F 001792 | FPS 0064 | D 29 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.416 | value -0.279 | policy_loss -0.059 | value_loss 0.000 | grad_norm 0.075
U 8 | F 002048 | FPS 0060 | D 33 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.499 | value -0.327 | policy_loss 0.040 | value_loss 0.013 | grad_norm 0.088
U 9 | F 002304 | FPS 0059 | D 38 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.430 | value -0.303 | policy_loss -0.059 | value_loss 0.001 | grad_norm 0.110
U 10 | F 002560 | FPS 0059 | D 42 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.625 | value -0.281 | policy_loss -0.047 | value_loss 0.000 | grad_norm 0.094
U 11 | F 002816 | FPS 0061 | D 46 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.686 | value -0.241 | policy_loss -0.036 | value_loss 0.000 | grad_norm 0.061
U 12 | F 003072 | FPS 0064 | D 50 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.783 | value -0.353 | policy_loss 0.166 | value_loss 0.054 | grad_norm 0.187
U 13 | F 003328 | FPS 0065 | D 54 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.752 | value -0.300 | policy_loss -0.038 | value_loss 0.008 | grad_norm 0.071
U 14 | F 003584 | FPS 0065 | D 58 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.704 | value -0.316 | policy_loss -0.016 | value_loss 0.009 | grad_norm 0.153
U 15 | F 003840 | FPS 0060 | D 62 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.762 | value -0.300 | policy_loss 0.040 | value_loss 0.016 | grad_norm 0.031
U 16 | F 004096 | FPS 0064 | D 66 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.785 | value -0.372 | policy_loss 0.081 | value_loss 0.024 | grad_norm 0.057
U 17 | F 004352 | FPS 0063 | D 70 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.772 | value -0.394 | policy_loss 0.096 | value_loss 0.026 | grad_norm 0.119
U 18 | F 004608 | FPS 0061 | D 74 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.694 | value -0.421 | policy_loss 0.055 | value_loss 0.025 | grad_norm 0.151
U 19 | F 004864 | FPS 0064 | D 78 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.756 | value -0.477 | policy_loss 0.040 | value_loss 0.021 | grad_norm 0.101
U 20 | F 005120 | FPS 0064 | D 82 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.717 | value -0.443 | policy_loss 0.030 | value_loss 0.018 | grad_norm 0.113
Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', 0.2524473607540131), ('std', 0.821247475698436), ('min', -1.0), ('max', 0.9076315760612488)])
Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', 0.3571842133998871), ('std', 0.7221507797869504), ('min', -1.0), ('max', 0.933684229850769)])
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0060 | D 4 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.672 | value -0.346 | policy_loss -0.079 | value_loss 0.001 | grad_norm 0.082
U 2 | F 000512 | FPS 0062 | D 8 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.751 | value -0.384 | policy_loss -0.081 | value_loss 0.001 | grad_norm 0.085
U 3 | F 000768 | FPS 0062 | D 12 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.551 | value -0.260 | policy_loss -0.050 | value_loss 0.000 | grad_norm 0.060
U 4 | F 001024 | FPS 0063 | D 16 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.453 | value -0.219 | policy_loss -0.045 | value_loss 0.000 | grad_norm 0.038
U 5 | F 001280 | FPS 0061 | D 20 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.511 | value -0.185 | policy_loss -0.036 | value_loss 0.000 | grad_norm 0.041
U 6 | F 001536 | FPS 0061 | D 25 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.594 | value -0.155 | policy_loss -0.030 | value_loss 0.000 | grad_norm 0.021
U 7 | F 001792 | FPS 0064 | D 29 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.790 | value -0.185 | policy_loss -0.034 | value_loss 0.000 | grad_norm 0.028
U 8 | F 002048 | FPS 0061 | D 33 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.724 | value -0.143 | policy_loss -0.024 | value_loss 0.000 | grad_norm 0.023
U 9 | F 002304 | FPS 0062 | D 37 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.814 | value -0.209 | policy_loss 0.078 | value_loss 0.030 | grad_norm 0.084
U 10 | F 002560 | FPS 0059 | D 41 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.857 | value -0.166 | policy_loss 0.024 | value_loss 0.026 | grad_norm 0.067
U 11 | F 002816 | FPS 0061 | D 45 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.801 | value -0.346 | policy_loss 0.038 | value_loss 0.032 | grad_norm 0.084
U 12 | F 003072 | FPS 0062 | D 49 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.864 | value -0.188 | policy_loss -0.046 | value_loss 0.000 | grad_norm 0.024
U 13 | F 003328 | FPS 0064 | D 53 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.798 | value -0.244 | policy_loss 0.036 | value_loss 0.019 | grad_norm 0.036
U 14 | F 003584 | FPS 0064 | D 57 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.790 | value -0.187 | policy_loss -0.057 | value_loss 0.003 | grad_norm 0.147
U 15 | F 003840 | FPS 0062 | D 62 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.800 | value -0.184 | policy_loss 0.009 | value_loss 0.013 | grad_norm 0.052
U 16 | F 004096 | FPS 0063 | D 66 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.817 | value -0.226 | policy_loss 0.050 | value_loss 0.029 | grad_norm 0.055
U 17 | F 004352 | FPS 0060 | D 70 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.787 | value -0.186 | policy_loss -0.042 | value_loss 0.000 | grad_norm 0.095
U 18 | F 004608 | FPS 0063 | D 74 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.812 | value -0.273 | policy_loss 0.065 | value_loss 0.025 | grad_norm 0.084
U 19 | F 004864 | FPS 0061 | D 78 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.855 | value -0.166 | policy_loss -0.034 | value_loss 0.000 | grad_norm 0.022
U 20 | F 005120 | FPS 0062 | D 82 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.865 | value -0.144 | policy_loss -0.032 | value_loss 0.000 | grad_norm 0.019
Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', 0.36571053266525266), ('std', 0.727177602271088), ('min', -1.0), ('max', 0.9100000262260437)])
Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', 0.009052634239196777), ('std', 0.8573918733984106), ('min', -1.0), ('max', 0.850789487361908)])
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0060 | D 4 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.818 | value -0.131 | policy_loss -0.028 | value_loss 0.000 | grad_norm 0.028
U 2 | F 000512 | FPS 0061 | D 8 | Reward:μσmM -0.18 0.82 -1.00 0.65 |  entropy 1.762 | value -0.119 | policy_loss -0.038 | value_loss 0.024 | grad_norm 0.092
U 3 | F 000768 | FPS 0060 | D 12 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.826 | value -0.201 | policy_loss 0.019 | value_loss 0.017 | grad_norm 0.089
U 4 | F 001024 | FPS 0065 | D 16 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.809 | value -0.224 | policy_loss 0.009 | value_loss 0.010 | grad_norm 0.071
U 5 | F 001280 | FPS 0063 | D 20 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.794 | value -0.263 | policy_loss 0.095 | value_loss 0.038 | grad_norm 0.057
U 6 | F 001536 | FPS 0063 | D 24 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.790 | value -0.252 | policy_loss -0.002 | value_loss 0.018 | grad_norm 0.096
U 7 | F 001792 | FPS 0064 | D 28 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.809 | value -0.274 | policy_loss -0.063 | value_loss 0.006 | grad_norm 0.208
U 8 | F 002048 | FPS 0063 | D 32 | Reward:μσmM -0.27 0.73 -1.00 0.46 |  entropy 1.796 | value -0.173 | policy_loss -0.065 | value_loss 0.019 | grad_norm 0.086
U 9 | F 002304 | FPS 0062 | D 36 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.767 | value -0.220 | policy_loss 0.005 | value_loss 0.010 | grad_norm 0.051
U 10 | F 002560 | FPS 0063 | D 41 | Reward:μσmM -0.17 0.83 -1.00 0.66 |  entropy 1.721 | value -0.170 | policy_loss -0.045 | value_loss 0.023 | grad_norm 0.111
U 11 | F 002816 | FPS 0064 | D 45 | Reward:μσmM -0.45 0.77 -1.00 0.64 |  entropy 1.726 | value -0.076 | policy_loss -0.019 | value_loss 0.016 | grad_norm 0.058
U 12 | F 003072 | FPS 0061 | D 49 | Reward:μσmM 0.21 0.86 -1.00 0.84 |  entropy 1.603 | value 0.011 | policy_loss -0.058 | value_loss 0.021 | grad_norm 0.195
U 13 | F 003328 | FPS 0057 | D 53 | Reward:μσmM 0.21 0.86 -1.00 0.86 |  entropy 1.621 | value 0.007 | policy_loss -0.091 | value_loss 0.035 | grad_norm 0.146
U 14 | F 003584 | FPS 0058 | D 58 | Reward:μσmM 0.22 0.86 -1.00 0.89 |  entropy 1.534 | value 0.070 | policy_loss -0.055 | value_loss 0.027 | grad_norm 0.123
U 15 | F 003840 | FPS 0059 | D 62 | Reward:μσmM 0.90 0.04 0.81 0.94 |  entropy 1.355 | value 0.438 | policy_loss -0.253 | value_loss 0.045 | grad_norm 0.341
U 16 | F 004096 | FPS 0060 | D 66 | Reward:μσmM 0.85 0.08 0.71 0.90 |  entropy 1.381 | value 0.414 | policy_loss -0.118 | value_loss 0.014 | grad_norm 0.239
U 17 | F 004352 | FPS 0062 | D 70 | Reward:μσmM 0.90 0.02 0.88 0.93 |  entropy 1.365 | value 0.579 | policy_loss -0.141 | value_loss 0.005 | grad_norm 0.154
U 18 | F 004608 | FPS 0063 | D 74 | Reward:μσmM 0.92 0.02 0.89 0.94 |  entropy 1.291 | value 0.738 | policy_loss -0.057 | value_loss 0.004 | grad_norm 0.093
U 19 | F 004864 | FPS 0064 | D 78 | Reward:μσmM 0.93 0.01 0.91 0.95 |  entropy 1.273 | value 0.714 | policy_loss -0.030 | value_loss 0.004 | grad_norm 0.091
U 20 | F 005120 | FPS 0060 | D 83 | Reward:μσmM 0.93 0.03 0.87 0.97 |  entropy 1.257 | value 0.765 | policy_loss -0.035 | value_loss 0.004 | grad_norm 0.127
Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', 0.2178684264421463), ('std', 0.6748622494119398), ('min', -1.0), ('max', 0.8318421244621277)])
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0058 | D 4 | Reward:μσmM 0.92 0.01 0.90 0.94 |  entropy 1.380 | value 0.777 | policy_loss 0.012 | value_loss 0.001 | grad_norm 0.085
U 2 | F 000512 | FPS 0059 | D 8 | Reward:μσmM 0.92 0.02 0.88 0.94 |  entropy 1.610 | value 0.772 | policy_loss 0.004 | value_loss 0.002 | grad_norm 0.050
U 3 | F 000768 | FPS 0059 | D 13 | Reward:μσmM 0.08 0.96 -1.00 0.96 |  entropy 1.623 | value 0.551 | policy_loss 0.265 | value_loss 0.283 | grad_norm 0.504
U 4 | F 001024 | FPS 0061 | D 17 | Reward:μσmM 0.09 0.95 -1.00 0.94 |  entropy 1.674 | value 0.398 | policy_loss 0.110 | value_loss 0.147 | grad_norm 0.258
U 5 | F 001280 | FPS 0063 | D 21 | Reward:μσmM 0.25 0.88 -1.00 0.89 |  entropy 1.632 | value 0.427 | policy_loss 0.003 | value_loss 0.077 | grad_norm 0.204
U 6 | F 001536 | FPS 0060 | D 25 | Reward:μσmM 0.51 0.75 -1.00 0.93 |  entropy 1.545 | value 0.648 | policy_loss -0.042 | value_loss 0.011 | grad_norm 0.127
U 7 | F 001792 | FPS 0062 | D 29 | Reward:μσmM 0.49 0.75 -1.00 0.90 |  entropy 1.670 | value 0.625 | policy_loss 0.019 | value_loss 0.045 | grad_norm 0.116
U 8 | F 002048 | FPS 0066 | D 33 | Reward:μσmM 0.85 0.07 0.74 0.90 |  entropy 1.723 | value 0.557 | policy_loss 0.017 | value_loss 0.004 | grad_norm 0.084
U 9 | F 002304 | FPS 0064 | D 37 | Reward:μσmM 0.85 0.03 0.81 0.87 |  entropy 1.754 | value 0.557 | policy_loss 0.021 | value_loss 0.002 | grad_norm 0.039
U 10 | F 002560 | FPS 0062 | D 41 | Reward:μσmM -0.27 0.90 -1.00 0.89 |  entropy 1.765 | value 0.440 | policy_loss 0.205 | value_loss 0.138 | grad_norm 0.103
U 11 | F 002816 | FPS 0061 | D 46 | Reward:μσmM -0.79 0.59 -1.00 0.88 |  entropy 1.735 | value 0.203 | policy_loss 0.373 | value_loss 0.305 | grad_norm 0.366
U 12 | F 003072 | FPS 0064 | D 50 | Reward:μσmM -0.40 0.85 -1.00 0.86 |  entropy 1.722 | value 0.185 | policy_loss 0.051 | value_loss 0.083 | grad_norm 0.154
U 13 | F 003328 | FPS 0064 | D 54 | Reward:μσmM 0.76 0.13 0.64 0.89 |  entropy 1.680 | value 0.301 | policy_loss -0.100 | value_loss 0.013 | grad_norm 0.134
U 14 | F 003584 | FPS 0063 | D 58 | Reward:μσmM 0.75 0.01 0.73 0.76 |  entropy 1.656 | value 0.475 | policy_loss -0.033 | value_loss 0.003 | grad_norm 0.063
U 15 | F 003840 | FPS 0059 | D 62 | Reward:μσmM 0.81 0.03 0.77 0.85 |  entropy 1.641 | value 0.487 | policy_loss -0.050 | value_loss 0.002 | grad_norm 0.084
U 16 | F 004096 | FPS 0059 | D 66 | Reward:μσmM 0.88 0.03 0.84 0.92 |  entropy 1.597 | value 0.556 | policy_loss -0.096 | value_loss 0.004 | grad_norm 0.111
U 17 | F 004352 | FPS 0060 | D 71 | Reward:μσmM 0.90 0.04 0.83 0.94 |  entropy 1.470 | value 0.663 | policy_loss -0.081 | value_loss 0.005 | grad_norm 0.102
U 18 | F 004608 | FPS 0059 | D 75 | Reward:μσmM 0.91 0.01 0.89 0.94 |  entropy 1.377 | value 0.698 | policy_loss -0.042 | value_loss 0.002 | grad_norm 0.081
U 19 | F 004864 | FPS 0058 | D 79 | Reward:μσmM 0.69 0.64 -1.00 0.95 |  entropy 1.543 | value 0.707 | policy_loss 0.034 | value_loss 0.090 | grad_norm 0.176
U 20 | F 005120 | FPS 0059 | D 84 | Reward:μσmM 0.51 0.75 -1.00 0.94 |  entropy 1.679 | value 0.599 | policy_loss 0.091 | value_loss 0.084 | grad_norm 0.193
U 101 | F 025856 | FPS 0043 | D 5 | Reward:μσmM 1.52 0.76 0.00 1.93 | policy_loss ['None', 'None', '0.011', '-0.335'] | value_loss ['None', 'None', '0.002', '0.153']
U 102 | F 026112 | FPS 0050 | D 11 | Reward:μσmM 1.57 0.67 0.00 1.95 | policy_loss ['None', 'None', '0.155', '-0.592'] | value_loss ['None', 'None', '0.219', '0.229']
U 103 | F 026368 | FPS 0050 | D 16 | Reward:μσmM 1.69 0.36 1.00 1.96 | policy_loss ['None', 'None', '0.049', '-0.608'] | value_loss ['None', 'None', '0.005', '0.135']
U 104 | F 026624 | FPS 0047 | D 21 | Reward:μσmM 1.70 0.35 1.00 1.94 | policy_loss ['None', 'None', '0.023', '-0.080'] | value_loss ['None', 'None', '0.004', '0.005']
U 105 | F 026880 | FPS 0050 | D 26 | Reward:μσmM 1.20 0.79 0.00 1.91 | policy_loss ['None', 'None', '0.150', '0.066'] | value_loss ['None', 'None', '0.085', '0.005']
U 106 | F 027136 | FPS 0046 | D 32 | Reward:μσmM 1.52 0.70 0.00 1.95 | policy_loss ['None', 'None', '-0.018', '0.159'] | value_loss ['None', 'None', '0.027', '0.135']
U 107 | F 027392 | FPS 0050 | D 37 | Reward:μσmM 1.66 0.38 1.00 1.93 | policy_loss ['None', 'None', '0.015', '0.014'] | value_loss ['None', 'None', '0.004', '0.031']
U 108 | F 027648 | FPS 0045 | D 43 | Reward:μσmM 1.78 0.32 1.00 1.95 | policy_loss ['None', 'None', '-0.049', '-0.169'] | value_loss ['None', 'None', '0.002', '0.019']
U 109 | F 027904 | FPS 0047 | D 48 | Reward:μσmM 1.81 0.31 1.00 1.95 | policy_loss ['None', 'None', '-0.022', '-0.158'] | value_loss ['None', 'None', '0.002', '0.006']
U 110 | F 028160 | FPS 0047 | D 53 | Reward:μσmM 1.35 0.99 -1.00 1.95 | policy_loss ['None', 'None', '0.067', '0.266'] | value_loss ['None', 'None', '0.111', '0.492']
U 10 | Test reward:μσmM 0.93 0.02 0.90 0.96 | Test num frames:μσmM 30.50 9.43 17.00 42.00
Status saved
U 111 | F 028416 | FPS 0045 | D 70 | Reward:μσmM 1.83 0.29 1.00 1.96 | policy_loss ['None', 'None', '-0.036', '-0.104'] | value_loss ['None', 'None', '0.004', '0.005']
U 112 | F 028672 | FPS 0046 | D 75 | Reward:μσmM 1.93 0.02 1.91 1.95 | policy_loss ['None', 'None', '-0.002', '-0.128'] | value_loss ['None', 'None', '0.002', '0.012']
U 113 | F 028928 | FPS 0049 | D 80 | Reward:μσmM 1.83 0.29 1.00 1.95 | policy_loss ['None', 'None', '0.005', '-0.053'] | value_loss ['None', 'None', '0.002', '0.003']
U 114 | F 029184 | FPS 0049 | D 86 | Reward:μσmM 1.83 0.29 1.00 1.95 | policy_loss ['None', 'None', '-0.009', '-0.001'] | value_loss ['None', 'None', '0.002', '0.001']
U 115 | F 029440 | FPS 0048 | D 91 | Reward:μσmM 1.85 0.28 1.00 1.96 | policy_loss ['None', 'None', '-0.017', '-0.015'] | value_loss ['None', 'None', '0.001', '0.003']
U 116 | F 029696 | FPS 0044 | D 97 | Reward:μσmM 1.84 0.28 1.00 1.96 | policy_loss ['None', 'None', '0.005', '-0.026'] | value_loss ['None', 'None', '0.002', '0.005']
U 117 | F 029952 | FPS 0045 | D 102 | Reward:μσmM 1.42 0.87 0.00 1.97 | policy_loss ['None', 'None', '0.254', '0.026'] | value_loss ['None', 'None', '0.285', '0.023']
U 118 | F 030208 | FPS 0047 | D 108 | Reward:μσmM 1.63 0.67 0.00 1.95 | policy_loss ['None', 'None', '0.027', '0.001'] | value_loss ['None', 'None', '0.006', '0.005']
U 119 | F 030464 | FPS 0047 | D 113 | Reward:μσmM 0.69 0.90 0.00 1.95 | policy_loss ['None', 'None', '0.178', '0.445'] | value_loss ['None', 'None', '0.227', '0.359']
U 120 | F 030720 | FPS 0047 | D 119 | Reward:μσmM 1.57 0.71 0.00 1.95 | policy_loss ['None', 'None', '-0.032', '0.039'] | value_loss ['None', 'None', '0.010', '0.014']
U 10 | Test reward:μσmM 0.70 0.57 -1.00 0.93 | Test num frames:μσmM 40.80 19.87 1.00 83.00
Status saved
U 121 | F 030976 | FPS 0045 | D 139 | Reward:μσmM 0.93 1.27 -1.00 1.95 | policy_loss ['None', 'None', '-0.014', '0.071'] | value_loss ['None', 'None', '0.005', '0.241']
U 122 | F 031232 | FPS 0047 | D 144 | Reward:μσmM 1.19 1.05 -1.00 1.95 | policy_loss ['None', 'None', '0.065', '-0.133'] | value_loss ['None', 'None', '0.070', '0.078']
U 123 | F 031488 | FPS 0049 | D 149 | Reward:μσmM 1.63 0.67 0.00 1.95 | policy_loss ['None', 'None', '-0.010', '-0.078'] | value_loss ['None', 'None', '0.004', '0.006']
U 124 | F 031744 | FPS 0047 | D 155 | Reward:μσmM 1.50 0.69 0.00 1.95 | policy_loss ['None', 'None', '0.044', '0.038'] | value_loss ['None', 'None', '0.085', '0.109']
U 125 | F 032000 | FPS 0051 | D 160 | Reward:μσmM 1.71 0.35 1.00 1.91 | policy_loss ['None', 'None', '-0.024', '0.057'] | value_loss ['None', 'None', '0.002', '0.007']
U 126 | F 032256 | FPS 0050 | D 165 | Reward:μσmM 0.95 1.11 -1.00 1.94 | policy_loss ['None', 'None', '0.219', '0.088'] | value_loss ['None', 'None', '0.150', '0.153']
U 127 | F 032512 | FPS 0053 | D 170 | Reward:μσmM 1.83 0.29 1.00 1.96 | policy_loss ['None', 'None', '-0.049', '-0.162'] | value_loss ['None', 'None', '0.003', '0.012']
U 128 | F 032768 | FPS 0044 | D 175 | Reward:μσmM 1.83 0.29 1.00 1.95 | policy_loss ['None', 'None', '-0.025', '-0.075'] | value_loss ['None', 'None', '0.001', '0.005']
U 129 | F 033024 | FPS 0042 | D 182 | Reward:μσmM 1.84 0.30 1.00 1.97 | policy_loss ['None', 'None', '-0.027', '-0.006'] | value_loss ['None', 'None', '0.006', '0.007']
U 130 | F 033280 | FPS 0050 | D 187 | Reward:μσmM 1.62 0.73 0.00 1.96 | policy_loss ['None', 'None', '0.080', '0.065'] | value_loss ['None', 'None', '0.211', '0.113']
U 10 | Test reward:μσmM 0.92 0.03 0.86 0.95 | Test num frames:μσmM 34.70 11.90 19.00 61.00
Status saved
U 131 | F 033536 | FPS 0044 | D 204 | Reward:μσmM 1.44 0.97 -1.00 1.95 | policy_loss ['None', 'None', '-0.083', '0.078'] | value_loss ['None', 'None', '0.002', '0.054']
U 132 | F 033792 | FPS 0050 | D 209 | Reward:μσmM 1.17 1.10 -1.00 1.96 | policy_loss ['None', 'None', '0.157', '0.198'] | value_loss ['None', 'None', '0.171', '0.523']
U 133 | F 034048 | FPS 0049 | D 215 | Reward:μσmM 1.76 0.34 1.00 1.95 | policy_loss ['None', 'None', '-0.004', '-0.046'] | value_loss ['None', 'None', '0.005', '0.009']
U 134 | F 034304 | FPS 0047 | D 220 | Reward:μσmM 1.81 0.31 1.00 1.95 | policy_loss ['None', 'None', '-0.089', '-0.014'] | value_loss ['None', 'None', '0.004', '0.005']
U 135 | F 034560 | FPS 0045 | D 226 | Reward:μσmM 0.74 1.19 -1.00 1.95 | policy_loss ['None', 'None', '0.160', '0.254'] | value_loss ['None', 'None', '0.176', '0.475']
U 136 | F 034816 | FPS 0047 | D 231 | Reward:μσmM 1.63 0.67 0.00 1.93 | policy_loss ['None', 'None', '-0.092', '-0.108'] | value_loss ['None', 'None', '0.004', '0.010']
U 137 | F 035072 | FPS 0045 | D 237 | Reward:μσmM 1.38 1.03 -1.00 1.97 | policy_loss ['None', 'None', '-0.084', '-0.030'] | value_loss ['None', 'None', '0.002', '0.046']
U 138 | F 035328 | FPS 0046 | D 243 | Reward:μσmM 1.13 1.26 -1.00 1.96 | policy_loss ['None', 'None', '-0.030', '0.259'] | value_loss ['None', 'None', '0.001', '0.786']
U 139 | F 035584 | FPS 0046 | D 248 | Reward:μσmM 1.74 0.58 0.00 1.96 | policy_loss ['None', 'None', '-0.003', '-0.234'] | value_loss ['None', 'None', '0.001', '0.026']
U 140 | F 035840 | FPS 0042 | D 254 | Reward:μσmM 1.62 0.73 0.00 1.97 | policy_loss ['None', 'None', '0.080', '-0.039'] | value_loss ['None', 'None', '0.151', '0.097']
U 10 | Test reward:μσmM 0.90 0.04 0.84 0.95 | Test num frames:μσmM 41.50 16.00 23.00 69.00
Status saved
U 141 | F 036096 | FPS 0046 | D 274 | Reward:μσmM 1.20 1.06 -1.00 1.95 | policy_loss ['None', 'None', '0.115', '0.365'] | value_loss ['None', 'None', '0.085', '0.592']
U 142 | F 036352 | FPS 0047 | D 279 | Reward:μσmM 1.62 0.36 1.00 1.87 | policy_loss ['None', 'None', '0.068', '-0.051'] | value_loss ['None', 'None', '0.002', '0.002']
U 143 | F 036608 | FPS 0049 | D 285 | Reward:μσmM 0.92 0.80 0.00 1.87 | policy_loss ['None', 'None', '0.180', '0.190'] | value_loss ['None', 'None', '0.124', '0.094']
U 144 | F 036864 | FPS 0048 | D 290 | Reward:μσmM 0.60 0.85 0.00 1.81 | policy_loss ['None', 'None', '0.151', '0.027'] | value_loss ['None', 'None', '0.079', '0.002']
U 145 | F 037120 | FPS 0047 | D 295 | Reward:μσmM -0.14 1.02 -1.00 1.88 | policy_loss ['None', 'None', '0.129', '0.690'] | value_loss ['None', 'None', '0.088', '1.029']
U 146 | F 037376 | FPS 0048 | D 301 | Reward:μσmM -0.50 0.87 -1.00 1.00 | policy_loss ['None', 'None', '-0.070', '0.419'] | value_loss ['None', 'None', '0.000', '0.378']
U 147 | F 037632 | FPS 0053 | D 306 | Reward:μσmM -0.67 0.47 -1.00 0.00 | policy_loss ['None', 'None', 'None', '0.264'] | value_loss ['None', 'None', 'None', '0.189']
U 148 | F 037888 | FPS 0048 | D 311 | Reward:μσmM 0.18 1.21 -1.00 1.73 | policy_loss ['None', 'None', '-0.033', '0.066'] | value_loss ['None', 'None', '0.006', '0.181']
U 149 | F 038144 | FPS 0050 | D 316 | Reward:μσmM 0.12 1.28 -1.00 1.92 | policy_loss ['None', 'None', '-0.069', '-0.027'] | value_loss ['None', 'None', '0.009', '0.237']
U 150 | F 038400 | FPS 0051 | D 321 | Reward:μσmM -0.04 1.02 -1.00 1.78 | policy_loss ['None', 'None', '0.153', '0.044'] | value_loss ['None', 'None', '0.164', '0.145']
U 10 | Test reward:μσmM -0.02 0.71 -1.00 0.85 | Test num frames:μσmM 90.30 61.75 1.00 178.00
Status saved
U 151 | F 038656 | FPS 0044 | D 348 | Reward:μσmM 0.30 1.21 -1.00 1.91 | policy_loss ['None', 'None', '-0.243', '0.005'] | value_loss ['None', 'None', '0.005', '0.057']
U 152 | F 038912 | FPS 0049 | D 354 | Reward:μσmM -0.36 1.05 -1.00 1.85 | policy_loss ['None', 'None', '-0.017', '0.132'] | value_loss ['None', 'None', '0.001', '0.134']
U 153 | F 039168 | FPS 0047 | D 359 | Reward:μσmM 1.37 0.79 0.00 1.86 | policy_loss ['None', 'None', '0.019', '-0.356'] | value_loss ['None', 'None', '0.001', '0.148']
U 154 | F 039424 | FPS 0051 | D 364 | Reward:μσmM 1.40 0.81 0.00 1.91 | policy_loss ['None', 'None', '-0.002', '-0.270'] | value_loss ['None', 'None', '0.002', '0.106']
U 155 | F 039680 | FPS 0049 | D 369 | Reward:μσmM 1.63 0.37 1.00 1.90 | policy_loss ['None', 'None', '0.008', '-0.330'] | value_loss ['None', 'None', '0.003', '0.058']
U 156 | F 039936 | FPS 0047 | D 375 | Reward:μσmM 1.56 0.39 1.00 1.87 | policy_loss ['None', 'None', '0.045', '-0.069'] | value_loss ['None', 'None', '0.002', '0.021']
U 157 | F 040192 | FPS 0050 | D 380 | Reward:μσmM 0.11 0.75 -1.00 1.90 | policy_loss ['None', 'None', '0.627', '0.301'] | value_loss ['None', 'None', '0.757', '0.295']
U 158 | F 040448 | FPS 0052 | D 385 | Reward:μσmM -0.20 1.15 -1.00 1.77 | policy_loss ['None', 'None', '-0.049', '0.452'] | value_loss ['None', 'None', '0.008', '0.533']
U 159 | F 040704 | FPS 0051 | D 390 | Reward:μσmM 0.53 1.12 -1.00 1.88 | policy_loss ['None', 'None', '0.154', '-0.028'] | value_loss ['None', 'None', '0.153', '0.158']
U 160 | F 040960 | FPS 0053 | D 395 | Reward:μσmM 1.32 0.32 1.00 1.65 | policy_loss ['None', 'None', '-0.054', '-0.082'] | value_loss ['None', 'None', '0.009', '0.032']
U 10 | Test reward:μσmM -0.63 0.74 -1.00 0.91 | Test num frames:μσmM 52.00 19.76 27.00 84.00
Status saved
U 161 | F 041216 | FPS 0048 | D 417 | Reward:μσmM 0.37 0.73 0.00 1.83 | policy_loss ['None', 'None', '0.328', '0.053'] | value_loss ['None', 'None', '0.320', '0.048']
U 162 | F 041472 | FPS 0049 | D 422 | Reward:μσmM 0.33 0.47 0.00 1.00 | policy_loss ['None', 'None', '0.488', '0.086'] | value_loss ['None', 'None', '0.327', '0.024']
U 163 | F 041728 | FPS 0052 | D 427 | Reward:μσmM 1.15 0.74 0.00 1.86 | policy_loss ['None', 'None', '-0.133', '-0.015'] | value_loss ['None', 'None', '0.059', '0.011']
U 164 | F 041984 | FPS 0053 | D 432 | Reward:μσmM 1.50 0.36 1.00 1.77 | policy_loss ['None', 'None', '-0.151', '0.031'] | value_loss ['None', 'None', '0.008', '0.005']
U 165 | F 042240 | FPS 0051 | D 437 | Reward:μσmM -0.39 1.09 -1.00 1.87 | policy_loss ['None', 'None', '-0.198', '0.534'] | value_loss ['None', 'None', '0.014', '0.710']
U 166 | F 042496 | FPS 0046 | D 443 | Reward:μσmM 1.05 1.34 -1.00 1.95 | policy_loss ['None', 'None', '-0.209', '-0.513'] | value_loss ['None', 'None', '0.007', '0.397']
U 167 | F 042752 | FPS 0046 | D 448 | Reward:μσmM 1.18 1.26 -1.00 1.94 | policy_loss ['None', 'None', '-0.051', '-0.143'] | value_loss ['None', 'None', '0.001', '0.528']
U 168 | F 043008 | FPS 0047 | D 454 | Reward:μσmM 1.85 0.28 1.00 1.96 | policy_loss ['None', 'None', '-0.066', '-0.546'] | value_loss ['None', 'None', '0.001', '0.199']
U 169 | F 043264 | FPS 0049 | D 459 | Reward:μσmM 1.94 0.02 1.91 1.96 | policy_loss ['None', 'None', '0.008', '-0.334'] | value_loss ['None', 'None', '0.002', '0.064']
U 170 | F 043520 | FPS 0045 | D 465 | Reward:μσmM 1.65 0.62 0.00 1.96 | policy_loss ['None', 'None', '0.048', '-0.085'] | value_loss ['None', 'None', '0.043', '0.082']
U 10 | Test reward:μσmM 0.54 0.77 -1.00 0.96 | Test num frames:μσmM 37.60 15.24 18.00 66.00
Status saved
U 171 | F 043776 | FPS 0047 | D 484 | Reward:μσmM 1.79 0.32 1.00 1.96 | policy_loss ['None', 'None', '0.005', '-0.014'] | value_loss ['None', 'None', '0.008', '0.011']
U 172 | F 044032 | FPS 0049 | D 489 | Reward:μσmM 1.40 0.80 0.00 1.97 | policy_loss ['None', 'None', '0.123', '0.145'] | value_loss ['None', 'None', '0.196', '0.094']
U 173 | F 044288 | FPS 0051 | D 494 | Reward:μσmM 1.74 0.58 0.00 1.95 | policy_loss ['None', 'None', '-0.106', '-0.124'] | value_loss ['None', 'None', '0.008', '0.008']
U 174 | F 044544 | FPS 0050 | D 499 | Reward:μσmM 1.87 0.26 1.00 1.97 | policy_loss ['None', 'None', '-0.053', '-0.088'] | value_loss ['None', 'None', '0.001', '0.003']
U 175 | F 044800 | FPS 0049 | D 505 | Reward:μσmM 1.68 0.69 0.00 1.97 | policy_loss ['None', 'None', '0.070', '-0.052'] | value_loss ['None', 'None', '0.057', '0.028']
U 176 | F 045056 | FPS 0048 | D 510 | Reward:μσmM 1.65 0.62 0.00 1.96 | policy_loss ['None', 'None', '-0.012', '-0.037'] | value_loss ['None', 'None', '0.117', '0.036']
U 177 | F 045312 | FPS 0048 | D 515 | Reward:μσmM 1.07 1.23 -1.00 1.92 | policy_loss ['None', 'None', '-0.036', '0.508'] | value_loss ['None', 'None', '0.007', '0.760']
U 178 | F 045568 | FPS 0051 | D 520 | Reward:μσmM 1.49 1.02 -1.00 1.94 | policy_loss ['None', 'None', '0.013', '-0.102'] | value_loss ['None', 'None', '0.003', '0.009']
U 179 | F 045824 | FPS 0049 | D 526 | Reward:μσmM 1.75 0.34 1.00 1.93 | policy_loss ['None', 'None', '-0.002', '-0.017'] | value_loss ['None', 'None', '0.002', '0.003']
U 180 | F 046080 | FPS 0050 | D 531 | Reward:μσmM 1.71 0.61 0.00 1.95 | policy_loss ['None', 'None', '-0.072', '-0.020'] | value_loss ['None', 'None', '0.001', '0.008']
U 10 | Test reward:μσmM 0.94 0.02 0.91 0.96 | Test num frames:μσmM 23.70 7.09 16.00 39.00
Status saved
U 181 | F 046336 | FPS 0044 | D 544 | Reward:μσmM 1.85 0.28 1.00 1.96 | policy_loss ['None', 'None', '-0.026', '-0.089'] | value_loss ['None', 'None', '0.001', '0.004']
U 182 | F 046592 | FPS 0049 | D 549 | Reward:μσmM 1.87 0.26 1.00 1.97 | policy_loss ['None', 'None', '-0.026', '-0.052'] | value_loss ['None', 'None', '0.001', '0.002']
U 183 | F 046848 | FPS 0049 | D 555 | Reward:μσmM 1.87 0.26 1.00 1.97 | policy_loss ['None', 'None', '-0.002', '-0.030'] | value_loss ['None', 'None', '0.001', '0.001']
U 184 | F 047104 | FPS 0046 | D 560 | Reward:μσmM 1.73 0.56 0.00 1.97 | policy_loss ['None', 'None', '0.054', '0.036'] | value_loss ['None', 'None', '0.147', '0.072']
U 185 | F 047360 | FPS 0044 | D 566 | Reward:μσmM 1.27 1.11 -1.00 1.97 | policy_loss ['None', 'None', '0.014', '0.272'] | value_loss ['None', 'None', '0.096', '0.507']
U 186 | F 047616 | FPS 0050 | D 571 | Reward:μσmM 1.37 0.79 0.00 1.89 | policy_loss ['None', 'None', '0.063', '0.103'] | value_loss ['None', 'None', '0.003', '0.005']
U 187 | F 047872 | FPS 0049 | D 576 | Reward:μσmM 1.50 0.37 1.00 1.89 | policy_loss ['None', 'None', '0.085', '0.035'] | value_loss ['None', 'None', '0.001', '0.004']
U 188 | F 048128 | FPS 0049 | D 582 | Reward:μσmM 0.89 1.18 -1.00 1.87 | policy_loss ['None', 'None', '0.020', '0.162'] | value_loss ['None', 'None', '0.005', '0.341']
U 189 | F 048384 | FPS 0047 | D 587 | Reward:μσmM 1.35 0.78 0.00 1.86 | policy_loss ['None', 'None', '0.007', '0.014'] | value_loss ['None', 'None', '0.004', '0.001']
U 190 | F 048640 | FPS 0048 | D 592 | Reward:μσmM 0.77 1.15 -1.00 1.88 | policy_loss ['None', 'None', '0.117', '0.094'] | value_loss ['None', 'None', '0.065', '0.284']
U 10 | Test reward:μσmM -0.13 0.91 -1.00 0.93 | Test num frames:μσmM 48.70 48.28 4.00 156.00
Status saved
U 191 | F 048896 | FPS 0048 | D 611 | Reward:μσmM 0.96 0.96 0.00 1.92 | policy_loss ['None', 'None', '0.137', '-0.012'] | value_loss ['None', 'None', '0.103', '0.062']
U 192 | F 049152 | FPS 0049 | D 616 | Reward:μσmM 0.27 1.16 -1.00 1.87 | policy_loss ['None', 'None', '0.169', '0.189'] | value_loss ['None', 'None', '0.083', '0.340']
U 193 | F 049408 | FPS 0047 | D 621 | Reward:μσmM 0.68 1.12 -1.00 1.92 | policy_loss ['None', 'None', '0.217', '0.267'] | value_loss ['None', 'None', '0.274', '0.373']
U 194 | F 049664 | FPS 0046 | D 627 | Reward:μσmM 0.76 0.93 0.00 1.90 | policy_loss ['None', 'None', '0.161', '-0.062'] | value_loss ['None', 'None', '0.086', '0.026']
U 195 | F 049920 | FPS 0050 | D 632 | Reward:μσmM 1.36 0.78 0.00 1.87 | policy_loss ['None', 'None', '-0.180', '0.057'] | value_loss ['None', 'None', '0.030', '0.003']
U 196 | F 050176 | FPS 0047 | D 637 | Reward:μσmM 1.22 1.12 -1.00 1.97 | policy_loss ['None', 'None', '-0.171', '0.068'] | value_loss ['None', 'None', '0.015', '0.220']
U 197 | F 050432 | FPS 0052 | D 642 | Reward:μσmM 1.68 0.63 0.00 1.95 | policy_loss ['None', 'None', '-0.110', '-0.251'] | value_loss ['None', 'None', '0.002', '0.024']
U 198 | F 050688 | FPS 0047 | D 648 | Reward:μσmM 1.83 0.29 1.00 1.95 | policy_loss ['None', 'None', '-0.037', '-0.168'] | value_loss ['None', 'None', '0.001', '0.005']
U 199 | F 050944 | FPS 0050 | D 653 | Reward:μσmM 1.71 0.61 0.00 1.95 | policy_loss ['None', 'None', '-0.010', '-0.056'] | value_loss ['None', 'None', '0.001', '0.005']
U 200 | F 051200 | FPS 0047 | D 658 | Reward:μσmM 1.77 0.56 0.00 1.96 | policy_loss ['None', 'None', '-0.006', '-0.096'] | value_loss ['None', 'None', '0.001', '0.003']
U 10 | Test reward:μσmM 0.74 0.58 -1.00 0.96 | Test num frames:μσmM 30.40 13.61 17.00 65.00
Status saved
U 201 | F 051456 | FPS 0047 | D 675 | Reward:μσmM 1.78 0.32 1.00 1.95 | policy_loss ['None', 'None', '0.046', '0.004'] | value_loss ['None', 'None', '0.002', '0.002']
U 202 | F 051712 | FPS 0051 | D 680 | Reward:μσmM 1.79 0.32 1.00 1.95 | policy_loss ['None', 'None', '0.012', '0.043'] | value_loss ['None', 'None', '0.002', '0.001']
U 203 | F 051968 | FPS 0050 | D 685 | Reward:μσmM 1.52 0.70 0.00 1.96 | policy_loss ['None', 'None', '0.132', '0.041'] | value_loss ['None', 'None', '0.125', '0.008']
U 204 | F 052224 | FPS 0050 | D 690 | Reward:μσmM 1.59 0.71 0.00 1.94 | policy_loss ['None', 'None', '-0.073', '0.059'] | value_loss ['None', 'None', '0.005', '0.016']
U 205 | F 052480 | FPS 0048 | D 695 | Reward:μσmM 0.74 1.19 -1.00 1.96 | policy_loss ['None', 'None', '0.171', '0.376'] | value_loss ['None', 'None', '0.164', '0.622']
U 206 | F 052736 | FPS 0052 | D 700 | Reward:μσmM 0.62 1.05 -1.00 1.92 | policy_loss ['None', 'None', '0.201', '0.165'] | value_loss ['None', 'None', '0.227', '0.211']
U 207 | F 052992 | FPS 0051 | D 705 | Reward:μσmM 1.78 0.32 1.00 1.96 | policy_loss ['None', 'None', '-0.241', '-0.119'] | value_loss ['None', 'None', '0.025', '0.022']
U 208 | F 053248 | FPS 0048 | D 711 | Reward:μσmM 1.68 0.63 0.00 1.95 | policy_loss ['None', 'None', '-0.087', '-0.169'] | value_loss ['None', 'None', '0.003', '0.016']
U 209 | F 053504 | FPS 0043 | D 717 | Reward:μσmM 1.50 0.96 -1.00 1.96 | policy_loss ['None', 'None', '-0.041', '-0.121'] | value_loss ['None', 'None', '0.002', '0.185']
U 210 | F 053760 | FPS 0047 | D 722 | Reward:μσmM 1.85 0.28 1.00 1.95 | policy_loss ['None', 'None', '0.002', '-0.118'] | value_loss ['None', 'None', '0.001', '0.005']
U 10 | Test reward:μσmM 0.94 0.02 0.88 0.96 | Test num frames:μσmM 26.40 9.24 17.00 51.00
Status saved
discover.py --algo ppo --env MiniGrid-ConfigWorld-v0 --lr 0.00025 --AnomalyNN test_8 --model 20240819-seed1 --test-interval 10 --task-config task2 --discover 1

Namespace(task_config='task2', discover=1, algo='ppo', env='MiniGrid-ConfigWorld-v0', model='20240819-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_8', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.00025, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Optimizer loaded

Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', -0.6), ('std', 0.48989794855663565), ('min', -1.0), ('max', 0.0)])
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0049 | D 5 | Reward:μσmM -0.71 0.70 -1.00 1.00 |  entropy 1.786 | value 0.566 | policy_loss 0.276 | value_loss 0.239 | grad_norm 1.357
U 2 | F 000512 | FPS 0062 | D 9 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.743 | value 0.252 | policy_loss 0.243 | value_loss 0.140 | grad_norm 0.352
U 3 | F 000768 | FPS 0064 | D 13 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.647 | value -0.010 | policy_loss 0.198 | value_loss 0.093 | grad_norm 0.222
U 4 | F 001024 | FPS 0065 | D 17 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.731 | value -0.102 | policy_loss 0.114 | value_loss 0.047 | grad_norm 0.157
discover.py --algo ppo --env MiniGrid-ConfigWorld-v0 --lr 0.00025 --AnomalyNN test_8 --model 20240819-seed1 --test-interval 10 --task-config task2 --discover 1

Namespace(task_config='task2', discover=1, algo='ppo', env='MiniGrid-ConfigWorld-v0', model='20240819-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_8', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.00025, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Optimizer loaded

Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', -0.6), ('std', 0.48989794855663565), ('min', -1.0), ('max', 0.0)])
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0056 | D 4 | Reward:μσmM -0.50 0.87 -1.00 1.00 |  entropy 1.818 | value 0.598 | policy_loss 0.157 | value_loss 0.108 | grad_norm 0.414
U 2 | F 000512 | FPS 0061 | D 8 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.768 | value 0.186 | policy_loss 0.210 | value_loss 0.128 | grad_norm 0.139
U 3 | F 000768 | FPS 0062 | D 12 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.767 | value 0.012 | policy_loss 0.279 | value_loss 0.126 | grad_norm 0.247
U 4 | F 001024 | FPS 0065 | D 16 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.842 | value -0.074 | policy_loss 0.051 | value_loss 0.024 | grad_norm 0.190
U 5 | F 001280 | FPS 0064 | D 20 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.836 | value -0.105 | policy_loss 0.107 | value_loss 0.035 | grad_norm 0.068
U 6 | F 001536 | FPS 0064 | D 24 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.789 | value -0.221 | policy_loss 0.138 | value_loss 0.051 | grad_norm 0.209
U 7 | F 001792 | FPS 0063 | D 28 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.784 | value -0.316 | policy_loss 0.041 | value_loss 0.017 | grad_norm 0.156
U 8 | F 002048 | FPS 0061 | D 33 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.791 | value -0.440 | policy_loss 0.219 | value_loss 0.068 | grad_norm 0.186
U 9 | F 002304 | FPS 0065 | D 36 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.811 | value -0.484 | policy_loss 0.060 | value_loss 0.028 | grad_norm 0.164
U 10 | F 002560 | FPS 0061 | D 41 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.782 | value -0.583 | policy_loss 0.050 | value_loss 0.023 | grad_norm 0.149
U 11 | F 002816 | FPS 0063 | D 45 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.729 | value -0.577 | policy_loss -0.092 | value_loss 0.006 | grad_norm 0.106
U 12 | F 003072 | FPS 0061 | D 49 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.727 | value -0.533 | policy_loss -0.029 | value_loss 0.010 | grad_norm 0.120
U 13 | F 003328 | FPS 0066 | D 53 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.646 | value -0.498 | policy_loss -0.094 | value_loss 0.003 | grad_norm 0.138
U 14 | F 003584 | FPS 0064 | D 57 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.663 | value -0.434 | policy_loss -0.041 | value_loss 0.008 | grad_norm 0.124
U 15 | F 003840 | FPS 0063 | D 61 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.578 | value -0.366 | policy_loss -0.072 | value_loss 0.000 | grad_norm 0.121
U 16 | F 004096 | FPS 0063 | D 65 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.563 | value -0.311 | policy_loss -0.059 | value_loss 0.000 | grad_norm 0.104
U 17 | F 004352 | FPS 0063 | D 69 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.482 | value -0.261 | policy_loss -0.052 | value_loss 0.000 | grad_norm 0.087
U 18 | F 004608 | FPS 0063 | D 73 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.506 | value -0.227 | policy_loss -0.045 | value_loss 0.000 | grad_norm 0.035
U 19 | F 004864 | FPS 0066 | D 77 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.489 | value -0.190 | policy_loss -0.037 | value_loss 0.000 | grad_norm 0.031
U 20 | F 005120 | FPS 0060 | D 81 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.623 | value -0.160 | policy_loss -0.031 | value_loss 0.000 | grad_norm 0.040
Test 10 turns results: Start from 3, reward per episode: OrderedDict([('mean', 0.0), ('std', 0.0), ('min', 0.0), ('max', 0.0)])
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0060 | D 4 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.621 | value -0.137 | policy_loss -0.026 | value_loss 0.000 | grad_norm 0.025
U 2 | F 000512 | FPS 0063 | D 8 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.720 | value -0.133 | policy_loss -0.028 | value_loss 0.000 | grad_norm 0.052
U 3 | F 000768 | FPS 0063 | D 12 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.747 | value -0.106 | policy_loss -0.019 | value_loss 0.000 | grad_norm 0.020
U 4 | F 001024 | FPS 0062 | D 16 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.745 | value -0.173 | policy_loss 0.047 | value_loss 0.019 | grad_norm 0.051
U 5 | F 001280 | FPS 0064 | D 20 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.778 | value -0.160 | policy_loss 0.036 | value_loss 0.023 | grad_norm 0.070
U 6 | F 001536 | FPS 0065 | D 24 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.786 | value -0.130 | policy_loss -0.026 | value_loss 0.000 | grad_norm 0.028
U 7 | F 001792 | FPS 0064 | D 28 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.796 | value -0.118 | policy_loss -0.025 | value_loss 0.000 | grad_norm 0.042
U 8 | F 002048 | FPS 0062 | D 32 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.744 | value -0.190 | policy_loss 0.011 | value_loss 0.013 | grad_norm 0.119
U 9 | F 002304 | FPS 0064 | D 36 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.710 | value -0.213 | policy_loss -0.047 | value_loss 0.001 | grad_norm 0.156
U 10 | F 002560 | FPS 0065 | D 40 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.780 | value -0.198 | policy_loss 0.014 | value_loss 0.019 | grad_norm 0.121
U 11 | F 002816 | FPS 0064 | D 44 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.810 | value -0.227 | policy_loss 0.012 | value_loss 0.017 | grad_norm 0.044
U 12 | F 003072 | FPS 0062 | D 48 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.768 | value -0.238 | policy_loss 0.004 | value_loss 0.013 | grad_norm 0.126
U 13 | F 003328 | FPS 0064 | D 52 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.798 | value -0.273 | policy_loss 0.010 | value_loss 0.009 | grad_norm 0.068
U 14 | F 003584 | FPS 0065 | D 56 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.788 | value -0.239 | policy_loss 0.013 | value_loss 0.014 | grad_norm 0.055
U 15 | F 003840 | FPS 0065 | D 60 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.768 | value -0.212 | policy_loss -0.048 | value_loss 0.000 | grad_norm 0.063
U 16 | F 004096 | FPS 0063 | D 64 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.723 | value -0.174 | policy_loss -0.042 | value_loss 0.000 | grad_norm 0.049
U 17 | F 004352 | FPS 0063 | D 68 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.751 | value -0.132 | policy_loss -0.029 | value_loss 0.000 | grad_norm 0.022
U 18 | F 004608 | FPS 0065 | D 72 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.753 | value -0.113 | policy_loss -0.023 | value_loss 0.000 | grad_norm 0.022
U 19 | F 004864 | FPS 0062 | D 76 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.811 | value -0.095 | policy_loss -0.019 | value_loss 0.000 | grad_norm 0.010
U 20 | F 005120 | FPS 0065 | D 80 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.811 | value -0.141 | policy_loss 0.035 | value_loss 0.017 | grad_norm 0.102
Test 10 turns results: Start from 3, reward per episode: OrderedDict([('mean', 0.0), ('std', 0.0), ('min', 0.0), ('max', 0.0)])
Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', -0.8), ('std', 0.4), ('min', -1.0), ('max', 0.0)])
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0061 | D 4 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.731 | value -0.124 | policy_loss -0.068 | value_loss 0.030 | grad_norm 0.101
U 2 | F 000512 | FPS 0061 | D 8 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.678 | value -0.133 | policy_loss 0.008 | value_loss 0.016 | grad_norm 0.106
U 3 | F 000768 | FPS 0065 | D 12 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.583 | value -0.120 | policy_loss -0.053 | value_loss 0.004 | grad_norm 0.118
U 4 | F 001024 | FPS 0055 | D 17 | Reward:μσmM 0.33 0.94 -1.00 1.00 |  entropy 1.586 | value 0.007 | policy_loss -0.131 | value_loss 0.047 | grad_norm 0.252
U 5 | F 001280 | FPS 0058 | D 21 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.655 | value -0.012 | policy_loss -0.091 | value_loss 0.009 | grad_norm 0.129
U 6 | F 001536 | FPS 0062 | D 25 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.710 | value -0.089 | policy_loss -0.054 | value_loss 0.025 | grad_norm 0.108
U 7 | F 001792 | FPS 0062 | D 29 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.685 | value 0.043 | policy_loss -0.095 | value_loss 0.010 | grad_norm 0.166
U 8 | F 002048 | FPS 0064 | D 33 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.720 | value 0.129 | policy_loss -0.036 | value_loss 0.006 | grad_norm 0.179
U 9 | F 002304 | FPS 0059 | D 38 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.596 | value 0.167 | policy_loss 0.058 | value_loss 0.034 | grad_norm 0.159
U 10 | F 002560 | FPS 0061 | D 42 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.677 | value 0.066 | policy_loss 0.069 | value_loss 0.030 | grad_norm 0.117
U 11 | F 002816 | FPS 0060 | D 46 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.689 | value 0.087 | policy_loss 0.071 | value_loss 0.021 | grad_norm 0.166
U 12 | F 003072 | FPS 0066 | D 50 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.782 | value 0.097 | policy_loss -0.036 | value_loss 0.001 | grad_norm 0.033
U 13 | F 003328 | FPS 0065 | D 54 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.721 | value 0.452 | policy_loss 0.223 | value_loss 0.142 | grad_norm 0.174
U 14 | F 003584 | FPS 0063 | D 58 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.755 | value 0.253 | policy_loss 0.416 | value_loss 0.242 | grad_norm 0.266
U 15 | F 003840 | FPS 0065 | D 62 | Reward:μσmM -0.33 0.94 -1.00 1.00 |  entropy 1.738 | value 0.109 | policy_loss 0.102 | value_loss 0.059 | grad_norm 0.095
U 16 | F 004096 | FPS 0063 | D 66 | Reward:μσmM -0.33 0.94 -1.00 1.00 |  entropy 1.664 | value 0.240 | policy_loss 0.069 | value_loss 0.058 | grad_norm 0.206
U 17 | F 004352 | FPS 0063 | D 70 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.805 | value 0.284 | policy_loss 0.028 | value_loss 0.003 | grad_norm 0.124
U 18 | F 004608 | FPS 0062 | D 74 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.770 | value 0.133 | policy_loss 0.000 | value_loss 0.010 | grad_norm 0.087
U 19 | F 004864 | FPS 0064 | D 78 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.786 | value 0.138 | policy_loss 0.090 | value_loss 0.034 | grad_norm 0.084
U 20 | F 005120 | FPS 0064 | D 82 | Reward:μσmM -0.33 0.94 -1.00 1.00 |  entropy 1.701 | value 0.172 | policy_loss 0.095 | value_loss 0.081 | grad_norm 0.209
Test 10 turns results: Start from 3, reward per episode: OrderedDict([('mean', 0.0), ('std', 0.0), ('min', 0.0), ('max', 0.0)])
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0062 | D 4 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.598 | value -0.504 | policy_loss -0.156 | value_loss 0.007 | grad_norm 0.230
U 2 | F 000512 | FPS 0063 | D 8 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.611 | value -0.292 | policy_loss -0.113 | value_loss 0.005 | grad_norm 0.194
U 3 | F 000768 | FPS 0062 | D 12 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.554 | value -0.203 | policy_loss -0.083 | value_loss 0.005 | grad_norm 0.160
U 4 | F 001024 | FPS 0064 | D 16 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.432 | value -0.120 | policy_loss -0.060 | value_loss 0.003 | grad_norm 0.063
U 5 | F 001280 | FPS 0064 | D 20 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.467 | value -0.080 | policy_loss -0.054 | value_loss 0.003 | grad_norm 0.060
U 6 | F 001536 | FPS 0066 | D 24 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.518 | value -0.067 | policy_loss -0.039 | value_loss 0.003 | grad_norm 0.096
U 7 | F 001792 | FPS 0064 | D 28 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.639 | value 0.173 | policy_loss -0.002 | value_loss 0.010 | grad_norm 0.148
U 8 | F 002048 | FPS 0063 | D 32 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.694 | value 0.015 | policy_loss -0.029 | value_loss 0.002 | grad_norm 0.032
U 9 | F 002304 | FPS 0063 | D 36 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.742 | value 0.178 | policy_loss -0.004 | value_loss 0.002 | grad_norm 0.048
U 10 | F 002560 | FPS 0062 | D 40 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.745 | value 0.402 | policy_loss 0.083 | value_loss 0.018 | grad_norm 0.098
U 11 | F 002816 | FPS 0063 | D 44 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.708 | value -0.055 | policy_loss -0.047 | value_loss 0.000 | grad_norm 0.062
U 12 | F 003072 | FPS 0064 | D 48 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.733 | value 0.166 | policy_loss -0.010 | value_loss 0.002 | grad_norm 0.040
U 13 | F 003328 | FPS 0064 | D 52 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.748 | value 0.254 | policy_loss 0.056 | value_loss 0.013 | grad_norm 0.087
U 14 | F 003584 | FPS 0063 | D 56 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.800 | value 0.314 | policy_loss 0.019 | value_loss 0.001 | grad_norm 0.052
U 15 | F 003840 | FPS 0064 | D 60 | Reward:μσmM -0.60 0.80 -1.00 1.00 |  entropy 1.765 | value 0.392 | policy_loss 0.243 | value_loss 0.169 | grad_norm 0.296
U 16 | F 004096 | FPS 0060 | D 64 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.716 | value 0.023 | policy_loss -0.053 | value_loss 0.009 | grad_norm 0.235
U 17 | F 004352 | FPS 0062 | D 68 | Reward:μσmM -0.50 0.87 -1.00 1.00 |  entropy 1.683 | value 0.161 | policy_loss 0.150 | value_loss 0.098 | grad_norm 0.185
U 18 | F 004608 | FPS 0063 | D 72 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.694 | value 0.090 | policy_loss -0.028 | value_loss 0.032 | grad_norm 0.191
U 19 | F 004864 | FPS 0063 | D 76 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.714 | value 0.157 | policy_loss 0.144 | value_loss 0.098 | grad_norm 0.090
U 20 | F 005120 | FPS 0064 | D 80 | Reward:μσmM -0.20 0.98 -1.00 1.00 |  entropy 1.674 | value 0.142 | policy_loss 0.075 | value_loss 0.082 | grad_norm 0.232
Test 10 turns results: Start from 3, reward per episode: OrderedDict([('mean', 0.0), ('std', 0.0), ('min', 0.0), ('max', 0.0)])
Test 10 turns results: Start from 3, reward per episode: OrderedDict([('mean', 0.0), ('std', 0.0), ('min', 0.0), ('max', 0.0)])
Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', -0.5), ('std', 0.5), ('min', -1.0), ('max', 0.0)])
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0065 | D 3 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.648 | value 0.272 | policy_loss 0.103 | value_loss 0.049 | grad_norm 0.211
U 2 | F 000512 | FPS 0063 | D 7 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.824 | value 0.232 | policy_loss 0.063 | value_loss 0.064 | grad_norm 0.435
U 3 | F 000768 | FPS 0062 | D 12 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.741 | value 0.069 | policy_loss 0.247 | value_loss 0.122 | grad_norm 0.202
U 4 | F 001024 | FPS 0062 | D 16 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.751 | value -0.041 | policy_loss 0.092 | value_loss 0.055 | grad_norm 0.189
U 5 | F 001280 | FPS 0065 | D 20 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.782 | value -0.164 | policy_loss 0.186 | value_loss 0.074 | grad_norm 0.074
U 6 | F 001536 | FPS 0065 | D 24 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.796 | value -0.199 | policy_loss 0.106 | value_loss 0.036 | grad_norm 0.210
U 7 | F 001792 | FPS 0063 | D 28 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.634 | value -0.380 | policy_loss 0.037 | value_loss 0.029 | grad_norm 0.318
U 8 | F 002048 | FPS 0062 | D 32 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.744 | value -0.214 | policy_loss -0.016 | value_loss 0.005 | grad_norm 0.085
U 9 | F 002304 | FPS 0063 | D 36 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.788 | value -0.252 | policy_loss 0.058 | value_loss 0.018 | grad_norm 0.089
U 10 | F 002560 | FPS 0062 | D 40 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.760 | value -0.210 | policy_loss -0.036 | value_loss 0.032 | grad_norm 0.115
U 11 | F 002816 | FPS 0049 | D 45 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.820 | value -0.118 | policy_loss -0.040 | value_loss 0.000 | grad_norm 0.100
U 12 | F 003072 | FPS 0061 | D 49 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.748 | value -0.152 | policy_loss 0.033 | value_loss 0.023 | grad_norm 0.101
U 13 | F 003328 | FPS 0060 | D 54 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.789 | value -0.146 | policy_loss -0.023 | value_loss 0.000 | grad_norm 0.028
U 14 | F 003584 | FPS 0063 | D 58 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.790 | value -0.099 | policy_loss -0.115 | value_loss 0.028 | grad_norm 0.123
U 15 | F 003840 | FPS 0066 | D 62 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.813 | value -0.144 | policy_loss 0.037 | value_loss 0.021 | grad_norm 0.095
U 16 | F 004096 | FPS 0061 | D 66 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.725 | value -0.081 | policy_loss -0.071 | value_loss 0.025 | grad_norm 0.151
U 17 | F 004352 | FPS 0062 | D 70 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.782 | value -0.085 | policy_loss -0.052 | value_loss 0.034 | grad_norm 0.120
U 18 | F 004608 | FPS 0062 | D 74 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.701 | value -0.110 | policy_loss -0.093 | value_loss 0.008 | grad_norm 0.090
U 19 | F 004864 | FPS 0064 | D 78 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.665 | value 0.090 | policy_loss -0.027 | value_loss 0.008 | grad_norm 0.057
U 20 | F 005120 | FPS 0064 | D 82 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.669 | value 0.065 | policy_loss -0.055 | value_loss 0.004 | grad_norm 0.146
Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', -0.2), ('std', 0.4000000000000001), ('min', -1.0), ('max', 0.0)])
Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', -0.2), ('std', 0.4000000000000001), ('min', -1.0), ('max', 0.0)])
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0058 | D 4 | Reward:μσmM 0.33 0.94 -1.00 1.00 |  entropy 1.673 | value 0.078 | policy_loss -0.009 | value_loss 0.018 | grad_norm 0.106
U 2 | F 000512 | FPS 0062 | D 8 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.757 | value 0.079 | policy_loss -0.017 | value_loss 0.003 | grad_norm 0.060
U 3 | F 000768 | FPS 0064 | D 12 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.773 | value 0.079 | policy_loss -0.011 | value_loss 0.002 | grad_norm 0.036
U 4 | F 001024 | FPS 0058 | D 16 | Reward:μσmM 0.19 0.86 -1.00 1.00 |  entropy 1.674 | value 0.455 | policy_loss 0.072 | value_loss 0.106 | grad_norm 0.459
U 5 | F 001280 | FPS 0063 | D 21 | Reward:μσmM 0.42 0.83 -1.00 1.00 |  entropy 1.669 | value 0.418 | policy_loss -0.008 | value_loss 0.022 | grad_norm 0.142
U 6 | F 001536 | FPS 0062 | D 25 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.783 | value 0.157 | policy_loss 0.109 | value_loss 0.026 | grad_norm 0.070
U 7 | F 001792 | FPS 0065 | D 29 | Reward:μσmM 0.23 0.88 -1.00 1.00 |  entropy 1.734 | value 0.347 | policy_loss 0.030 | value_loss 0.060 | grad_norm 0.178
U 8 | F 002048 | FPS 0063 | D 33 | Reward:μσmM -0.33 0.94 -1.00 1.00 |  entropy 1.745 | value 0.108 | policy_loss 0.073 | value_loss 0.064 | grad_norm 0.099
U 9 | F 002304 | FPS 0065 | D 37 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.744 | value 0.139 | policy_loss 0.019 | value_loss 0.039 | grad_norm 0.079
U 10 | F 002560 | FPS 0064 | D 41 | Reward:μσmM -0.33 0.94 -1.00 1.00 |  entropy 1.664 | value 0.231 | policy_loss 0.086 | value_loss 0.090 | grad_norm 0.212
U 11 | F 002816 | FPS 0065 | D 45 | Reward:μσmM -0.33 0.94 -1.00 1.00 |  entropy 1.643 | value 0.166 | policy_loss 0.057 | value_loss 0.039 | grad_norm 0.147
U 12 | F 003072 | FPS 0066 | D 49 | Reward:μσmM 0.15 0.84 -1.00 1.00 |  entropy 1.617 | value 0.126 | policy_loss -0.098 | value_loss 0.017 | grad_norm 0.120
U 13 | F 003328 | FPS 0065 | D 52 | Reward:μσmM 0.91 0.13 0.73 1.00 |  entropy 1.595 | value 0.207 | policy_loss -0.144 | value_loss 0.010 | grad_norm 0.171
U 14 | F 003584 | FPS 0066 | D 56 | Reward:μσmM 0.85 0.15 0.66 1.00 |  entropy 1.532 | value 0.456 | policy_loss -0.137 | value_loss 0.015 | grad_norm 0.343
U 15 | F 003840 | FPS 0062 | D 60 | Reward:μσmM 0.94 0.07 0.85 1.00 |  entropy 1.349 | value 0.672 | policy_loss -0.183 | value_loss 0.017 | grad_norm 0.278
U 16 | F 004096 | FPS 0059 | D 65 | Reward:μσmM 0.94 0.07 0.86 1.00 |  entropy 1.195 | value 0.766 | policy_loss -0.101 | value_loss 0.004 | grad_norm 0.177
U 17 | F 004352 | FPS 0060 | D 69 | Reward:μσmM 0.94 0.06 0.85 1.00 |  entropy 1.203 | value 0.826 | policy_loss -0.056 | value_loss 0.002 | grad_norm 0.137
U 18 | F 004608 | FPS 0060 | D 73 | Reward:μσmM 0.76 0.59 -1.00 1.00 |  entropy 1.423 | value 0.776 | policy_loss 0.036 | value_loss 0.085 | grad_norm 0.212
U 19 | F 004864 | FPS 0063 | D 77 | Reward:μσmM 0.61 0.72 -1.00 1.00 |  entropy 1.609 | value 0.671 | policy_loss 0.102 | value_loss 0.043 | grad_norm 0.089
U 20 | F 005120 | FPS 0062 | D 81 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.763 | value 0.431 | policy_loss 0.365 | value_loss 0.298 | grad_norm 0.119
Test 10 turns results: Start from 3, reward per episode: OrderedDict([('mean', -0.1), ('std', 0.30000000000000004), ('min', -1.0), ('max', 0.0)])
Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', -0.4), ('std', 0.48989794855663565), ('min', -1.0), ('max', 0.0)])
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0062 | D 4 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.800 | value 0.150 | policy_loss 0.267 | value_loss 0.113 | grad_norm 0.303
U 2 | F 000512 | FPS 0061 | D 8 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.845 | value -0.024 | policy_loss 0.148 | value_loss 0.059 | grad_norm 0.128
U 3 | F 000768 | FPS 0060 | D 12 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.792 | value -0.133 | policy_loss 0.192 | value_loss 0.084 | grad_norm 0.090
U 4 | F 001024 | FPS 0062 | D 16 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.816 | value -0.252 | policy_loss 0.064 | value_loss 0.027 | grad_norm 0.066
U 5 | F 001280 | FPS 0061 | D 20 | Reward:μσmM 0.20 0.87 -1.00 1.00 |  entropy 1.729 | value -0.008 | policy_loss -0.094 | value_loss 0.034 | grad_norm 0.117
U 6 | F 001536 | FPS 0062 | D 25 | Reward:μσmM 0.80 0.20 0.59 1.00 |  entropy 1.703 | value 0.057 | policy_loss -0.147 | value_loss 0.021 | grad_norm 0.096
U 7 | F 001792 | FPS 0061 | D 29 | Reward:μσmM 0.50 0.76 -1.00 1.00 |  entropy 1.501 | value 0.289 | policy_loss -0.120 | value_loss 0.042 | grad_norm 0.161
U 8 | F 002048 | FPS 0060 | D 33 | Reward:μσmM 0.93 0.10 0.78 1.00 |  entropy 1.459 | value 0.354 | policy_loss -0.151 | value_loss 0.010 | grad_norm 0.245
U 9 | F 002304 | FPS 0061 | D 37 | Reward:μσmM 0.87 0.15 0.65 1.00 |  entropy 1.379 | value 0.387 | policy_loss -0.121 | value_loss 0.012 | grad_norm 0.163
U 10 | F 002560 | FPS 0057 | D 42 | Reward:μσmM 0.72 0.61 -1.00 1.00 |  entropy 1.174 | value 0.563 | policy_loss -0.092 | value_loss 0.050 | grad_norm 0.241
U 11 | F 002816 | FPS 0053 | D 47 | Reward:μσmM 0.94 0.06 0.85 1.00 |  entropy 1.115 | value 0.758 | policy_loss -0.121 | value_loss 0.008 | grad_norm 0.199
U 12 | F 003072 | FPS 0044 | D 52 | Reward:μσmM 0.77 0.56 -1.00 1.00 |  entropy 1.281 | value 0.833 | policy_loss -0.034 | value_loss 0.012 | grad_norm 0.109
U 13 | F 003328 | FPS 0027 | D 62 | Reward:μσmM 0.94 0.06 0.83 1.00 |  entropy 1.399 | value 0.816 | policy_loss -0.015 | value_loss 0.002 | grad_norm 0.064
U 14 | F 003584 | FPS 0049 | D 67 | Reward:μσmM 0.91 0.10 0.76 1.00 |  entropy 1.555 | value 0.773 | policy_loss 0.023 | value_loss 0.002 | grad_norm 0.059
U 15 | F 003840 | FPS 0051 | D 72 | Reward:μσmM 0.90 0.10 0.75 1.00 |  entropy 1.695 | value 0.718 | policy_loss 0.040 | value_loss 0.003 | grad_norm 0.073
U 16 | F 004096 | FPS 0059 | D 77 | Reward:μσmM -0.33 0.94 -1.00 1.00 |  entropy 1.742 | value 0.523 | policy_loss 0.235 | value_loss 0.218 | grad_norm 0.326
U 17 | F 004352 | FPS 0059 | D 81 | Reward:μσmM -0.56 0.83 -1.00 1.00 |  entropy 1.656 | value 0.401 | policy_loss 0.321 | value_loss 0.269 | grad_norm 0.277
U 18 | F 004608 | FPS 0065 | D 85 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.758 | value 0.120 | policy_loss -0.048 | value_loss 0.020 | grad_norm 0.092
U 19 | F 004864 | FPS 0064 | D 89 | Reward:μσmM -0.33 0.94 -1.00 1.00 |  entropy 1.766 | value 0.111 | policy_loss 0.075 | value_loss 0.052 | grad_norm 0.124
U 20 | F 005120 | FPS 0063 | D 93 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.737 | value 0.199 | policy_loss 0.183 | value_loss 0.146 | grad_norm 0.080
Test 10 turns results: Start from 3, reward per episode: OrderedDict([('mean', 0.0), ('std', 0.0), ('min', 0.0), ('max', 0.0)])
Test 10 turns results: Start from 3, reward per episode: OrderedDict([('mean', 0.0), ('std', 0.0), ('min', 0.0), ('max', 0.0)])
Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', -0.26039473712444305), ('std', 0.49787583728507234), ('min', -1.0), ('max', 0.39605262875556946)])
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0062 | D 4 | Reward:μσmM 0.77 0.23 0.55 1.00 |  entropy 1.722 | value 0.159 | policy_loss -0.125 | value_loss 0.015 | grad_norm 0.135
U 2 | F 000512 | FPS 0061 | D 8 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.550 | value 0.215 | policy_loss -0.022 | value_loss 0.029 | grad_norm 0.230
U 3 | F 000768 | FPS 0061 | D 12 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.708 | value 0.021 | policy_loss -0.007 | value_loss 0.019 | grad_norm 0.094
U 4 | F 001024 | FPS 0063 | D 16 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.720 | value 0.078 | policy_loss -0.040 | value_loss 0.004 | grad_norm 0.063
U 5 | F 001280 | FPS 0063 | D 20 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.638 | value -0.052 | policy_loss -0.049 | value_loss 0.003 | grad_norm 0.044
U 6 | F 001536 | FPS 0062 | D 24 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.673 | value 0.111 | policy_loss -0.023 | value_loss 0.002 | grad_norm 0.074
U 7 | F 001792 | FPS 0063 | D 28 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.769 | value 0.039 | policy_loss -0.027 | value_loss 0.002 | grad_norm 0.058
U 8 | F 002048 | FPS 0061 | D 33 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.650 | value 0.165 | policy_loss 0.075 | value_loss 0.029 | grad_norm 0.070
U 9 | F 002304 | FPS 0063 | D 37 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.748 | value -0.099 | policy_loss -0.057 | value_loss 0.004 | grad_norm 0.063
U 10 | F 002560 | FPS 0061 | D 41 | Reward:μσmM 0.33 0.94 -1.00 1.00 |  entropy 1.758 | value -0.010 | policy_loss 0.012 | value_loss 0.018 | grad_norm 0.109
U 11 | F 002816 | FPS 0060 | D 45 | Reward:μσmM 0.94 0.08 0.83 1.00 |  entropy 1.751 | value 0.189 | policy_loss -0.073 | value_loss 0.024 | grad_norm 0.105
U 12 | F 003072 | FPS 0060 | D 49 | Reward:μσmM -0.20 0.98 -1.00 1.00 |  entropy 1.741 | value 0.635 | policy_loss 0.195 | value_loss 0.115 | grad_norm 0.178
U 13 | F 003328 | FPS 0063 | D 53 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.789 | value 0.509 | policy_loss 0.055 | value_loss 0.013 | grad_norm 0.108
U 14 | F 003584 | FPS 0062 | D 58 | Reward:μσmM 0.22 0.87 -1.00 1.00 |  entropy 1.750 | value 0.340 | policy_loss -0.006 | value_loss 0.118 | grad_norm 0.281
U 15 | F 003840 | FPS 0062 | D 62 | Reward:μσmM 0.07 0.93 -1.00 1.00 |  entropy 1.618 | value 0.290 | policy_loss -0.062 | value_loss 0.157 | grad_norm 0.278
U 16 | F 004096 | FPS 0062 | D 66 | Reward:μσmM -0.07 0.94 -1.00 1.00 |  entropy 1.698 | value 0.389 | policy_loss 0.046 | value_loss 0.061 | grad_norm 0.118
U 17 | F 004352 | FPS 0064 | D 70 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.769 | value 0.191 | policy_loss 0.242 | value_loss 0.160 | grad_norm 0.312
U 18 | F 004608 | FPS 0063 | D 74 | Reward:μσmM 0.86 0.18 0.57 1.00 |  entropy 1.604 | value 0.298 | policy_loss -0.145 | value_loss 0.022 | grad_norm 0.272
U 19 | F 004864 | FPS 0061 | D 78 | Reward:μσmM 0.90 0.11 0.69 1.00 |  entropy 1.371 | value 0.533 | policy_loss -0.146 | value_loss 0.015 | grad_norm 0.187
U 20 | F 005120 | FPS 0060 | D 82 | Reward:μσmM 0.93 0.07 0.81 1.00 |  entropy 1.272 | value 0.623 | policy_loss -0.148 | value_loss 0.008 | grad_norm 0.178
Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', -0.4), ('std', 0.4898979485566356), ('min', -1.0), ('max', 0.0)])
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0056 | D 4 | Reward:μσmM 0.95 0.05 0.90 1.00 |  entropy 1.236 | value 0.695 | policy_loss -0.095 | value_loss 0.006 | grad_norm 0.219
U 2 | F 000512 | FPS 0055 | D 9 | Reward:μσmM 0.80 0.52 -1.00 1.00 |  entropy 1.165 | value 0.778 | policy_loss -0.002 | value_loss 0.081 | grad_norm 0.232
U 3 | F 000768 | FPS 0057 | D 13 | Reward:μσmM 0.95 0.06 0.85 1.00 |  entropy 1.422 | value 0.792 | policy_loss -0.018 | value_loss 0.003 | grad_norm 0.143
U 4 | F 001024 | FPS 0057 | D 18 | Reward:μσmM 0.75 0.59 -1.00 1.00 |  entropy 1.515 | value 0.762 | policy_loss 0.074 | value_loss 0.087 | grad_norm 0.127
U 5 | F 001280 | FPS 0059 | D 22 | Reward:μσmM 0.32 0.93 -1.00 1.00 |  entropy 1.667 | value 0.705 | policy_loss 0.091 | value_loss 0.120 | grad_norm 0.375
U 6 | F 001536 | FPS 0060 | D 26 | Reward:μσmM 0.15 0.94 -1.00 1.00 |  entropy 1.624 | value 0.459 | policy_loss 0.124 | value_loss 0.085 | grad_norm 0.115
U 7 | F 001792 | FPS 0060 | D 31 | Reward:μσmM 0.91 0.10 0.74 1.00 |  entropy 1.529 | value 0.694 | policy_loss -0.075 | value_loss 0.009 | grad_norm 0.112
U 8 | F 002048 | FPS 0061 | D 35 | Reward:μσmM 0.58 0.72 -1.00 1.00 |  entropy 1.611 | value 0.562 | policy_loss 0.025 | value_loss 0.070 | grad_norm 0.152
U 9 | F 002304 | FPS 0060 | D 39 | Reward:μσmM 0.91 0.12 0.74 1.00 |  entropy 1.663 | value 0.641 | policy_loss -0.052 | value_loss 0.006 | grad_norm 0.153
U 10 | F 002560 | FPS 0058 | D 43 | Reward:μσmM 0.46 0.85 -1.00 1.00 |  entropy 1.596 | value 0.654 | policy_loss 0.001 | value_loss 0.054 | grad_norm 0.134
U 11 | F 002816 | FPS 0058 | D 48 | Reward:μσmM 0.94 0.08 0.81 1.00 |  entropy 1.588 | value 0.611 | policy_loss -0.061 | value_loss 0.012 | grad_norm 0.118
U 12 | F 003072 | FPS 0064 | D 52 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.807 | value 0.384 | policy_loss 0.056 | value_loss 0.001 | grad_norm 0.136
U 13 | F 003328 | FPS 0059 | D 56 | Reward:μσmM 0.91 0.13 0.63 1.00 |  entropy 1.616 | value 0.668 | policy_loss -0.009 | value_loss 0.007 | grad_norm 0.118
U 14 | F 003584 | FPS 0060 | D 60 | Reward:μσmM 0.90 0.10 0.79 1.00 |  entropy 1.594 | value 0.750 | policy_loss 0.005 | value_loss 0.003 | grad_norm 0.082
U 15 | F 003840 | FPS 0056 | D 65 | Reward:μσmM 0.46 0.85 -1.00 1.00 |  entropy 1.632 | value 0.681 | policy_loss 0.099 | value_loss 0.172 | grad_norm 0.303
U 16 | F 004096 | FPS 0060 | D 69 | Reward:μσmM 0.64 0.67 -1.00 1.00 |  entropy 1.663 | value 0.621 | policy_loss 0.010 | value_loss 0.066 | grad_norm 0.126
U 17 | F 004352 | FPS 0058 | D 74 | Reward:μσmM 0.61 0.72 -1.00 1.00 |  entropy 1.622 | value 0.661 | policy_loss 0.048 | value_loss 0.102 | grad_norm 0.158
U 18 | F 004608 | FPS 0059 | D 78 | Reward:μσmM 0.60 0.72 -1.00 1.00 |  entropy 1.639 | value 0.648 | policy_loss 0.049 | value_loss 0.076 | grad_norm 0.335
U 19 | F 004864 | FPS 0065 | D 82 | Reward:μσmM 0.85 0.21 0.56 1.00 |  entropy 1.719 | value 0.560 | policy_loss 0.009 | value_loss 0.002 | grad_norm 0.067
U 20 | F 005120 | FPS 0059 | D 86 | Reward:μσmM 0.42 0.90 -1.00 1.00 |  entropy 1.606 | value 0.608 | policy_loss 0.097 | value_loss 0.098 | grad_norm 0.138
discover.py --algo ppo --env MiniGrid-ConfigWorld-v0 --lr 0.00025 --AnomalyNN test_8 --model 20240819-seed1 --test-interval 10 --task-config task2 --discover 1

Namespace(task_config='task2', discover=1, algo='ppo', env='MiniGrid-ConfigWorld-v0', model='20240819-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_8', epochs=32, batch_size=128, frames_per_proc=256, discount=0.99, lr=0.00025, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Optimizer loaded

Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', -0.6), ('std', 0.48989794855663565), ('min', -1.0), ('max', 0.0)])
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0053 | D 4 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.848 | value 0.414 | policy_loss 0.426 | value_loss 0.317 | grad_norm 0.737
U 2 | F 000512 | FPS 0063 | D 8 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.841 | value 0.310 | policy_loss 0.136 | value_loss 0.048 | grad_norm 0.272
U 3 | F 000768 | FPS 0062 | D 13 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.743 | value 0.057 | policy_loss 0.282 | value_loss 0.169 | grad_norm 0.291
U 4 | F 001024 | FPS 0061 | D 17 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.849 | value 0.124 | policy_loss 0.004 | value_loss 0.000 | grad_norm 0.035
U 5 | F 001280 | FPS 0058 | D 21 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.819 | value 0.027 | policy_loss 0.103 | value_loss 0.046 | grad_norm 0.069
U 6 | F 001536 | FPS 0063 | D 25 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.819 | value -0.001 | policy_loss 0.046 | value_loss 0.025 | grad_norm 0.050
U 7 | F 001792 | FPS 0061 | D 29 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.813 | value -0.152 | policy_loss 0.165 | value_loss 0.077 | grad_norm 0.150
U 8 | F 002048 | FPS 0063 | D 34 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.785 | value -0.197 | policy_loss 0.037 | value_loss 0.028 | grad_norm 0.099
U 9 | F 002304 | FPS 0065 | D 38 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.772 | value -0.219 | policy_loss 0.010 | value_loss 0.024 | grad_norm 0.099
U 10 | F 002560 | FPS 0063 | D 42 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.722 | value -0.295 | policy_loss 0.101 | value_loss 0.050 | grad_norm 0.477
U 11 | F 002816 | FPS 0064 | D 46 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.786 | value -0.291 | policy_loss 0.041 | value_loss 0.019 | grad_norm 0.139
U 12 | F 003072 | FPS 0063 | D 50 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.772 | value -0.271 | policy_loss -0.001 | value_loss 0.013 | grad_norm 0.127
U 13 | F 003328 | FPS 0061 | D 54 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.776 | value -0.213 | policy_loss -0.044 | value_loss 0.000 | grad_norm 0.042
U 14 | F 003584 | FPS 0063 | D 58 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.772 | value -0.349 | policy_loss 0.120 | value_loss 0.047 | grad_norm 0.140
U 15 | F 003840 | FPS 0063 | D 62 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.756 | value -0.342 | policy_loss -0.023 | value_loss 0.010 | grad_norm 0.086
U 16 | F 004096 | FPS 0065 | D 66 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.753 | value -0.256 | policy_loss -0.056 | value_loss 0.000 | grad_norm 0.048
U 17 | F 004352 | FPS 0065 | D 70 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.721 | value -0.217 | policy_loss -0.043 | value_loss 0.000 | grad_norm 0.057
U 18 | F 004608 | FPS 0060 | D 74 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.762 | value -0.289 | policy_loss 0.053 | value_loss 0.030 | grad_norm 0.071
U 19 | F 004864 | FPS 0061 | D 78 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.794 | value -0.250 | policy_loss -0.046 | value_loss 0.001 | grad_norm 0.060
U 20 | F 005120 | FPS 0063 | D 82 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.789 | value -0.222 | policy_loss -0.044 | value_loss 0.000 | grad_norm 0.045
Test 10 turns results: Start from 3, reward per episode: OrderedDict([('mean', 0.0), ('std', 0.0), ('min', 0.0), ('max', 0.0)])
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0061 | D 4 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.742 | value -0.243 | policy_loss 0.010 | value_loss 0.016 | grad_norm 0.061
U 2 | F 000512 | FPS 0063 | D 8 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.746 | value -0.203 | policy_loss -0.039 | value_loss 0.000 | grad_norm 0.055
U 3 | F 000768 | FPS 0065 | D 12 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.772 | value -0.167 | policy_loss -0.036 | value_loss 0.000 | grad_norm 0.039
U 4 | F 001024 | FPS 0061 | D 16 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.753 | value -0.197 | policy_loss 0.019 | value_loss 0.011 | grad_norm 0.057
U 5 | F 001280 | FPS 0064 | D 20 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.874 | value -0.197 | policy_loss 0.020 | value_loss 0.017 | grad_norm 0.037
U 6 | F 001536 | FPS 0063 | D 24 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.827 | value -0.301 | policy_loss 0.149 | value_loss 0.055 | grad_norm 0.144
U 7 | F 001792 | FPS 0061 | D 28 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.816 | value -0.317 | policy_loss -0.071 | value_loss 0.001 | grad_norm 0.082
U 8 | F 002048 | FPS 0059 | D 32 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.832 | value -0.347 | policy_loss 0.055 | value_loss 0.034 | grad_norm 0.095
U 9 | F 002304 | FPS 0064 | D 36 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.799 | value -0.289 | policy_loss -0.056 | value_loss 0.000 | grad_norm 0.062
U 10 | F 002560 | FPS 0065 | D 40 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.793 | value -0.239 | policy_loss -0.052 | value_loss 0.000 | grad_norm 0.043
U 11 | F 002816 | FPS 0065 | D 44 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.788 | value -0.252 | policy_loss 0.008 | value_loss 0.014 | grad_norm 0.059
U 12 | F 003072 | FPS 0064 | D 48 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.816 | value -0.220 | policy_loss 0.017 | value_loss 0.020 | grad_norm 0.065
U 13 | F 003328 | FPS 0066 | D 52 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.840 | value -0.247 | policy_loss 0.011 | value_loss 0.017 | grad_norm 0.053
U 14 | F 003584 | FPS 0062 | D 56 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.828 | value -0.197 | policy_loss -0.042 | value_loss 0.000 | grad_norm 0.055
U 15 | F 003840 | FPS 0064 | D 60 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.842 | value -0.172 | policy_loss -0.034 | value_loss 0.000 | grad_norm 0.033
U 16 | F 004096 | FPS 0064 | D 64 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.830 | value -0.260 | policy_loss 0.072 | value_loss 0.044 | grad_norm 0.110
U 17 | F 004352 | FPS 0061 | D 69 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.834 | value -0.362 | policy_loss 0.071 | value_loss 0.028 | grad_norm 0.061
U 18 | F 004608 | FPS 0064 | D 73 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.755 | value -0.226 | policy_loss -0.057 | value_loss 0.001 | grad_norm 0.088
U 19 | F 004864 | FPS 0061 | D 77 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.769 | value -0.227 | policy_loss 0.010 | value_loss 0.018 | grad_norm 0.063
U 20 | F 005120 | FPS 0063 | D 81 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.813 | value -0.183 | policy_loss -0.039 | value_loss 0.000 | grad_norm 0.091
Test 10 turns results: Start from 3, reward per episode: OrderedDict([('mean', 0.0), ('std', 0.0), ('min', 0.0), ('max', 0.0)])
Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', -0.6), ('std', 0.48989794855663565), ('min', -1.0), ('max', 0.0)])
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0060 | D 4 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.830 | value -0.159 | policy_loss -0.033 | value_loss 0.000 | grad_norm 0.035
U 2 | F 000512 | FPS 0062 | D 8 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.808 | value -0.137 | policy_loss -0.026 | value_loss 0.000 | grad_norm 0.021
U 3 | F 000768 | FPS 0062 | D 12 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.824 | value -0.126 | policy_loss -0.025 | value_loss 0.000 | grad_norm 0.020
U 4 | F 001024 | FPS 0063 | D 16 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.850 | value -0.160 | policy_loss 0.031 | value_loss 0.019 | grad_norm 0.051
U 5 | F 001280 | FPS 0062 | D 20 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.833 | value -0.232 | policy_loss 0.077 | value_loss 0.035 | grad_norm 0.057
U 6 | F 001536 | FPS 0060 | D 24 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.810 | value -0.174 | policy_loss -0.044 | value_loss 0.000 | grad_norm 0.042
U 7 | F 001792 | FPS 0061 | D 29 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.759 | value -0.200 | policy_loss 0.019 | value_loss 0.014 | grad_norm 0.066
U 8 | F 002048 | FPS 0063 | D 33 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.816 | value -0.154 | policy_loss -0.038 | value_loss 0.001 | grad_norm 0.087
U 9 | F 002304 | FPS 0061 | D 37 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.821 | value -0.184 | policy_loss 0.022 | value_loss 0.019 | grad_norm 0.061
U 10 | F 002560 | FPS 0061 | D 41 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.856 | value -0.153 | policy_loss -0.022 | value_loss 0.001 | grad_norm 0.033
U 11 | F 002816 | FPS 0059 | D 45 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.805 | value -0.300 | policy_loss 0.140 | value_loss 0.054 | grad_norm 0.154
U 12 | F 003072 | FPS 0060 | D 50 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.796 | value -0.388 | policy_loss 0.041 | value_loss 0.018 | grad_norm 0.086
U 13 | F 003328 | FPS 0062 | D 54 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.801 | value -0.320 | policy_loss -0.020 | value_loss 0.006 | grad_norm 0.169
U 14 | F 003584 | FPS 0061 | D 58 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.766 | value -0.257 | policy_loss -0.054 | value_loss 0.000 | grad_norm 0.064
U 15 | F 003840 | FPS 0063 | D 62 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.781 | value -0.270 | policy_loss 0.001 | value_loss 0.017 | grad_norm 0.042
U 16 | F 004096 | FPS 0062 | D 66 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.777 | value -0.237 | policy_loss -0.040 | value_loss 0.000 | grad_norm 0.037
U 17 | F 004352 | FPS 0062 | D 70 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.745 | value -0.328 | policy_loss 0.021 | value_loss 0.018 | grad_norm 0.094
U 18 | F 004608 | FPS 0061 | D 74 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.773 | value -0.210 | policy_loss -0.040 | value_loss 0.000 | grad_norm 0.068
U 19 | F 004864 | FPS 0061 | D 79 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.746 | value -0.173 | policy_loss -0.034 | value_loss 0.000 | grad_norm 0.057
U 20 | F 005120 | FPS 0064 | D 83 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.783 | value -0.145 | policy_loss -0.028 | value_loss 0.000 | grad_norm 0.022
Test 10 turns results: Start from 3, reward per episode: OrderedDict([('mean', 0.0), ('std', 0.0), ('min', 0.0), ('max', 0.0)])
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0057 | D 4 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.776 | value -0.126 | policy_loss -0.024 | value_loss 0.000 | grad_norm 0.025
U 2 | F 000512 | FPS 0062 | D 8 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.800 | value -0.105 | policy_loss -0.021 | value_loss 0.000 | grad_norm 0.011
U 3 | F 000768 | FPS 0064 | D 12 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.837 | value -0.147 | policy_loss 0.037 | value_loss 0.023 | grad_norm 0.060
U 4 | F 001024 | FPS 0060 | D 16 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.812 | value -0.189 | policy_loss 0.071 | value_loss 0.032 | grad_norm 0.166
U 5 | F 001280 | FPS 0060 | D 21 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.847 | value -0.195 | policy_loss 0.015 | value_loss 0.020 | grad_norm 0.072
U 6 | F 001536 | FPS 0065 | D 25 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.846 | value -0.142 | policy_loss -0.029 | value_loss 0.000 | grad_norm 0.032
U 7 | F 001792 | FPS 0062 | D 29 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.855 | value -0.176 | policy_loss 0.025 | value_loss 0.018 | grad_norm 0.023
U 8 | F 002048 | FPS 0063 | D 33 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.845 | value -0.224 | policy_loss -0.031 | value_loss 0.001 | grad_norm 0.052
U 9 | F 002304 | FPS 0063 | D 37 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.816 | value -0.294 | policy_loss 0.150 | value_loss 0.048 | grad_norm 0.076
U 10 | F 002560 | FPS 0064 | D 41 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.806 | value -0.214 | policy_loss -0.046 | value_loss 0.001 | grad_norm 0.093
U 11 | F 002816 | FPS 0063 | D 45 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.816 | value -0.241 | policy_loss 0.007 | value_loss 0.004 | grad_norm 0.027
U 12 | F 003072 | FPS 0063 | D 49 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.823 | value -0.303 | policy_loss 0.083 | value_loss 0.039 | grad_norm 0.084
U 13 | F 003328 | FPS 0062 | D 53 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.837 | value -0.339 | policy_loss -0.010 | value_loss 0.003 | grad_norm 0.096
U 14 | F 003584 | FPS 0064 | D 57 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.836 | value -0.314 | policy_loss 0.055 | value_loss 0.021 | grad_norm 0.131
U 15 | F 003840 | FPS 0064 | D 61 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.800 | value -0.418 | policy_loss 0.171 | value_loss 0.041 | grad_norm 0.184
U 16 | F 004096 | FPS 0065 | D 65 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.739 | value -0.359 | policy_loss -0.038 | value_loss 0.013 | grad_norm 0.084
U 17 | F 004352 | FPS 0064 | D 69 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.732 | value -0.397 | policy_loss 0.021 | value_loss 0.011 | grad_norm 0.060
U 18 | F 004608 | FPS 0065 | D 73 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.723 | value -0.337 | policy_loss -0.054 | value_loss 0.001 | grad_norm 0.128
U 19 | F 004864 | FPS 0063 | D 77 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.746 | value -0.265 | policy_loss -0.054 | value_loss 0.000 | grad_norm 0.068
U 20 | F 005120 | FPS 0063 | D 81 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.570 | value -0.218 | policy_loss -0.045 | value_loss 0.000 | grad_norm 0.057
Test 10 turns results: Start from 3, reward per episode: OrderedDict([('mean', 0.0), ('std', 0.0), ('min', 0.0), ('max', 0.0)])
Test 10 turns results: Start from 3, reward per episode: OrderedDict([('mean', 0.0), ('std', 0.0), ('min', 0.0), ('max', 0.0)])
Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', -0.4), ('std', 0.4898979485566356), ('min', -1.0), ('max', 0.0)])
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0062 | D 4 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.639 | value -0.233 | policy_loss 0.010 | value_loss 0.011 | grad_norm 0.099
U 2 | F 000512 | FPS 0065 | D 8 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.568 | value -0.232 | policy_loss -0.049 | value_loss 0.002 | grad_norm 0.192
U 3 | F 000768 | FPS 0066 | D 11 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.760 | value -0.235 | policy_loss 0.008 | value_loss 0.016 | grad_norm 0.067
U 4 | F 001024 | FPS 0063 | D 16 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.780 | value -0.251 | policy_loss 0.013 | value_loss 0.018 | grad_norm 0.182
U 5 | F 001280 | FPS 0061 | D 20 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.762 | value -0.352 | policy_loss 0.104 | value_loss 0.043 | grad_norm 0.147
U 6 | F 001536 | FPS 0064 | D 24 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.816 | value -0.382 | policy_loss 0.023 | value_loss 0.022 | grad_norm 0.094
U 7 | F 001792 | FPS 0061 | D 28 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.787 | value -0.437 | policy_loss 0.008 | value_loss 0.004 | grad_norm 0.037
U 8 | F 002048 | FPS 0064 | D 32 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.754 | value -0.345 | policy_loss -0.066 | value_loss 0.001 | grad_norm 0.052
U 9 | F 002304 | FPS 0061 | D 36 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.724 | value -0.306 | policy_loss -0.027 | value_loss 0.012 | grad_norm 0.075
U 10 | F 002560 | FPS 0063 | D 40 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.760 | value -0.261 | policy_loss -0.060 | value_loss 0.000 | grad_norm 0.088
U 11 | F 002816 | FPS 0055 | D 45 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.737 | value -0.221 | policy_loss -0.047 | value_loss 0.000 | grad_norm 0.074
U 12 | F 003072 | FPS 0058 | D 49 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.773 | value -0.188 | policy_loss -0.038 | value_loss 0.000 | grad_norm 0.020
U 13 | F 003328 | FPS 0062 | D 53 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.763 | value -0.158 | policy_loss -0.031 | value_loss 0.000 | grad_norm 0.019
U 14 | F 003584 | FPS 0063 | D 57 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.793 | value -0.186 | policy_loss 0.024 | value_loss 0.018 | grad_norm 0.050
U 15 | F 003840 | FPS 0065 | D 61 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.860 | value -0.196 | policy_loss 0.020 | value_loss 0.019 | grad_norm 0.059
U 16 | F 004096 | FPS 0064 | D 65 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.826 | value -0.177 | policy_loss -0.041 | value_loss 0.000 | grad_norm 0.030
U 17 | F 004352 | FPS 0063 | D 69 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.887 | value -0.139 | policy_loss -0.029 | value_loss 0.000 | grad_norm 0.025
U 18 | F 004608 | FPS 0062 | D 74 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.868 | value -0.236 | policy_loss 0.095 | value_loss 0.041 | grad_norm 0.131
U 19 | F 004864 | FPS 0063 | D 78 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.851 | value -0.291 | policy_loss 0.094 | value_loss 0.040 | grad_norm 0.160
U 20 | F 005120 | FPS 0064 | D 82 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.806 | value -0.261 | policy_loss -0.003 | value_loss 0.015 | grad_norm 0.136
Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', -0.4), ('std', 0.4898979485566356), ('min', -1.0), ('max', 0.0)])
Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', -0.5), ('std', 0.5), ('min', -1.0), ('max', 0.0)])
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0056 | D 4 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.765 | value -0.355 | policy_loss -0.040 | value_loss 0.004 | grad_norm 0.113
U 2 | F 000512 | FPS 0062 | D 8 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.797 | value -0.287 | policy_loss 0.035 | value_loss 0.027 | grad_norm 0.064
U 3 | F 000768 | FPS 0064 | D 12 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.772 | value -0.218 | policy_loss -0.059 | value_loss 0.001 | grad_norm 0.111
U 4 | F 001024 | FPS 0062 | D 16 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.744 | value -0.238 | policy_loss -0.003 | value_loss 0.008 | grad_norm 0.109
U 5 | F 001280 | FPS 0064 | D 20 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.730 | value -0.174 | policy_loss -0.034 | value_loss 0.000 | grad_norm 0.053
U 6 | F 001536 | FPS 0062 | D 24 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.731 | value -0.192 | policy_loss 0.021 | value_loss 0.017 | grad_norm 0.090
U 7 | F 001792 | FPS 0058 | D 29 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.771 | value -0.220 | policy_loss -0.012 | value_loss 0.013 | grad_norm 0.105
U 8 | F 002048 | FPS 0061 | D 33 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.743 | value -0.195 | policy_loss -0.050 | value_loss 0.001 | grad_norm 0.104
U 9 | F 002304 | FPS 0061 | D 37 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.801 | value -0.153 | policy_loss -0.035 | value_loss 0.000 | grad_norm 0.040
U 10 | F 002560 | FPS 0063 | D 41 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.788 | value -0.129 | policy_loss -0.027 | value_loss 0.000 | grad_norm 0.020
U 11 | F 002816 | FPS 0056 | D 46 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.840 | value -0.108 | policy_loss -0.021 | value_loss 0.000 | grad_norm 0.026
U 12 | F 003072 | FPS 0061 | D 50 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.810 | value -0.189 | policy_loss 0.089 | value_loss 0.032 | grad_norm 0.131
U 13 | F 003328 | FPS 0064 | D 54 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.786 | value -0.221 | policy_loss 0.025 | value_loss 0.015 | grad_norm 0.079
U 14 | F 003584 | FPS 0063 | D 58 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.741 | value -0.192 | policy_loss -0.040 | value_loss 0.002 | grad_norm 0.131
U 15 | F 003840 | FPS 0058 | D 63 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.786 | value -0.164 | policy_loss -0.042 | value_loss 0.000 | grad_norm 0.044
U 16 | F 004096 | FPS 0053 | D 67 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.704 | value -0.151 | policy_loss -0.032 | value_loss 0.001 | grad_norm 0.148
U 17 | F 004352 | FPS 0062 | D 72 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.795 | value -0.122 | policy_loss -0.025 | value_loss 0.000 | grad_norm 0.034
U 18 | F 004608 | FPS 0063 | D 76 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.827 | value -0.099 | policy_loss -0.020 | value_loss 0.000 | grad_norm 0.013
U 19 | F 004864 | FPS 0061 | D 80 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.858 | value -0.091 | policy_loss -0.011 | value_loss 0.006 | grad_norm 0.014
U 20 | F 005120 | FPS 0063 | D 84 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.900 | value -0.079 | policy_loss -0.016 | value_loss 0.000 | grad_norm 0.011
Test 10 turns results: Start from 3, reward per episode: OrderedDict([('mean', -0.2), ('std', 0.4000000000000001), ('min', -1.0), ('max', 0.0)])
Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', -0.4), ('std', 0.4898979485566356), ('min', -1.0), ('max', 0.0)])
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0060 | D 4 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.851 | value -0.120 | policy_loss 0.039 | value_loss 0.024 | grad_norm 0.060
U 2 | F 000512 | FPS 0063 | D 8 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.911 | value -0.058 | policy_loss -0.016 | value_loss 0.000 | grad_norm 0.016
U 3 | F 000768 | FPS 0063 | D 12 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.894 | value -0.093 | policy_loss 0.042 | value_loss 0.017 | grad_norm 0.048
U 4 | F 001024 | FPS 0057 | D 16 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.810 | value -0.226 | policy_loss 0.071 | value_loss 0.023 | grad_norm 0.063
U 5 | F 001280 | FPS 0061 | D 21 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.786 | value -0.240 | policy_loss 0.111 | value_loss 0.043 | grad_norm 0.161
U 6 | F 001536 | FPS 0062 | D 25 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.800 | value -0.350 | policy_loss 0.040 | value_loss 0.007 | grad_norm 0.056
U 7 | F 001792 | FPS 0065 | D 29 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.763 | value -0.281 | policy_loss 0.009 | value_loss 0.013 | grad_norm 0.061
U 8 | F 002048 | FPS 0062 | D 33 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.808 | value -0.391 | policy_loss 0.102 | value_loss 0.039 | grad_norm 0.245
U 9 | F 002304 | FPS 0063 | D 37 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.717 | value -0.288 | policy_loss -0.071 | value_loss 0.001 | grad_norm 0.089
U 10 | F 002560 | FPS 0063 | D 41 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.686 | value -0.313 | policy_loss -0.008 | value_loss 0.009 | grad_norm 0.057
U 11 | F 002816 | FPS 0065 | D 45 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.675 | value -0.349 | policy_loss -0.056 | value_loss 0.000 | grad_norm 0.081
U 12 | F 003072 | FPS 0063 | D 49 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.764 | value -0.190 | policy_loss -0.040 | value_loss 0.000 | grad_norm 0.075
U 13 | F 003328 | FPS 0062 | D 53 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.818 | value -0.267 | policy_loss 0.064 | value_loss 0.032 | grad_norm 0.096
U 14 | F 003584 | FPS 0058 | D 58 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.766 | value -0.243 | policy_loss -0.144 | value_loss 0.037 | grad_norm 0.192
U 15 | F 003840 | FPS 0062 | D 62 | Reward:μσmM -0.33 0.94 -1.00 1.00 |  entropy 1.713 | value -0.170 | policy_loss -0.061 | value_loss 0.038 | grad_norm 0.119
U 16 | F 004096 | FPS 0064 | D 66 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.714 | value -0.353 | policy_loss 0.025 | value_loss 0.037 | grad_norm 0.264
U 17 | F 004352 | FPS 0060 | D 70 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.764 | value -0.222 | policy_loss -0.019 | value_loss 0.016 | grad_norm 0.123
U 18 | F 004608 | FPS 0055 | D 75 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.627 | value -0.146 | policy_loss -0.091 | value_loss 0.030 | grad_norm 0.246
U 19 | F 004864 | FPS 0058 | D 79 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.633 | value -0.134 | policy_loss -0.134 | value_loss 0.034 | grad_norm 0.273
U 20 | F 005120 | FPS 0057 | D 84 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.692 | value -0.283 | policy_loss -0.086 | value_loss 0.009 | grad_norm 0.139
Test 10 turns results: Start from 3, reward per episode: OrderedDict([('mean', 0.0), ('std', 0.0), ('min', 0.0), ('max', 0.0)])
Test 10 turns results: Start from 3, reward per episode: OrderedDict([('mean', 0.0), ('std', 0.0), ('min', 0.0), ('max', 0.0)])
Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', -0.23742105364799498), ('std', 0.531782369536587), ('min', -1.0), ('max', 0.62578946352005)])
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0062 | D 4 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.639 | value -0.215 | policy_loss -0.096 | value_loss 0.001 | grad_norm 0.095
U 2 | F 000512 | FPS 0057 | D 8 | Reward:μσmM 0.33 0.94 -1.00 1.00 |  entropy 1.597 | value -0.074 | policy_loss -0.054 | value_loss 0.019 | grad_norm 0.138
U 3 | F 000768 | FPS 0060 | D 12 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.608 | value -0.237 | policy_loss -0.068 | value_loss 0.004 | grad_norm 0.160
U 4 | F 001024 | FPS 0062 | D 17 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.550 | value -0.196 | policy_loss -0.060 | value_loss 0.004 | grad_norm 0.063
U 5 | F 001280 | FPS 0057 | D 21 | Reward:μσmM 0.33 0.94 -1.00 1.00 |  entropy 1.631 | value -0.100 | policy_loss 0.004 | value_loss 0.019 | grad_norm 0.058
U 6 | F 001536 | FPS 0059 | D 25 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.616 | value -0.064 | policy_loss -0.055 | value_loss 0.007 | grad_norm 0.081
U 7 | F 001792 | FPS 0058 | D 30 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.633 | value 0.089 | policy_loss 0.062 | value_loss 0.079 | grad_norm 0.057
U 8 | F 002048 | FPS 0060 | D 34 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.654 | value 0.216 | policy_loss -0.008 | value_loss 0.003 | grad_norm 0.082
U 9 | F 002304 | FPS 0063 | D 38 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.701 | value 0.007 | policy_loss 0.008 | value_loss 0.010 | grad_norm 0.051
U 10 | F 002560 | FPS 0059 | D 43 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.792 | value 0.061 | policy_loss 0.018 | value_loss 0.016 | grad_norm 0.106
U 11 | F 002816 | FPS 0055 | D 47 | Reward:μσmM 0.33 0.94 -1.00 1.00 |  entropy 1.774 | value 0.104 | policy_loss -0.022 | value_loss 0.011 | grad_norm 0.115
U 12 | F 003072 | FPS 0055 | D 52 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.739 | value 0.238 | policy_loss 0.071 | value_loss 0.067 | grad_norm 0.158
U 13 | F 003328 | FPS 0060 | D 56 | Reward:μσmM -0.20 0.98 -1.00 1.00 |  entropy 1.755 | value -0.023 | policy_loss 0.031 | value_loss 0.072 | grad_norm 0.159
U 14 | F 003584 | FPS 0060 | D 60 | Reward:μσmM -0.33 0.94 -1.00 1.00 |  entropy 1.808 | value 0.293 | policy_loss 0.055 | value_loss 0.036 | grad_norm 0.165
U 15 | F 003840 | FPS 0062 | D 65 | Reward:μσmM -0.33 0.94 -1.00 1.00 |  entropy 1.722 | value 0.334 | policy_loss 0.075 | value_loss 0.049 | grad_norm 0.189
U 16 | F 004096 | FPS 0064 | D 69 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.756 | value 0.492 | policy_loss 0.054 | value_loss 0.002 | grad_norm 0.120
U 17 | F 004352 | FPS 0062 | D 73 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.756 | value 0.343 | policy_loss 0.014 | value_loss 0.002 | grad_norm 0.126
U 18 | F 004608 | FPS 0061 | D 77 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.748 | value 0.037 | policy_loss -0.047 | value_loss 0.006 | grad_norm 0.096
U 19 | F 004864 | FPS 0061 | D 81 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.698 | value 0.051 | policy_loss 0.019 | value_loss 0.050 | grad_norm 0.148
U 20 | F 005120 | FPS 0063 | D 85 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.765 | value 0.338 | policy_loss 0.157 | value_loss 0.082 | grad_norm 0.165
Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', -0.5), ('std', 0.5), ('min', -1.0), ('max', 0.0)])
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0063 | D 4 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.752 | value -0.066 | policy_loss -0.050 | value_loss 0.006 | grad_norm 0.124
U 2 | F 000512 | FPS 0065 | D 8 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.625 | value -0.256 | policy_loss -0.091 | value_loss 0.001 | grad_norm 0.214
U 3 | F 000768 | FPS 0063 | D 12 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.625 | value -0.293 | policy_loss -0.069 | value_loss 0.028 | grad_norm 0.125
U 4 | F 001024 | FPS 0064 | D 16 | Reward:μσmM 0.33 0.94 -1.00 1.00 |  entropy 1.613 | value -0.038 | policy_loss -0.056 | value_loss 0.017 | grad_norm 0.175
U 5 | F 001280 | FPS 0063 | D 20 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.674 | value -0.063 | policy_loss -0.042 | value_loss 0.004 | grad_norm 0.093
U 6 | F 001536 | FPS 0065 | D 24 | Reward:μσmM -0.33 0.94 -1.00 1.00 |  entropy 1.691 | value 0.027 | policy_loss 0.113 | value_loss 0.131 | grad_norm 0.394
U 7 | F 001792 | FPS 0065 | D 28 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.679 | value -0.012 | policy_loss -0.054 | value_loss 0.004 | grad_norm 0.145
U 8 | F 002048 | FPS 0063 | D 32 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.710 | value -0.027 | policy_loss -0.060 | value_loss 0.002 | grad_norm 0.108
U 9 | F 002304 | FPS 0064 | D 36 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.729 | value -0.061 | policy_loss -0.038 | value_loss 0.003 | grad_norm 0.041
U 10 | F 002560 | FPS 0062 | D 40 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.739 | value 0.029 | policy_loss -0.029 | value_loss 0.003 | grad_norm 0.070
U 11 | F 002816 | FPS 0063 | D 44 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.783 | value 0.063 | policy_loss -0.029 | value_loss 0.002 | grad_norm 0.036
U 12 | F 003072 | FPS 0065 | D 48 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.770 | value 0.113 | policy_loss 0.046 | value_loss 0.019 | grad_norm 0.068
U 13 | F 003328 | FPS 0062 | D 52 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.746 | value 0.511 | policy_loss 0.318 | value_loss 0.230 | grad_norm 0.262
U 14 | F 003584 | FPS 0065 | D 56 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.755 | value 0.325 | policy_loss 0.213 | value_loss 0.099 | grad_norm 0.153
U 15 | F 003840 | FPS 0062 | D 60 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.765 | value 0.164 | policy_loss 0.185 | value_loss 0.093 | grad_norm 0.117
U 16 | F 004096 | FPS 0063 | D 64 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.825 | value 0.068 | policy_loss -0.058 | value_loss 0.009 | grad_norm 0.033
U 17 | F 004352 | FPS 0064 | D 68 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.827 | value 0.053 | policy_loss 0.148 | value_loss 0.060 | grad_norm 0.179
U 18 | F 004608 | FPS 0064 | D 72 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.845 | value -0.013 | policy_loss 0.121 | value_loss 0.045 | grad_norm 0.043
U 19 | F 004864 | FPS 0064 | D 76 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.816 | value -0.209 | policy_loss 0.293 | value_loss 0.109 | grad_norm 0.229
U 20 | F 005120 | FPS 0064 | D 80 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.821 | value -0.305 | policy_loss 0.111 | value_loss 0.037 | grad_norm 0.074
Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', -0.19639473557472228), ('std', 0.5673073921401023), ('min', -1.0), ('max', 0.7015789747238159)])
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0064 | D 3 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.742 | value -0.372 | policy_loss 0.065 | value_loss 0.027 | grad_norm 0.167
U 2 | F 000512 | FPS 0062 | D 8 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.767 | value -0.326 | policy_loss -0.018 | value_loss 0.007 | grad_norm 0.079
U 3 | F 000768 | FPS 0065 | D 12 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.734 | value -0.364 | policy_loss 0.021 | value_loss 0.027 | grad_norm 0.115
U 4 | F 001024 | FPS 0065 | D 15 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.676 | value -0.327 | policy_loss -0.071 | value_loss 0.001 | grad_norm 0.090
U 5 | F 001280 | FPS 0064 | D 19 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.724 | value -0.259 | policy_loss -0.046 | value_loss 0.000 | grad_norm 0.062
U 6 | F 001536 | FPS 0062 | D 24 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.692 | value -0.216 | policy_loss -0.044 | value_loss 0.000 | grad_norm 0.045
U 7 | F 001792 | FPS 0063 | D 28 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.707 | value -0.182 | policy_loss -0.036 | value_loss 0.000 | grad_norm 0.051
U 8 | F 002048 | FPS 0063 | D 32 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.699 | value -0.156 | policy_loss -0.032 | value_loss 0.000 | grad_norm 0.029
U 9 | F 002304 | FPS 0063 | D 36 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.635 | value -0.222 | policy_loss 0.054 | value_loss 0.030 | grad_norm 0.102
U 10 | F 002560 | FPS 0061 | D 40 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.763 | value -0.141 | policy_loss -0.132 | value_loss 0.025 | grad_norm 0.137
U 11 | F 002816 | FPS 0062 | D 44 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.700 | value -0.108 | policy_loss -0.024 | value_loss 0.001 | grad_norm 0.094
U 12 | F 003072 | FPS 0063 | D 48 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.720 | value -0.045 | policy_loss -0.099 | value_loss 0.014 | grad_norm 0.091
U 13 | F 003328 | FPS 0063 | D 52 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.855 | value -0.067 | policy_loss -0.012 | value_loss 0.000 | grad_norm 0.013
U 14 | F 003584 | FPS 0061 | D 56 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.725 | value 0.027 | policy_loss -0.091 | value_loss 0.010 | grad_norm 0.062
U 15 | F 003840 | FPS 0061 | D 61 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.725 | value 0.081 | policy_loss -0.075 | value_loss 0.004 | grad_norm 0.093
U 16 | F 004096 | FPS 0062 | D 65 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.794 | value 0.049 | policy_loss -0.068 | value_loss 0.005 | grad_norm 0.106
U 17 | F 004352 | FPS 0059 | D 69 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.524 | value 0.256 | policy_loss 0.023 | value_loss 0.036 | grad_norm 0.113
U 18 | F 004608 | FPS 0060 | D 73 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.581 | value 0.531 | policy_loss 0.094 | value_loss 0.023 | grad_norm 0.132
U 19 | F 004864 | FPS 0061 | D 78 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.591 | value 0.134 | policy_loss 0.016 | value_loss 0.008 | grad_norm 0.194
U 20 | F 005120 | FPS 0063 | D 82 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.718 | value -0.220 | policy_loss -0.034 | value_loss 0.004 | grad_norm 0.080
Test 10 turns results: Start from 3, reward per episode: OrderedDict([('mean', 0.0), ('std', 0.0), ('min', 0.0), ('max', 0.0)])
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0059 | D 4 | Reward:μσmM 0.33 0.94 -1.00 1.00 |  entropy 1.729 | value 0.106 | policy_loss 0.024 | value_loss 0.006 | grad_norm 0.049
U 2 | F 000512 | FPS 0062 | D 8 | Reward:μσmM -0.33 0.94 -1.00 1.00 |  entropy 1.667 | value 0.326 | policy_loss 0.195 | value_loss 0.113 | grad_norm 0.487
U 3 | F 000768 | FPS 0060 | D 12 | Reward:μσmM -0.20 0.98 -1.00 1.00 |  entropy 1.618 | value 0.186 | policy_loss 0.103 | value_loss 0.107 | grad_norm 0.143
U 4 | F 001024 | FPS 0059 | D 17 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.682 | value -0.132 | policy_loss -0.072 | value_loss 0.004 | grad_norm 0.115
U 5 | F 001280 | FPS 0061 | D 21 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.652 | value -0.134 | policy_loss -0.061 | value_loss 0.003 | grad_norm 0.047
U 6 | F 001536 | FPS 0057 | D 25 | Reward:μσmM 0.33 0.94 -1.00 1.00 |  entropy 1.652 | value 0.185 | policy_loss 0.032 | value_loss 0.009 | grad_norm 0.065
U 7 | F 001792 | FPS 0058 | D 30 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.686 | value 0.159 | policy_loss -0.008 | value_loss 0.010 | grad_norm 0.146
U 8 | F 002048 | FPS 0057 | D 34 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.656 | value 0.139 | policy_loss 0.066 | value_loss 0.059 | grad_norm 0.102
U 9 | F 002304 | FPS 0060 | D 38 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.755 | value -0.034 | policy_loss -0.055 | value_loss 0.002 | grad_norm 0.106
U 10 | F 002560 | FPS 0058 | D 43 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.701 | value 0.360 | policy_loss 0.016 | value_loss 0.004 | grad_norm 0.109
U 11 | F 002816 | FPS 0056 | D 47 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.775 | value 0.048 | policy_loss 0.001 | value_loss 0.033 | grad_norm 0.032
U 12 | F 003072 | FPS 0060 | D 52 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.773 | value 0.129 | policy_loss -0.023 | value_loss 0.002 | grad_norm 0.044
U 13 | F 003328 | FPS 0059 | D 56 | Reward:μσmM 0.33 0.94 -1.00 1.00 |  entropy 1.740 | value 0.157 | policy_loss 0.023 | value_loss 0.020 | grad_norm 0.051
U 14 | F 003584 | FPS 0061 | D 60 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.715 | value 0.059 | policy_loss -0.017 | value_loss 0.003 | grad_norm 0.055
U 15 | F 003840 | FPS 0063 | D 64 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.673 | value 0.093 | policy_loss -0.049 | value_loss 0.018 | grad_norm 0.085
U 16 | F 004096 | FPS 0060 | D 69 | Reward:μσmM -0.43 0.90 -1.00 1.00 |  entropy 1.699 | value 0.209 | policy_loss 0.149 | value_loss 0.175 | grad_norm 0.232
U 17 | F 004352 | FPS 0064 | D 73 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.698 | value 0.202 | policy_loss -0.025 | value_loss 0.002 | grad_norm 0.055
U 18 | F 004608 | FPS 0063 | D 77 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.599 | value 0.124 | policy_loss -0.031 | value_loss 0.003 | grad_norm 0.100
U 19 | F 004864 | FPS 0060 | D 81 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.780 | value 0.062 | policy_loss -0.037 | value_loss 0.001 | grad_norm 0.076
U 20 | F 005120 | FPS 0061 | D 85 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.785 | value 0.109 | policy_loss -0.020 | value_loss 0.001 | grad_norm 0.051
Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', -0.9), ('std', 0.3), ('min', -1.0), ('max', 0.0)])
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0054 | D 4 | Reward:μσmM 0.33 0.94 -1.00 1.00 |  entropy 1.725 | value 0.233 | policy_loss 0.025 | value_loss 0.027 | grad_norm 0.110
U 2 | F 000512 | FPS 0060 | D 9 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.725 | value 0.089 | policy_loss -0.017 | value_loss 0.002 | grad_norm 0.051
U 3 | F 000768 | FPS 0061 | D 13 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.741 | value 0.290 | policy_loss 0.060 | value_loss 0.018 | grad_norm 0.049
U 4 | F 001024 | FPS 0060 | D 17 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.653 | value 0.391 | policy_loss 0.469 | value_loss 0.336 | grad_norm 0.304
U 5 | F 001280 | FPS 0061 | D 21 | Reward:μσmM -0.50 0.87 -1.00 1.00 |  entropy 1.768 | value 0.163 | policy_loss 0.101 | value_loss 0.119 | grad_norm 0.070
U 6 | F 001536 | FPS 0060 | D 25 | Reward:μσmM -0.33 0.94 -1.00 1.00 |  entropy 1.776 | value 0.265 | policy_loss 0.017 | value_loss 0.034 | grad_norm 0.096
U 7 | F 001792 | FPS 0059 | D 30 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.709 | value 0.204 | policy_loss -0.080 | value_loss 0.005 | grad_norm 0.090
U 8 | F 002048 | FPS 0062 | D 34 | Reward:μσmM -0.33 0.94 -1.00 1.00 |  entropy 1.738 | value 0.187 | policy_loss 0.027 | value_loss 0.028 | grad_norm 0.124
U 9 | F 002304 | FPS 0062 | D 38 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.803 | value 0.181 | policy_loss 0.094 | value_loss 0.047 | grad_norm 0.114
U 10 | F 002560 | FPS 0059 | D 42 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.759 | value 0.327 | policy_loss 0.183 | value_loss 0.130 | grad_norm 0.178
U 11 | F 002816 | FPS 0056 | D 47 | Reward:μσmM -0.71 0.70 -1.00 1.00 |  entropy 1.775 | value 0.094 | policy_loss 0.251 | value_loss 0.201 | grad_norm 0.197
U 12 | F 003072 | FPS 0062 | D 51 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.793 | value -0.001 | policy_loss 0.102 | value_loss 0.051 | grad_norm 0.136
U 13 | F 003328 | FPS 0061 | D 55 | Reward:μσmM -0.33 0.94 -1.00 1.00 |  entropy 1.799 | value -0.118 | policy_loss -0.014 | value_loss 0.046 | grad_norm 0.126
U 14 | F 003584 | FPS 0060 | D 60 | Reward:μσmM -0.67 0.75 -1.00 1.00 |  entropy 1.773 | value 0.060 | policy_loss 0.111 | value_loss 0.139 | grad_norm 0.231
U 15 | F 003840 | FPS 0061 | D 64 | Reward:μσmM -0.50 0.87 -1.00 1.00 |  entropy 1.738 | value -0.072 | policy_loss 0.027 | value_loss 0.089 | grad_norm 0.223
U 16 | F 004096 | FPS 0060 | D 68 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.686 | value -0.037 | policy_loss -0.076 | value_loss 0.060 | grad_norm 0.211
U 17 | F 004352 | FPS 0058 | D 72 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.756 | value -0.099 | policy_loss -0.111 | value_loss 0.010 | grad_norm 0.127
U 18 | F 004608 | FPS 0062 | D 77 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.713 | value 0.308 | policy_loss -0.026 | value_loss 0.015 | grad_norm 0.150
U 19 | F 004864 | FPS 0058 | D 81 | Reward:μσmM 0.33 0.94 -1.00 1.00 |  entropy 1.679 | value 0.096 | policy_loss -0.089 | value_loss 0.019 | grad_norm 0.195
U 20 | F 005120 | FPS 0063 | D 85 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.747 | value 0.007 | policy_loss -0.066 | value_loss 0.002 | grad_norm 0.065
Test 10 turns results: Start from 3, reward per episode: OrderedDict([('mean', 0.0), ('std', 0.0), ('min', 0.0), ('max', 0.0)])
Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', -0.3840789467096329), ('std', 0.5049932297300292), ('min', -1.0), ('max', 0.15921053290367126)])
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0061 | D 4 | Reward:μσmM 0.33 0.94 -1.00 1.00 |  entropy 1.674 | value 0.041 | policy_loss -0.013 | value_loss 0.012 | grad_norm 0.105
U 2 | F 000512 | FPS 0064 | D 8 | Reward:μσmM 0.89 0.15 0.68 1.00 |  entropy 1.658 | value 0.127 | policy_loss -0.112 | value_loss 0.021 | grad_norm 0.083
U 3 | F 000768 | FPS 0061 | D 12 | Reward:μσmM 0.33 0.94 -1.00 1.00 |  entropy 1.666 | value 0.240 | policy_loss 0.046 | value_loss 0.017 | grad_norm 0.058
U 4 | F 001024 | FPS 0060 | D 16 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.695 | value 0.210 | policy_loss 0.128 | value_loss 0.079 | grad_norm 0.100
U 5 | F 001280 | FPS 0064 | D 20 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.769 | value -0.029 | policy_loss -0.044 | value_loss 0.002 | grad_norm 0.030
U 6 | F 001536 | FPS 0066 | D 24 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.730 | value 0.044 | policy_loss 0.008 | value_loss 0.022 | grad_norm 0.168
U 7 | F 001792 | FPS 0063 | D 28 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.768 | value 0.060 | policy_loss -0.028 | value_loss 0.001 | grad_norm 0.054
U 8 | F 002048 | FPS 0062 | D 32 | Reward:μσmM 0.33 0.94 -1.00 1.00 |  entropy 1.795 | value 0.080 | policy_loss 0.021 | value_loss 0.016 | grad_norm 0.078
U 9 | F 002304 | FPS 0062 | D 36 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.818 | value 0.063 | policy_loss -0.025 | value_loss 0.003 | grad_norm 0.050
U 10 | F 002560 | FPS 0060 | D 41 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.824 | value 0.201 | policy_loss 0.082 | value_loss 0.085 | grad_norm 0.191
U 11 | F 002816 | FPS 0063 | D 45 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.823 | value 0.384 | policy_loss 0.025 | value_loss 0.004 | grad_norm 0.046
U 12 | F 003072 | FPS 0064 | D 49 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.798 | value 0.382 | policy_loss 0.204 | value_loss 0.115 | grad_norm 0.237
U 13 | F 003328 | FPS 0064 | D 53 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.778 | value 0.239 | policy_loss 0.125 | value_loss 0.051 | grad_norm 0.127
U 14 | F 003584 | FPS 0060 | D 57 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.751 | value 0.060 | policy_loss 0.341 | value_loss 0.194 | grad_norm 0.195
U 15 | F 003840 | FPS 0062 | D 61 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.775 | value -0.114 | policy_loss 0.175 | value_loss 0.076 | grad_norm 0.102
U 16 | F 004096 | FPS 0062 | D 65 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.791 | value -0.137 | policy_loss 0.092 | value_loss 0.042 | grad_norm 0.166
U 17 | F 004352 | FPS 0062 | D 69 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.766 | value -0.236 | policy_loss 0.126 | value_loss 0.052 | grad_norm 0.276
U 18 | F 004608 | FPS 0059 | D 74 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.734 | value -0.193 | policy_loss -0.146 | value_loss 0.034 | grad_norm 0.171
U 19 | F 004864 | FPS 0063 | D 78 | Reward:μσmM -1.00 0.00 -1.00 -1.00 |  entropy 1.716 | value -0.266 | policy_loss 0.019 | value_loss 0.020 | grad_norm 0.338
U 20 | F 005120 | FPS 0060 | D 82 | Reward:μσmM 0.75 0.25 0.50 1.00 |  entropy 1.678 | value -0.098 | policy_loss -0.181 | value_loss 0.030 | grad_norm 0.178
Test 10 turns results: Start from 2, reward per episode: OrderedDict([('mean', -0.3), ('std', 0.45825756949558394), ('min', -1.0), ('max', 0.0)])
Optimizer loaded

Start discovering in 5000 steps.

U 1 | F 000256 | FPS 0059 | D 4 | Reward:μσmM 0.75 0.25 0.49 1.00 |  entropy 1.656 | value 0.059 | policy_loss -0.141 | value_loss 0.013 | grad_norm 0.152
U 2 | F 000512 | FPS 0065 | D 8 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.719 | value 0.112 | policy_loss -0.050 | value_loss 0.003 | grad_norm 0.079
U 3 | F 000768 | FPS 0063 | D 12 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.800 | value 0.105 | policy_loss -0.050 | value_loss 0.005 | grad_norm 0.147
U 4 | F 001024 | FPS 0062 | D 16 | Reward:μσmM 1.00 0.00 1.00 1.00 |  entropy 1.802 | value 0.102 | policy_loss -0.021 | value_loss 0.001 | grad_norm 0.026
U 5 | F 001280 | FPS 0061 | D 20 | Reward:μσmM 0.89 0.16 0.66 1.00 |  entropy 1.565 | value 0.276 | policy_loss -0.066 | value_loss 0.006 | grad_norm 0.093
U 6 | F 001536 | FPS 0060 | D 24 | Reward:μσmM 0.00 1.00 -1.00 1.00 |  entropy 1.563 | value 0.193 | policy_loss 0.149 | value_loss 0.102 | grad_norm 0.156
U 7 | F 001792 | FPS 0057 | D 29 | Reward:μσmM 0.66 0.68 -1.00 1.00 |  entropy 1.446 | value 0.438 | policy_loss -0.119 | value_loss 0.026 | grad_norm 0.173
U 8 | F 002048 | FPS 0060 | D 33 | Reward:μσmM 0.94 0.07 0.83 1.00 |  entropy 1.613 | value 0.402 | policy_loss -0.090 | value_loss 0.012 | grad_norm 0.166
U 9 | F 002304 | FPS 0056 | D 38 | Reward:μσmM 0.46 0.84 -1.00 1.00 |  entropy 1.576 | value 0.641 | policy_loss 0.005 | value_loss 0.064 | grad_norm 0.310
U 10 | F 002560 | FPS 0062 | D 42 | Reward:μσmM -0.33 0.94 -1.00 1.00 |  entropy 1.697 | value 0.477 | policy_loss 0.110 | value_loss 0.019 | grad_norm 0.116
U 11 | F 002816 | FPS 0063 | D 46 | Reward:μσmM 0.83 0.17 0.66 1.00 |  entropy 1.639 | value 0.543 | policy_loss 0.008 | value_loss 0.008 | grad_norm 0.146
U 12 | F 003072 | FPS 0063 | D 50 | Reward:μσmM 0.66 0.00 0.66 0.66 |  entropy 1.789 | value 0.466 | policy_loss 0.072 | value_loss 0.002 | grad_norm 0.098
U 13 | F 003328 | FPS 0059 | D 54 | Reward:μσmM 0.52 0.77 -1.00 1.00 |  entropy 1.581 | value 0.493 | policy_loss 0.018 | value_loss 0.056 | grad_norm 0.195
U 14 | F 003584 | FPS 0061 | D 59 | Reward:μσmM 0.91 0.11 0.69 1.00 |  entropy 1.502 | value 0.560 | policy_loss -0.113 | value_loss 0.010 | grad_norm 0.171
U 15 | F 003840 | FPS 0054 | D 63 | Reward:μσmM 0.94 0.06 0.87 1.00 |  entropy 1.232 | value 0.826 | policy_loss -0.072 | value_loss 0.002 | grad_norm 0.194
U 16 | F 004096 | FPS 0054 | D 68 | Reward:μσmM 0.94 0.06 0.82 1.00 |  entropy 1.339 | value 0.814 | policy_loss -0.043 | value_loss 0.003 | grad_norm 0.164
U 17 | F 004352 | FPS 0053 | D 73 | Reward:μσmM 0.76 0.56 -1.00 1.00 |  entropy 1.397 | value 0.809 | policy_loss -0.007 | value_loss 0.015 | grad_norm 0.114
U 18 | F 004608 | FPS 0054 | D 78 | Reward:μσmM 0.77 0.56 -1.00 1.00 |  entropy 1.503 | value 0.789 | policy_loss 0.062 | value_loss 0.083 | grad_norm 0.116
U 19 | F 004864 | FPS 0056 | D 82 | Reward:μσmM 0.72 0.61 -1.00 1.00 |  entropy 1.525 | value 0.751 | policy_loss 0.007 | value_loss 0.022 | grad_norm 0.151
U 20 | F 005120 | FPS 0058 | D 87 | Reward:μσmM 0.93 0.10 0.70 1.00 |  entropy 1.586 | value 0.687 | policy_loss -0.031 | value_loss 0.004 | grad_norm 0.133
U 211 | F 054016 | FPS 0044 | D 5 | Reward:μσmM 2.49 0.75 1.00 2.91 | policy_loss ['None', 'None', '0.044', '0.025', '-0.761'] | value_loss ['None', 'None', '0.001', '0.014', '0.584']
U 212 | F 054272 | FPS 0052 | D 10 | Reward:μσmM 2.36 0.79 1.00 2.92 | policy_loss ['None', 'None', '0.081', '0.027', '-0.767'] | value_loss ['None', 'None', '0.002', '0.003', '0.478']
U 213 | F 054528 | FPS 0047 | D 16 | Reward:μσmM 1.95 0.76 1.00 2.85 | policy_loss ['None', 'None', '0.204', '0.114', '-0.993'] | value_loss ['None', 'None', '0.091', '0.004', '0.517']
U 214 | F 054784 | FPS 0047 | D 21 | Reward:μσmM 0.75 0.43 0.00 1.00 | policy_loss ['None', 'None', '0.505', '0.255', '0.396'] | value_loss ['None', 'None', '0.330', '0.187', '0.674']
U 215 | F 055040 | FPS 0049 | D 26 | Reward:μσmM 0.33 0.47 0.00 1.00 | policy_loss ['None', 'None', 'None', '0.652', '0.279'] | value_loss ['None', 'None', 'None', '0.749', '0.098']
U 216 | F 055296 | FPS 0047 | D 32 | Reward:μσmM 1.83 1.29 0.00 2.80 | policy_loss ['None', 'None', '-0.243', '-0.192', '0.166'] | value_loss ['None', 'None', '0.011', '0.011', '0.014']
U 217 | F 055552 | FPS 0046 | D 37 | Reward:μσmM 1.21 1.09 0.00 2.64 | policy_loss ['None', 'None', '0.293', '-0.093', '0.103'] | value_loss ['None', 'None', '0.162', '0.007', '0.018']
U 218 | F 055808 | FPS 0047 | D 43 | Reward:μσmM 1.50 0.50 1.00 2.00 | policy_loss ['None', 'None', '0.062', '0.186', '-0.131'] | value_loss ['None', 'None', '0.025', '0.038', '0.004']
U 219 | F 056064 | FPS 0052 | D 48 | Reward:μσmM 0.94 1.12 0.00 2.75 | policy_loss ['None', 'None', '-0.140', '0.325', '0.199'] | value_loss ['None', 'None', '0.004', '0.349', '0.055']
U 220 | F 056320 | FPS 0049 | D 53 | Reward:μσmM 0.45 1.42 -1.00 2.81 | policy_loss ['None', 'None', '-0.122', '-0.070', '0.265'] | value_loss ['None', 'None', '0.002', '0.228', '0.147']
U 10 | Test reward:μσmM -0.30 0.46 -1.00 0.00 | Test num frames:μσmM 211.70 98.95 109.00 380.00
Status saved
U 221 | F 056576 | FPS 0048 | D 116 | Reward:μσmM 1.00 0.00 1.00 1.00 | policy_loss ['None', 'None', '1.123', '0.094', '0.035'] | value_loss ['None', 'None', '1.464', '0.090', '0.016']
U 222 | F 056832 | FPS 0052 | D 121 | Reward:μσmM 1.00 0.82 0.00 2.00 | policy_loss ['None', 'None', '0.497', '0.307', '-0.087'] | value_loss ['None', 'None', '0.357', '0.269', '0.006']
U 223 | F 057088 | FPS 0053 | D 126 | Reward:μσmM -0.25 0.83 -1.00 1.00 | policy_loss ['None', 'None', '0.124', '-0.015', '0.351'] | value_loss ['None', 'None', '0.032', '0.002', '0.612']
U 224 | F 057344 | FPS 0048 | D 131 | Reward:μσmM 0.50 0.50 0.00 1.00 | policy_loss ['None', 'None', '0.089', '0.062', '-0.067'] | value_loss ['None', 'None', '0.017', '0.001', '0.024']
U 225 | F 057600 | FPS 0049 | D 137 | Reward:μσmM 1.50 0.50 1.00 2.00 | policy_loss ['None', 'None', '-0.072', '0.168', '-0.105'] | value_loss ['None', 'None', '0.016', '0.015', '0.009']
U 226 | F 057856 | FPS 0052 | D 142 | Reward:μσmM 0.50 0.50 0.00 1.00 | policy_loss ['None', 'None', 'None', '0.177', '0.076'] | value_loss ['None', 'None', 'None', '0.079', '0.008']
U 227 | F 058112 | FPS 0053 | D 146 | Reward:μσmM 0.67 0.47 0.00 1.00 | policy_loss ['None', 'None', '0.121', '0.037', '0.186'] | value_loss ['None', 'None', '0.022', '0.105', '0.062']
U 228 | F 058368 | FPS 0048 | D 152 | Reward:μσmM 0.50 0.50 0.00 1.00 | policy_loss ['None', 'None', '0.121', '0.011', '0.105'] | value_loss ['None', 'None', '0.036', '0.002', '0.009']
U 229 | F 058624 | FPS 0049 | D 157 | Reward:μσmM -0.06 1.43 -1.00 2.69 | policy_loss ['None', 'None', '-0.379', '-0.032', '0.318'] | value_loss ['None', 'None', '0.069', '0.001', '0.239']
U 230 | F 058880 | FPS 0047 | D 162 | Reward:μσmM 0.43 1.55 -1.00 2.71 | policy_loss ['None', 'None', '-0.273', '-0.038', '0.394'] | value_loss ['None', 'None', '0.039', '0.005', '0.480']
U 10 | Test reward:μσmM 0.00 0.00 0.00 0.00 | Test num frames:μσmM 242.00 104.74 128.00 380.00
Status saved
U 231 | F 059136 | FPS 0054 | D 226 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', 'None', 'None', '0.061'] | value_loss ['None', 'None', 'None', 'None', '0.009']
U 232 | F 059392 | FPS 0049 | D 232 | Reward:μσmM 1.14 1.68 -1.00 2.83 | policy_loss ['None', 'None', '-0.476', '-0.290', '0.019'] | value_loss ['None', 'None', '0.105', '0.064', '0.157']
U 233 | F 059648 | FPS 0051 | D 237 | Reward:μσmM 1.86 1.65 -1.00 2.84 | policy_loss ['None', 'None', '-0.160', '-0.608', '-0.237'] | value_loss ['None', 'None', '0.027', '0.131', '0.429']
U 234 | F 059904 | FPS 0053 | D 241 | Reward:μσmM 2.50 0.75 1.00 2.91 | policy_loss ['None', 'None', '-0.119', '-0.273', '-0.788'] | value_loss ['None', 'None', '0.013', '0.042', '0.370']
U 235 | F 060160 | FPS 0050 | D 247 | Reward:μσmM 2.63 0.67 1.00 2.94 | policy_loss ['None', 'None', '-0.083', '-0.306', '-0.834'] | value_loss ['None', 'None', '0.002', '0.037', '0.402']
U 236 | F 060416 | FPS 0047 | D 252 | Reward:μσmM 2.56 0.97 0.00 2.95 | policy_loss ['None', 'None', '-0.039', '-0.221', '-0.562'] | value_loss ['None', 'None', '0.001', '0.008', '0.284']
U 237 | F 060672 | FPS 0049 | D 257 | Reward:μσmM 2.60 0.92 0.00 2.95 | policy_loss ['None', 'None', '-0.009', '-0.022', '-0.642'] | value_loss ['None', 'None', '0.002', '0.006', '0.311']
U 238 | F 060928 | FPS 0048 | D 263 | Reward:μσmM 2.81 0.31 2.00 2.95 | policy_loss ['None', 'None', '0.000', '-0.027', '-0.277'] | value_loss ['None', 'None', '0.001', '0.001', '0.064']
U 239 | F 061184 | FPS 0050 | D 268 | Reward:μσmM 2.92 0.02 2.90 2.94 | policy_loss ['None', 'None', '0.027', '0.028', '-0.190'] | value_loss ['None', 'None', '0.001', '0.001', '0.021']
U 240 | F 061440 | FPS 0050 | D 273 | Reward:μσmM 2.48 0.74 1.00 2.91 | policy_loss ['None', 'None', '-0.003', '0.155', '0.061'] | value_loss ['None', 'None', '0.003', '0.006', '0.014']
U 10 | Test reward:μσmM -0.10 0.30 -1.00 0.00 | Test num frames:μσmM 230.40 121.76 12.00 380.00
Status saved
U 241 | F 061696 | FPS 0049 | D 333 | Reward:μσmM 1.20 0.40 1.00 2.00 | policy_loss ['None', 'None', '0.787', '0.287', '0.285'] | value_loss ['None', 'None', '0.789', '0.139', '0.120']
U 242 | F 061952 | FPS 0052 | D 338 | Reward:μσmM 1.00 1.00 0.00 2.00 | policy_loss ['None', 'None', '0.009', '0.399', '0.264'] | value_loss ['None', 'None', '0.000', '0.359', '0.026']
U 243 | F 062208 | FPS 0057 | D 342 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', 'None', 'None', '0.267'] | value_loss ['None', 'None', 'None', 'None', '0.023']
U 244 | F 062464 | FPS 0051 | D 347 | Reward:μσmM 1.96 0.77 1.00 2.89 | policy_loss ['None', 'None', '0.058', '0.079', '-0.110'] | value_loss ['None', 'None', '0.100', '0.029', '0.014']
U 245 | F 062720 | FPS 0050 | D 352 | Reward:μσmM 2.39 0.39 2.00 2.78 | policy_loss ['None', 'None', '-0.030', '0.062', '0.022'] | value_loss ['None', 'None', '0.008', '0.008', '0.002']
U 246 | F 062976 | FPS 0051 | D 357 | Reward:μσmM 1.67 1.22 0.00 2.85 | policy_loss ['None', 'None', '0.095', '-0.206', '0.052'] | value_loss ['None', 'None', '0.088', '0.006', '0.016']
U 247 | F 063232 | FPS 0051 | D 362 | Reward:μσmM 2.28 1.14 0.00 2.87 | policy_loss ['None', 'None', '-0.146', '-0.144', '-0.137'] | value_loss ['None', 'None', '0.005', '0.021', '0.010']
U 248 | F 063488 | FPS 0051 | D 367 | Reward:μσmM 2.08 1.16 0.00 2.93 | policy_loss ['None', 'None', '-0.069', '0.026', '-0.114'] | value_loss ['None', 'None', '0.002', '0.340', '0.083']
U 249 | F 063744 | FPS 0052 | D 372 | Reward:μσmM 2.40 1.07 0.00 2.93 | policy_loss ['None', 'None', '-0.073', '-0.236', '-0.124'] | value_loss ['None', 'None', '0.005', '0.023', '0.039']
U 250 | F 064000 | FPS 0047 | D 378 | Reward:μσmM 2.72 0.37 2.00 2.94 | policy_loss ['None', 'None', '0.026', '-0.195', '-0.294'] | value_loss ['None', 'None', '0.010', '0.018', '0.022']
U 10 | Test reward:μσmM -0.30 0.46 -1.00 0.00 | Test num frames:μσmM 192.10 126.03 10.00 380.00
Status saved
U 251 | F 064256 | FPS 0048 | D 431 | Reward:μσmM 2.93 0.01 2.92 2.95 | policy_loss ['None', 'None', '-0.116', '-0.102', '-0.217'] | value_loss ['None', 'None', '0.002', '0.003', '0.008']
U 252 | F 064512 | FPS 0051 | D 436 | Reward:μσmM 2.35 1.17 0.00 2.95 | policy_loss ['None', 'None', '-0.043', '0.200', '0.033'] | value_loss ['None', 'None', '0.000', '0.522', '0.134']
U 253 | F 064768 | FPS 0052 | D 441 | Reward:μσmM 2.79 0.32 2.00 2.95 | policy_loss ['None', 'None', '0.034', '-0.098', '0.007'] | value_loss ['None', 'None', '0.007', '0.011', '0.012']
U 254 | F 065024 | FPS 0051 | D 446 | Reward:μσmM 2.50 1.02 0.00 2.94 | policy_loss ['None', 'None', '-0.023', '-0.032', '0.060'] | value_loss ['None', 'None', '0.001', '0.005', '0.020']
U 255 | F 065280 | FPS 0049 | D 451 | Reward:μσmM 2.83 0.29 2.00 2.95 | policy_loss ['None', 'None', '-0.014', '-0.048', '-0.147'] | value_loss ['None', 'None', '0.000', '0.001', '0.005']
U 256 | F 065536 | FPS 0051 | D 456 | Reward:μσmM 2.79 0.32 2.00 2.96 | policy_loss ['None', 'None', '0.005', '0.072', '-0.054'] | value_loss ['None', 'None', '0.002', '0.007', '0.006']
U 257 | F 065792 | FPS 0044 | D 462 | Reward:μσmM 2.68 0.64 1.00 2.95 | policy_loss ['None', 'None', '0.001', '0.012', '-0.015'] | value_loss ['None', 'None', '0.003', '0.002', '0.002']
U 258 | F 066048 | FPS 0049 | D 467 | Reward:μσmM 2.07 1.31 0.00 2.93 | policy_loss ['None', 'None', '-0.004', '0.405', '0.290'] | value_loss ['None', 'None', '0.002', '0.796', '0.320']
U 259 | F 066304 | FPS 0051 | D 472 | Reward:μσmM 1.67 1.22 0.00 2.87 | policy_loss ['None', 'None', '0.230', '0.019', '0.153'] | value_loss ['None', 'None', '0.149', '0.009', '0.015']
U 260 | F 066560 | FPS 0049 | D 477 | Reward:μσmM 0.11 1.29 -1.00 2.78 | policy_loss ['None', 'None', '0.075', '0.809', '0.669'] | value_loss ['None', 'None', '0.132', '0.950', '1.261']
U 10 | Test reward:μσmM -0.90 0.30 -1.00 0.00 | Test num frames:μσmM 77.40 54.08 9.00 194.00
Status saved
U 261 | F 066816 | FPS 0046 | D 515 | Reward:μσmM -0.17 1.07 -1.00 2.00 | policy_loss ['None', 'None', '-0.063', '0.477', '0.714'] | value_loss ['None', 'None', '0.000', '0.757', '1.057']
U 262 | F 067072 | FPS 0051 | D 520 | Reward:μσmM 0.69 1.71 -1.00 2.74 | policy_loss ['None', 'None', '-0.050', '-0.037', '0.488'] | value_loss ['None', 'None', '0.003', '0.011', '0.725']
U 263 | F 067328 | FPS 0050 | D 526 | Reward:μσmM -0.08 1.40 -1.00 2.60 | policy_loss ['None', 'None', '0.030', '-0.115', '0.325'] | value_loss ['None', 'None', '0.000', '0.010', '0.370']
U 264 | F 067584 | FPS 0050 | D 531 | Reward:μσmM 0.12 1.41 -1.00 2.83 | policy_loss ['None', 'None', '0.205', '-0.002', '0.226'] | value_loss ['None', 'None', '0.219', '0.028', '0.599']
U 265 | F 067840 | FPS 0047 | D 536 | Reward:μσmM -0.20 1.17 -1.00 2.00 | policy_loss ['None', 'None', '0.054', '0.734', '0.503'] | value_loss ['None', 'None', '0.000', '0.984', '0.762']
U 266 | F 068096 | FPS 0050 | D 541 | Reward:μσmM -0.41 1.22 -1.00 2.70 | policy_loss ['None', 'None', '-0.059', '0.011', '0.691'] | value_loss ['None', 'None', '0.002', '0.001', '0.993']
U 267 | F 068352 | FPS 0051 | D 546 | Reward:μσmM 1.61 1.19 0.00 2.84 | policy_loss ['None', 'None', '-0.042', '0.225', '-0.432'] | value_loss ['None', 'None', '0.009', '0.265', '0.172']
U 268 | F 068608 | FPS 0047 | D 552 | Reward:μσmM 0.70 1.40 -1.00 2.80 | policy_loss ['None', 'None', '-0.262', '0.131', '0.091'] | value_loss ['None', 'None', '0.016', '0.111', '0.296']
U 269 | F 068864 | FPS 0046 | D 557 | Reward:μσmM 0.58 1.16 0.00 2.90 | policy_loss ['None', 'None', '-0.124', '0.517', '0.115'] | value_loss ['None', 'None', '0.001', '0.819', '0.084']
U 270 | F 069120 | FPS 0049 | D 562 | Reward:μσmM 1.83 1.29 0.00 2.76 | policy_loss ['None', 'None', '0.063', '-0.628', '-0.003'] | value_loss ['None', 'None', '0.000', '0.211', '0.024']
U 10 | Test reward:μσmM -0.10 0.30 -1.00 0.00 | Test num frames:μσmM 230.00 122.48 8.00 380.00
Status saved
U 271 | F 069376 | FPS 0046 | D 623 | Reward:μσmM 1.28 1.64 -1.00 2.83 | policy_loss ['None', 'None', '0.028', '-0.354', '0.240'] | value_loss ['None', 'None', '0.001', '0.043', '0.381']
U 272 | F 069632 | FPS 0048 | D 628 | Reward:μσmM 2.16 0.83 1.00 2.82 | policy_loss ['None', 'None', '0.017', '-0.195', '-0.432'] | value_loss ['None', 'None', '0.001', '0.023', '0.107']
U 273 | F 069888 | FPS 0053 | D 633 | Reward:μσmM 1.70 0.75 1.00 2.78 | policy_loss ['None', 'None', '0.216', '0.136', '-0.256'] | value_loss ['None', 'None', '0.179', '0.139', '0.062']
U 274 | F 070144 | FPS 0052 | D 638 | Reward:μσmM 1.36 0.96 0.00 2.81 | policy_loss ['None', 'None', '0.312', '0.169', '-0.025'] | value_loss ['None', 'None', '0.326', '0.414', '0.068']
U 275 | F 070400 | FPS 0047 | D 643 | Reward:μσmM 1.18 1.21 0.00 2.73 | policy_loss ['None', 'None', '-0.110', '0.196', '0.294'] | value_loss ['None', 'None', '0.009', '0.438', '0.390']
U 276 | F 070656 | FPS 0052 | D 648 | Reward:μσmM 0.90 1.56 -1.00 2.83 | policy_loss ['None', 'None', '-0.122', '0.084', '0.295'] | value_loss ['None', 'None', '0.007', '0.305', '0.532']
U 277 | F 070912 | FPS 0051 | D 653 | Reward:μσmM 1.39 1.56 -1.00 2.79 | policy_loss ['None', 'None', '-0.105', '-0.069', '0.063'] | value_loss ['None', 'None', '0.002', '0.034', '0.268']
U 278 | F 071168 | FPS 0050 | D 658 | Reward:μσmM 0.71 1.23 0.00 2.84 | policy_loss ['None', 'None', '-0.046', '0.253', '0.178'] | value_loss ['None', 'None', '0.000', '0.570', '0.074']
U 279 | F 071424 | FPS 0054 | D 663 | Reward:μσmM 1.58 0.83 1.00 2.75 | policy_loss ['None', 'None', '0.296', '-0.070', '-0.364'] | value_loss ['None', 'None', '0.195', '0.025', '0.039']
U 280 | F 071680 | FPS 0053 | D 668 | Reward:μσmM 2.12 1.22 0.00 2.87 | policy_loss ['None', 'None', '-0.126', '-0.161', '-0.232'] | value_loss ['None', 'None', '0.005', '0.011', '0.039']
U 10 | Test reward:μσmM 0.00 0.00 0.00 0.00 | Test num frames:μσmM 242.00 104.74 128.00 380.00
Status saved
U 281 | F 071936 | FPS 0049 | D 733 | Reward:μσmM 2.28 1.14 0.00 2.88 | policy_loss ['None', 'None', '-0.067', '-0.204', '-0.331'] | value_loss ['None', 'None', '0.003', '0.004', '0.045']
U 282 | F 072192 | FPS 0048 | D 738 | Reward:μσmM 2.22 1.35 -1.00 2.92 | policy_loss ['None', 'None', '-0.057', '-0.068', '-0.309'] | value_loss ['None', 'None', '0.004', '0.004', '0.238']
U 283 | F 072448 | FPS 0046 | D 744 | Reward:μσmM 2.78 0.32 2.00 2.94 | policy_loss ['None', 'None', '-0.013', '-0.104', '-0.140'] | value_loss ['None', 'None', '0.005', '0.005', '0.006']
U 284 | F 072704 | FPS 0046 | D 750 | Reward:μσmM 2.81 0.31 2.00 2.94 | policy_loss ['None', 'None', '-0.059', '-0.044', '-0.144'] | value_loss ['None', 'None', '0.001', '0.003', '0.034']
U 285 | F 072960 | FPS 0047 | D 755 | Reward:μσmM 2.83 0.29 2.00 2.95 | policy_loss ['None', 'None', '-0.032', '-0.058', '-0.062'] | value_loss ['None', 'None', '0.001', '0.002', '0.004']
U 286 | F 073216 | FPS 0046 | D 761 | Reward:μσmM 2.84 0.28 2.00 2.95 | policy_loss ['None', 'None', '-0.010', '-0.049', '-0.086'] | value_loss ['None', 'None', '0.001', '0.001', '0.005']
U 287 | F 073472 | FPS 0047 | D 766 | Reward:μσmM 2.77 0.56 1.00 2.96 | policy_loss ['None', 'None', '-0.007', '-0.030', '-0.035'] | value_loss ['None', 'None', '0.001', '0.001', '0.001']
U 288 | F 073728 | FPS 0045 | D 772 | Reward:μσmM 2.72 0.61 1.00 2.95 | policy_loss ['None', 'None', '0.028', '0.018', '0.003'] | value_loss ['None', 'None', '0.004', '0.001', '0.001']
U 289 | F 073984 | FPS 0048 | D 777 | Reward:μσmM 2.60 0.92 0.00 2.95 | policy_loss ['None', 'None', '0.010', '0.021', '0.025'] | value_loss ['None', 'None', '0.000', '0.002', '0.001']
U 290 | F 074240 | FPS 0050 | D 782 | Reward:μσmM 2.85 0.04 2.79 2.90 | policy_loss ['None', 'None', '0.059', '0.050', '0.220'] | value_loss ['None', 'None', '0.001', '0.001', '0.013']
U 10 | Test reward:μσmM -0.60 0.49 -1.00 0.00 | Test num frames:μσmM 117.20 71.77 4.00 245.00
Status saved
U 291 | F 074496 | FPS 0043 | D 827 | Reward:μσmM 1.10 1.18 0.00 2.86 | policy_loss ['None', 'None', '0.209', '0.698', '0.337'] | value_loss ['None', 'None', '0.339', '1.079', '0.184']
U 292 | F 074752 | FPS 0049 | D 832 | Reward:μσmM 0.67 0.47 0.00 1.00 | policy_loss ['None', 'None', '0.859', '0.378', '0.255'] | value_loss ['None', 'None', '0.883', '0.100', '0.013']
U 293 | F 075008 | FPS 0049 | D 837 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', 'None', '0.760', '0.341'] | value_loss ['None', 'None', 'None', '0.814', '0.130']
U 294 | F 075264 | FPS 0059 | D 841 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', 'None', 'None', '0.231'] | value_loss ['None', 'None', 'None', 'None', '0.021']
U 295 | F 075520 | FPS 0056 | D 846 | Reward:μσmM -0.83 0.37 -1.00 0.00 | policy_loss ['None', 'None', 'None', 'None', '0.679'] | value_loss ['None', 'None', 'None', 'None', '0.840']
U 296 | F 075776 | FPS 0058 | D 850 | Reward:μσmM -0.80 0.40 -1.00 0.00 | policy_loss ['None', 'None', 'None', 'None', '0.462'] | value_loss ['None', 'None', 'None', 'None', '0.442']
discover.py --algo ppo --env MiniGrid-ConfigWorld-v0 --lr 0.00025 --AnomalyNN test_8 --model 20240819-seed1 --test-interval 10 --task-config task3 --discover 0 --frames-per-proc 512

Namespace(task_config='task3', discover=0, algo='ppo', env='MiniGrid-ConfigWorld-v0', model='20240819-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_8', epochs=32, batch_size=128, frames_per_proc=512, discount=0.99, lr=0.00025, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Optimizer loaded

Optimizer loaded

U 291 | F 074752 | FPS 0040 | D 12 | Reward:μσmM 1.31 1.24 0.00 2.88 | policy_loss ['None', 'None', '0.239', '0.265', '0.362'] | value_loss ['None', 'None', '0.189', '0.274', '0.155']
U 292 | F 075264 | FPS 0052 | D 22 | Reward:μσmM 0.78 1.13 0.00 2.71 | policy_loss ['None', 'None', '0.027', '0.851', '0.380'] | value_loss ['None', 'None', '0.002', '1.196', '0.153']
U 293 | F 075776 | FPS 0052 | D 32 | Reward:μσmM 0.00 0.00 0.00 0.00 | policy_loss ['None', 'None', 'None', '0.334', '0.228'] | value_loss ['None', 'None', 'None', '0.241', '0.008']
U 294 | F 076288 | FPS 0054 | D 41 | Reward:μσmM 1.52 1.47 -1.00 2.64 | policy_loss ['None', 'None', '0.064', '0.016', '0.456'] | value_loss ['None', 'None', '0.001', '0.021', '0.652']
U 295 | F 076800 | FPS 0054 | D 51 | Reward:μσmM 0.13 1.32 -1.00 2.88 | policy_loss ['None', 'None', '0.228', '0.113', '0.417'] | value_loss ['None', 'None', '0.143', '0.400', '0.439']
U 296 | F 077312 | FPS 0051 | D 61 | Reward:μσmM 0.75 1.24 -1.00 2.74 | policy_loss ['None', 'None', '0.262', '0.154', '0.191'] | value_loss ['None', 'None', '0.138', '0.261', '0.171']
U 297 | F 077824 | FPS 0052 | D 71 | Reward:μσmM -0.03 1.27 -1.00 2.73 | policy_loss ['None', 'None', '0.172', '0.004', '0.347'] | value_loss ['None', 'None', '0.132', '0.017', '0.458']
U 298 | F 078336 | FPS 0052 | D 81 | Reward:μσmM 0.54 1.40 -1.00 2.68 | policy_loss ['None', 'None', '0.034', '0.029', '0.212'] | value_loss ['None', 'None', '0.026', '0.004', '0.294']
U 299 | F 078848 | FPS 0050 | D 91 | Reward:μσmM 1.79 1.63 -1.00 2.90 | policy_loss ['None', 'None', '-0.177', '-0.248', '-0.318'] | value_loss ['None', 'None', '0.013', '0.040', '0.346']
U 300 | F 079360 | FPS 0049 | D 101 | Reward:μσmM 2.75 0.29 2.00 2.91 | policy_loss ['None', 'None', '-0.091', '-0.204', '-0.486'] | value_loss ['None', 'None', '0.006', '0.021', '0.118']
U 10 | Test reward:μσmM -0.40 0.49 -1.00 0.00 | Test num frames:μσmM 161.60 124.17 5.00 380.00
Status saved
U 301 | F 079872 | FPS 0049 | D 154 | Reward:μσmM 2.66 0.59 1.00 2.93 | policy_loss ['None', 'None', '-0.040', '-0.150', '-0.336'] | value_loss ['None', 'None', '0.004', '0.008', '0.081']
U 302 | F 080384 | FPS 0050 | D 164 | Reward:μσmM 2.81 0.26 2.00 2.92 | policy_loss ['None', 'None', '-0.035', '-0.080', '-0.397'] | value_loss ['None', 'None', '0.003', '0.004', '0.072']
U 303 | F 080896 | FPS 0051 | D 174 | Reward:μσmM 2.38 0.97 0.00 2.95 | policy_loss ['None', 'None', '0.116', '0.053', '-0.295'] | value_loss ['None', 'None', '0.203', '0.064', '0.050']
U 304 | F 081408 | FPS 0049 | D 185 | Reward:μσmM 2.46 0.89 0.00 2.93 | policy_loss ['None', 'None', '-0.095', '0.124', '-0.075'] | value_loss ['None', 'None', '0.044', '0.182', '0.140']
U 305 | F 081920 | FPS 0049 | D 195 | Reward:μσmM 2.09 1.23 0.00 2.92 | policy_loss ['None', 'None', '-0.123', '0.207', '0.186'] | value_loss ['None', 'None', '0.003', '0.409', '0.194']
U 306 | F 082432 | FPS 0054 | D 204 | Reward:μσmM 2.01 1.18 0.00 2.91 | policy_loss ['None', 'None', '0.182', '0.001', '0.080'] | value_loss ['None', 'None', '0.269', '0.175', '0.049']
U 307 | F 082944 | FPS 0053 | D 214 | Reward:μσmM 2.43 0.65 1.00 2.91 | policy_loss ['None', 'None', '0.065', '-0.043', '0.098'] | value_loss ['None', 'None', '0.066', '0.038', '0.023']
U 308 | F 083456 | FPS 0051 | D 224 | Reward:μσmM 2.76 0.27 2.00 2.89 | policy_loss ['None', 'None', '-0.065', '-0.062', '-0.099'] | value_loss ['None', 'None', '0.005', '0.009', '0.009']
U 309 | F 083968 | FPS 0049 | D 235 | Reward:μσmM 2.88 0.04 2.82 2.95 | policy_loss ['None', 'None', '-0.057', '-0.096', '-0.120'] | value_loss ['None', 'None', '0.003', '0.012', '0.016']
U 310 | F 084480 | FPS 0050 | D 245 | Reward:μσmM 2.70 0.75 0.00 2.94 | policy_loss ['None', 'None', '-0.037', '-0.139', '-0.143'] | value_loss ['None', 'None', '0.002', '0.005', '0.012']
U 10 | Test reward:μσmM -0.10 0.30 -1.00 0.00 | Test num frames:μσmM 232.40 118.27 32.00 380.00
Status saved
U 311 | F 084992 | FPS 0046 | D 314 | Reward:μσmM 2.59 0.84 0.00 2.95 | policy_loss ['None', 'None', '-0.024', '0.052', '-0.040'] | value_loss ['None', 'None', '0.001', '0.183', '0.070']
U 312 | F 085504 | FPS 0048 | D 325 | Reward:μσmM 2.49 1.14 -1.00 2.94 | policy_loss ['None', 'None', '-0.004', '-0.061', '-0.105'] | value_loss ['None', 'None', '0.001', '0.004', '0.053']
U 313 | F 086016 | FPS 0049 | D 336 | Reward:μσmM 2.72 0.73 0.00 2.95 | policy_loss ['None', 'None', '0.010', '-0.002', '-0.079'] | value_loss ['None', 'None', '0.002', '0.004', '0.019']
U 314 | F 086528 | FPS 0050 | D 346 | Reward:μσmM 2.64 0.80 0.00 2.95 | policy_loss ['None', 'None', '0.004', '-0.006', '0.167'] | value_loss ['None', 'None', '0.001', '0.110', '0.577']
U 315 | F 087040 | FPS 0049 | D 356 | Reward:μσmM 2.50 0.78 1.00 2.94 | policy_loss ['None', 'None', '0.139', '0.070', '-0.016'] | value_loss ['None', 'None', '0.128', '0.074', '0.019']
U 316 | F 087552 | FPS 0049 | D 367 | Reward:μσmM 2.21 1.04 0.00 2.90 | policy_loss ['None', 'None', '0.189', '0.091', '0.127'] | value_loss ['None', 'None', '0.155', '0.010', '0.007']
U 317 | F 088064 | FPS 0050 | D 377 | Reward:μσmM 2.10 1.19 0.00 2.90 | policy_loss ['None', 'None', '-0.057', '0.254', '0.236'] | value_loss ['None', 'None', '0.014', '0.426', '0.094']
U 318 | F 088576 | FPS 0053 | D 387 | Reward:μσmM 1.26 1.48 -1.00 2.87 | policy_loss ['None', 'None', '0.082', '0.174', '0.373'] | value_loss ['None', 'None', '0.102', '0.397', '0.415']
U 319 | F 089088 | FPS 0052 | D 397 | Reward:μσmM 1.78 1.08 0.00 2.85 | policy_loss ['None', 'None', '0.124', '0.160', '0.044'] | value_loss ['None', 'None', '0.207', '0.169', '0.022']
U 320 | F 089600 | FPS 0051 | D 407 | Reward:μσmM 1.48 1.50 -1.00 2.90 | policy_loss ['None', 'None', '0.000', '0.052', '0.129'] | value_loss ['None', 'None', '0.163', '0.037', '0.312']
U 10 | Test reward:μσmM -0.40 0.49 -1.00 0.00 | Test num frames:μσmM 176.70 102.79 20.00 380.00
Status saved
U 321 | F 090112 | FPS 0052 | D 463 | Reward:μσmM 2.42 0.71 1.00 2.83 | policy_loss ['None', 'None', '0.010', '-0.012', '0.011'] | value_loss ['None', 'None', '0.058', '0.013', '0.030']
U 322 | F 090624 | FPS 0052 | D 473 | Reward:μσmM 2.67 0.30 2.00 2.84 | policy_loss ['None', 'None', '-0.078', '-0.110', '-0.052'] | value_loss ['None', 'None', '0.007', '0.024', '0.011']
U 323 | F 091136 | FPS 0051 | D 483 | Reward:μσmM 2.58 0.86 0.00 2.93 | policy_loss ['None', 'None', '-0.078', '-0.181', '-0.226'] | value_loss ['None', 'None', '0.003', '0.016', '0.030']
U 324 | F 091648 | FPS 0050 | D 493 | Reward:μσmM 2.81 0.26 2.00 2.93 | policy_loss ['None', 'None', '-0.047', '-0.180', '-0.254'] | value_loss ['None', 'None', '0.005', '0.011', '0.023']
U 325 | F 092160 | FPS 0049 | D 503 | Reward:μσmM 2.78 0.49 1.00 2.94 | policy_loss ['None', 'None', '-0.050', '-0.082', '-0.233'] | value_loss ['None', 'None', '0.001', '0.007', '0.019']
U 326 | F 092672 | FPS 0050 | D 514 | Reward:μσmM 2.75 0.69 0.00 2.95 | policy_loss ['None', 'None', '-0.040', '-0.095', '-0.172'] | value_loss ['None', 'None', '0.001', '0.003', '0.004']
U 327 | F 093184 | FPS 0047 | D 525 | Reward:μσmM 2.88 0.21 2.00 2.95 | policy_loss ['None', 'None', '-0.043', '-0.031', '-0.084'] | value_loss ['None', 'None', '0.002', '0.002', '0.003']
U 328 | F 093696 | FPS 0048 | D 535 | Reward:μσmM 2.89 0.21 2.00 2.96 | policy_loss ['None', 'None', '-0.013', '-0.032', '-0.053'] | value_loss ['None', 'None', '0.003', '0.001', '0.001']
U 329 | F 094208 | FPS 0049 | D 546 | Reward:μσmM 2.79 0.64 0.00 2.95 | policy_loss ['None', 'None', '-0.007', '-0.012', '-0.015'] | value_loss ['None', 'None', '0.000', '0.001', '0.001']
U 330 | F 094720 | FPS 0051 | D 556 | Reward:μσmM 2.83 0.43 1.00 2.95 | policy_loss ['None', 'None', '0.010', '-0.038', '0.019'] | value_loss ['None', 'None', '0.001', '0.008', '0.004']
U 10 | Test reward:μσmM 0.00 0.00 0.00 0.00 | Test num frames:μσmM 242.00 104.74 128.00 380.00
Status saved
U 331 | F 095232 | FPS 0050 | D 628 | Reward:μσmM 2.46 0.89 0.00 2.94 | policy_loss ['None', 'None', '0.033', '0.243', '0.201'] | value_loss ['None', 'None', '0.030', '0.384', '0.224']
U 332 | F 095744 | FPS 0052 | D 638 | Reward:μσmM 2.45 0.94 0.00 2.92 | policy_loss ['None', 'None', '-0.001', '0.125', '0.096'] | value_loss ['None', 'None', '0.002', '0.221', '0.044']
U 333 | F 096256 | FPS 0050 | D 648 | Reward:μσmM 2.44 0.90 0.00 2.91 | policy_loss ['None', 'None', '0.029', '-0.008', '0.225'] | value_loss ['None', 'None', '0.002', '0.124', '0.119']
U 334 | F 096768 | FPS 0053 | D 658 | Reward:μσmM 2.24 1.04 0.00 2.91 | policy_loss ['None', 'None', '0.096', '0.027', '0.001'] | value_loss ['None', 'None', '0.079', '0.019', '0.010']
U 335 | F 097280 | FPS 0051 | D 668 | Reward:μσmM 1.69 1.35 -1.00 2.74 | policy_loss ['None', 'None', '0.099', '0.152', '0.012'] | value_loss ['None', 'None', '0.051', '0.026', '0.130']
U 336 | F 097792 | FPS 0052 | D 678 | Reward:μσmM 0.52 1.25 -1.00 2.85 | policy_loss ['None', 'None', '0.222', '0.461', '0.559'] | value_loss ['None', 'None', '0.179', '0.598', '0.570']
U 337 | F 098304 | FPS 0051 | D 688 | Reward:μσmM 1.05 1.69 -1.00 2.88 | policy_loss ['None', 'None', '0.098', '-0.080', '0.581'] | value_loss ['None', 'None', '0.077', '0.032', '1.262']
U 338 | F 098816 | FPS 0052 | D 697 | Reward:μσmM 0.60 1.49 -1.00 2.89 | policy_loss ['None', 'None', '-0.190', '0.477', '0.411'] | value_loss ['None', 'None', '0.015', '0.770', '0.560']
U 339 | F 099328 | FPS 0052 | D 707 | Reward:μσmM 0.77 1.79 -1.00 2.92 | policy_loss ['None', 'None', '-0.050', '-0.245', '0.248'] | value_loss ['None', 'None', '0.003', '0.456', '0.507']
U 340 | F 099840 | FPS 0051 | D 717 | Reward:μσmM 2.44 0.90 0.00 2.92 | policy_loss ['None', 'None', '0.001', '-0.162', '-0.206'] | value_loss ['None', 'None', '0.003', '0.341', '0.066']
U 10 | Test reward:μσmM -0.30 0.46 -1.00 0.00 | Test num frames:μσmM 187.40 123.01 15.00 380.00
Status saved
U 341 | F 100352 | FPS 0050 | D 776 | Reward:μσmM 2.26 1.32 -1.00 2.91 | policy_loss ['None', 'None', '-0.018', '-0.261', '-0.274'] | value_loss ['None', 'None', '0.001', '0.063', '0.214']
U 342 | F 100864 | FPS 0054 | D 785 | Reward:μσmM 2.81 0.26 2.00 2.92 | policy_loss ['None', 'None', '-0.013', '-0.135', '-0.398'] | value_loss ['None', 'None', '0.002', '0.003', '0.068']
U 343 | F 101376 | FPS 0051 | D 795 | Reward:μσmM 2.82 0.25 2.00 2.93 | policy_loss ['None', 'None', '-0.009', '-0.044', '-0.317'] | value_loss ['None', 'None', '0.001', '0.002', '0.037']
U 344 | F 101888 | FPS 0051 | D 805 | Reward:μσmM 2.59 0.84 0.00 2.95 | policy_loss ['None', 'None', '0.013', '0.035', '-0.206'] | value_loss ['None', 'None', '0.077', '0.075', '0.040']
U 345 | F 102400 | FPS 0050 | D 816 | Reward:μσmM 2.47 0.80 1.00 2.93 | policy_loss ['None', 'None', '0.156', '0.033', '-0.054'] | value_loss ['None', 'None', '0.193', '0.032', '0.018']
U 346 | F 102912 | FPS 0050 | D 826 | Reward:μσmM 2.65 0.80 0.00 2.92 | policy_loss ['None', 'None', '-0.103', '-0.012', '0.073'] | value_loss ['None', 'None', '0.004', '0.004', '0.015']
U 347 | F 103424 | FPS 0052 | D 836 | Reward:μσmM 2.21 1.19 0.00 2.92 | policy_loss ['None', 'None', '-0.032', '0.075', '0.153'] | value_loss ['None', 'None', '0.039', '0.182', '0.163']
U 348 | F 103936 | FPS 0049 | D 846 | Reward:μσmM 2.76 0.27 2.00 2.91 | policy_loss ['None', 'None', '-0.020', '-0.023', '-0.013'] | value_loss ['None', 'None', '0.003', '0.005', '0.012']
U 349 | F 104448 | FPS 0049 | D 857 | Reward:μσmM 2.21 1.35 -1.00 2.93 | policy_loss ['None', 'None', '-0.061', '0.142', '0.057'] | value_loss ['None', 'None', '0.003', '0.210', '0.249']
U 350 | F 104960 | FPS 0051 | D 867 | Reward:μσmM 2.64 0.67 1.00 2.94 | policy_loss ['None', 'None', '0.039', '-0.023', '-0.161'] | value_loss ['None', 'None', '0.090', '0.079', '0.049']
U 10 | Test reward:μσmM 0.00 0.00 0.00 0.00 | Test num frames:μσmM 242.00 104.74 128.00 380.00
Status saved
U 351 | F 105472 | FPS 0051 | D 938 | Reward:μσmM 2.76 0.51 1.00 2.93 | policy_loss ['None', 'None', '-0.064', '-0.031', '-0.136'] | value_loss ['None', 'None', '0.000', '0.001', '0.010']
U 352 | F 105984 | FPS 0051 | D 948 | Reward:μσmM 2.72 0.73 0.00 2.94 | policy_loss ['None', 'None', '-0.008', '-0.039', '-0.115'] | value_loss ['None', 'None', '0.001', '0.003', '0.005']
U 353 | F 106496 | FPS 0050 | D 958 | Reward:μσmM 2.55 0.87 0.00 2.95 | policy_loss ['None', 'None', '0.061', '0.024', '0.019'] | value_loss ['None', 'None', '0.111', '0.096', '0.155']
U 354 | F 107008 | FPS 0051 | D 968 | Reward:μσmM 2.47 0.81 1.00 2.94 | policy_loss ['None', 'None', '0.158', '0.088', '0.016'] | value_loss ['None', 'None', '0.158', '0.050', '0.006']
U 355 | F 107520 | FPS 0049 | D 979 | Reward:μσmM 2.84 0.24 2.00 2.95 | policy_loss ['None', 'None', '-0.022', '-0.076', '0.024'] | value_loss ['None', 'None', '0.002', '0.006', '0.005']
U 356 | F 108032 | FPS 0051 | D 989 | Reward:μσmM 2.69 0.56 1.00 2.94 | policy_loss ['None', 'None', '0.003', '-0.042', '0.175'] | value_loss ['None', 'None', '0.002', '0.005', '0.022']
U 357 | F 108544 | FPS 0050 | D 999 | Reward:μσmM 2.57 0.62 1.00 2.92 | policy_loss ['None', 'None', '0.043', '0.075', '0.065'] | value_loss ['None', 'None', '0.099', '0.041', '0.034']
U 358 | F 109056 | FPS 0048 | D 1010 | Reward:μσmM 2.58 0.86 0.00 2.94 | policy_loss ['None', 'None', '-0.034', '0.029', '0.001'] | value_loss ['None', 'None', '0.003', '0.007', '0.020']
U 359 | F 109568 | FPS 0051 | D 1020 | Reward:μσmM 1.74 1.70 -1.00 2.92 | policy_loss ['None', 'None', '-0.005', '0.011', '0.271'] | value_loss ['None', 'None', '0.002', '0.016', '0.697']
U 360 | F 110080 | FPS 0050 | D 1030 | Reward:μσmM 1.54 1.57 -1.00 2.92 | policy_loss ['None', 'None', '0.062', '0.215', '0.252'] | value_loss ['None', 'None', '0.109', '0.274', '0.590']
U 10 | Test reward:μσmM -0.50 0.50 -1.00 0.00 | Test num frames:μσmM 130.40 110.97 5.00 380.00
Status saved
U 361 | F 110592 | FPS 0052 | D 1081 | Reward:μσmM 2.55 0.64 1.00 2.93 | policy_loss ['None', 'None', '0.022', '-0.091', '-0.070'] | value_loss ['None', 'None', '0.003', '0.014', '0.029']
U 362 | F 111104 | FPS 0050 | D 1091 | Reward:μσmM 2.02 1.34 -1.00 2.93 | policy_loss ['None', 'None', '0.137', '0.010', '0.084'] | value_loss ['None', 'None', '0.154', '0.046', '0.438']
U 363 | F 111616 | FPS 0049 | D 1102 | Reward:μσmM 2.43 1.09 -1.00 2.93 | policy_loss ['None', 'None', '-0.060', '-0.110', '-0.097'] | value_loss ['None', 'None', '0.071', '0.026', '0.302']
U 364 | F 112128 | FPS 0049 | D 1112 | Reward:μσmM 2.79 0.48 1.00 2.94 | policy_loss ['None', 'None', '-0.035', '-0.100', '-0.211'] | value_loss ['None', 'None', '0.000', '0.004', '0.018']
U 365 | F 112640 | FPS 0050 | D 1122 | Reward:μσmM 2.72 0.73 0.00 2.94 | policy_loss ['None', 'None', '0.011', '-0.105', '-0.126'] | value_loss ['None', 'None', '0.001', '0.011', '0.024']
U 366 | F 113152 | FPS 0050 | D 1133 | Reward:μσmM 2.86 0.23 2.00 2.94 | policy_loss ['None', 'None', '-0.036', '-0.026', '-0.067'] | value_loss ['None', 'None', '0.010', '0.010', '0.011']
U 367 | F 113664 | FPS 0048 | D 1143 | Reward:μσmM 2.59 0.73 1.00 2.94 | policy_loss ['None', 'None', '0.051', '0.098', '0.104'] | value_loss ['None', 'None', '0.124', '0.118', '0.075']
U 368 | F 114176 | FPS 0052 | D 1153 | Reward:μσmM 2.72 0.54 1.00 2.92 | policy_loss ['None', 'None', '-0.027', '0.086', '0.116'] | value_loss ['None', 'None', '0.053', '0.085', '0.020']
U 369 | F 114688 | FPS 0053 | D 1163 | Reward:μσmM 2.01 1.08 0.00 2.89 | policy_loss ['None', 'None', '0.153', '0.076', '0.234'] | value_loss ['None', 'None', '0.114', '0.075', '0.232']
U 370 | F 115200 | FPS 0053 | D 1172 | Reward:μσmM 1.02 1.63 -1.00 2.84 | policy_loss ['None', 'None', '-0.003', '0.501', '0.245'] | value_loss ['None', 'None', '0.002', '0.764', '0.425']
U 10 | Test reward:μσmM -0.30 0.46 -1.00 0.00 | Test num frames:μσmM 184.60 99.98 14.00 380.00
Status saved
U 371 | F 115712 | FPS 0052 | D 1228 | Reward:μσmM -0.03 1.40 -1.00 2.81 | policy_loss ['None', 'None', '-0.003', '0.426', '0.689'] | value_loss ['None', 'None', '0.002', '0.451', '1.110']
U 372 | F 116224 | FPS 0053 | D 1238 | Reward:μσmM 1.42 1.55 -1.00 2.79 | policy_loss ['None', 'None', '-0.025', '-0.120', '0.218'] | value_loss ['None', 'None', '0.001', '0.181', '0.201']
U 373 | F 116736 | FPS 0052 | D 1248 | Reward:μσmM 1.61 1.67 -1.00 2.88 | policy_loss ['None', 'None', '-0.031', '-0.210', '0.162'] | value_loss ['None', 'None', '0.002', '0.025', '0.313']
U 374 | F 117248 | FPS 0053 | D 1257 | Reward:μσmM 1.99 1.37 -1.00 2.79 | policy_loss ['None', 'None', '-0.003', '-0.081', '-0.052'] | value_loss ['None', 'None', '0.001', '0.014', '0.141']
U 375 | F 117760 | FPS 0054 | D 1267 | Reward:μσmM 2.71 0.29 2.00 2.88 | policy_loss ['None', 'None', '-0.008', '-0.070', '-0.334'] | value_loss ['None', 'None', '0.002', '0.004', '0.062']
U 376 | F 118272 | FPS 0053 | D 1276 | Reward:μσmM 2.58 0.86 0.00 2.91 | policy_loss ['None', 'None', '-0.046', '-0.063', '-0.296'] | value_loss ['None', 'None', '0.004', '0.003', '0.053']
U 377 | F 118784 | FPS 0052 | D 1286 | Reward:μσmM 2.62 0.83 0.00 2.91 | policy_loss ['None', 'None', '-0.028', '-0.057', '-0.290'] | value_loss ['None', 'None', '0.003', '0.001', '0.048']
U 378 | F 119296 | FPS 0052 | D 1296 | Reward:μσmM 2.62 0.83 0.00 2.92 | policy_loss ['None', 'None', '-0.025', '-0.006', '-0.187'] | value_loss ['None', 'None', '0.002', '0.002', '0.017']
U 379 | F 119808 | FPS 0050 | D 1307 | Reward:μσmM 2.77 0.49 1.00 2.94 | policy_loss ['None', 'None', '-0.047', '-0.050', '-0.160'] | value_loss ['None', 'None', '0.001', '0.002', '0.010']
U 380 | F 120320 | FPS 0049 | D 1317 | Reward:μσmM 2.76 0.49 1.00 2.95 | policy_loss ['None', 'None', '-0.004', '0.023', '-0.066'] | value_loss ['None', 'None', '0.063', '0.092', '0.020']
U 10 | Test reward:μσmM 0.00 0.00 0.00 0.00 | Test num frames:μσmM 242.00 104.74 128.00 380.00
Status saved
U 381 | F 120832 | FPS 0048 | D 1389 | Reward:μσmM 2.68 0.77 0.00 2.95 | policy_loss ['None', 'None', '-0.005', '-0.031', '-0.029'] | value_loss ['None', 'None', '0.003', '0.005', '0.006']
U 382 | F 121344 | FPS 0052 | D 1399 | Reward:μσmM 2.72 0.73 0.00 2.94 | policy_loss ['None', 'None', '-0.004', '-0.034', '0.022'] | value_loss ['None', 'None', '0.002', '0.006', '0.007']
U 383 | F 121856 | FPS 0052 | D 1409 | Reward:μσmM 2.92 0.02 2.86 2.94 | policy_loss ['None', 'None', '-0.007', '-0.014', '0.003'] | value_loss ['None', 'None', '0.002', '0.004', '0.004']
U 384 | F 122368 | FPS 0052 | D 1419 | Reward:μσmM 2.67 0.56 1.00 2.93 | policy_loss ['None', 'None', '0.059', '0.057', '0.004'] | value_loss ['None', 'None', '0.045', '0.020', '0.006']
U 385 | F 122880 | FPS 0051 | D 1429 | Reward:μσmM 1.59 1.01 0.00 2.89 | policy_loss ['None', 'None', '0.350', '0.408', '0.254'] | value_loss ['None', 'None', '0.448', '0.246', '0.043']
U 386 | F 123392 | FPS 0052 | D 1439 | Reward:μσmM 1.90 1.25 0.00 2.89 | policy_loss ['None', 'None', '-0.056', '0.370', '0.121'] | value_loss ['None', 'None', '0.003', '0.399', '0.007']
U 387 | F 123904 | FPS 0054 | D 1448 | Reward:μσmM 2.07 0.88 1.00 2.86 | policy_loss ['None', 'None', '0.227', '-0.061', '0.237'] | value_loss ['None', 'None', '0.272', '0.026', '0.016']
U 388 | F 124416 | FPS 0053 | D 1458 | Reward:μσmM 0.82 1.50 -1.00 2.84 | policy_loss ['None', 'None', '0.021', '0.494', '0.374'] | value_loss ['None', 'None', '0.005', '0.761', '0.564']
U 389 | F 124928 | FPS 0050 | D 1468 | Reward:μσmM -0.30 1.14 -1.00 2.71 | policy_loss ['None', 'None', '-0.024', '0.501', '0.518'] | value_loss ['None', 'None', '0.000', '0.744', '0.669']
U 390 | F 125440 | FPS 0052 | D 1478 | Reward:μσmM 0.17 1.57 -1.00 2.65 | policy_loss ['None', 'None', '0.061', '-0.379', '0.397'] | value_loss ['None', 'None', '0.001', '0.036', '0.470']
U 10 | Test reward:μσmM -0.20 0.40 -1.00 0.00 | Test num frames:μσmM 222.10 115.64 5.00 380.00
Status saved
U 391 | F 125952 | FPS 0051 | D 1538 | Reward:μσmM 0.19 1.70 -1.00 2.87 | policy_loss ['None', 'None', '-0.038', '-0.033', '0.424'] | value_loss ['None', 'None', '0.005', '0.008', '0.782']
U 392 | F 126464 | FPS 0051 | D 1548 | Reward:μσmM 1.37 1.49 -1.00 2.87 | policy_loss ['None', 'None', '0.111', '-0.020', '0.007'] | value_loss ['None', 'None', '0.163', '0.058', '0.107']
U 393 | F 126976 | FPS 0051 | D 1558 | Reward:μσmM 0.82 1.58 -1.00 2.91 | policy_loss ['None', 'None', '0.081', '0.119', '0.113'] | value_loss ['None', 'None', '0.120', '0.084', '0.451']
U 394 | F 127488 | FPS 0052 | D 1568 | Reward:μσmM 2.75 0.29 2.00 2.91 | policy_loss ['None', 'None', '-0.183', '-0.064', '-0.457'] | value_loss ['None', 'None', '0.017', '0.013', '0.132']
U 395 | F 128000 | FPS 0052 | D 1578 | Reward:μσmM 2.88 0.04 2.83 2.93 | policy_loss ['None', 'None', '-0.146', '-0.193', '-0.486'] | value_loss ['None', 'None', '0.003', '0.018', '0.104']
U 396 | F 128512 | FPS 0049 | D 1589 | Reward:μσmM 2.46 1.17 -1.00 2.94 | policy_loss ['None', 'None', '-0.052', '-0.215', '-0.521'] | value_loss ['None', 'None', '0.001', '0.007', '0.148']
U 397 | F 129024 | FPS 0052 | D 1598 | Reward:μσmM 2.74 0.71 0.00 2.94 | policy_loss ['None', 'None', '-0.036', '-0.102', '-0.424'] | value_loss ['None', 'None', '0.001', '0.006', '0.098']
U 398 | F 129536 | FPS 0051 | D 1608 | Reward:μσmM 2.87 0.23 2.00 2.95 | policy_loss ['None', 'None', '0.017', '-0.044', '-0.324'] | value_loss ['None', 'None', '0.002', '0.003', '0.045']
U 399 | F 130048 | FPS 0050 | D 1619 | Reward:μσmM 2.83 0.44 1.00 2.95 | policy_loss ['None', 'None', '-0.011', '-0.013', '-0.141'] | value_loss ['None', 'None', '0.001', '0.001', '0.013']
U 400 | F 130560 | FPS 0050 | D 1629 | Reward:μσmM 2.87 0.23 2.00 2.96 | policy_loss ['None', 'None', '0.021', '0.018', '-0.027'] | value_loss ['None', 'None', '0.002', '0.002', '0.003']
U 10 | Test reward:μσmM 0.00 0.00 0.00 0.00 | Test num frames:μσmM 242.00 104.74 128.00 380.00
Status saved
U 401 | F 131072 | FPS 0049 | D 1700 | Reward:μσmM 2.86 0.23 2.00 2.95 | policy_loss ['None', 'None', '0.011', '0.020', '0.035'] | value_loss ['None', 'None', '0.002', '0.002', '0.002']
U 402 | F 131584 | FPS 0049 | D 1710 | Reward:μσmM 2.76 0.51 1.00 2.94 | policy_loss ['None', 'None', '-0.002', '0.016', '0.066'] | value_loss ['None', 'None', '0.002', '0.002', '0.004']
U 403 | F 132096 | FPS 0048 | D 1721 | Reward:μσmM 2.46 0.89 0.00 2.93 | policy_loss ['None', 'None', '0.019', '0.227', '0.130'] | value_loss ['None', 'None', '0.090', '0.392', '0.078']
U 404 | F 132608 | FPS 0052 | D 1731 | Reward:μσmM 2.74 0.28 2.00 2.90 | policy_loss ['None', 'None', '0.012', '0.025', '0.100'] | value_loss ['None', 'None', '0.002', '0.005', '0.004']
U 405 | F 133120 | FPS 0052 | D 1741 | Reward:μσmM 1.92 1.29 0.00 2.92 | policy_loss ['None', 'None', '0.023', '0.287', '0.191'] | value_loss ['None', 'None', '0.053', '0.427', '0.153']
U 406 | F 133632 | FPS 0052 | D 1750 | Reward:μσmM 2.03 0.85 1.00 2.87 | policy_loss ['None', 'None', '0.270', '-0.015', '0.112'] | value_loss ['None', 'None', '0.184', '0.025', '0.016']
U 407 | F 134144 | FPS 0053 | D 1760 | Reward:μσmM 2.34 0.67 1.00 2.87 | policy_loss ['None', 'None', '-0.035', '0.251', '-0.050'] | value_loss ['None', 'None', '0.074', '0.024', '0.005']
U 408 | F 134656 | FPS 0050 | D 1770 | Reward:μσmM 1.81 1.38 -1.00 2.89 | policy_loss ['None', 'None', '0.007', '-0.008', '0.202'] | value_loss ['None', 'None', '0.083', '0.025', '0.355']
U 409 | F 135168 | FPS 0053 | D 1780 | Reward:μσmM 1.90 1.48 -1.00 2.91 | policy_loss ['None', 'None', '-0.165', '-0.044', '0.285'] | value_loss ['None', 'None', '0.033', '0.105', '0.500']
U 410 | F 135680 | FPS 0050 | D 1790 | Reward:μσmM 2.40 1.16 -1.00 2.91 | policy_loss ['None', 'None', '-0.092', '-0.127', '-0.192'] | value_loss ['None', 'None', '0.004', '0.010', '0.052']
U 10 | Test reward:μσmM -0.30 0.46 -1.00 0.00 | Test num frames:μσmM 185.90 94.11 29.00 380.00
Status saved
U 411 | F 136192 | FPS 0048 | D 1855 | Reward:μσmM 2.77 0.27 2.00 2.90 | policy_loss ['None', 'None', '0.023', '-0.097', '-0.245'] | value_loss ['None', 'None', '0.002', '0.007', '0.014']
U 412 | F 136704 | FPS 0050 | D 1865 | Reward:μσmM 2.72 0.54 1.00 2.95 | policy_loss ['None', 'None', '-0.036', '-0.058', '-0.174'] | value_loss ['None', 'None', '0.002', '0.006', '0.016']
U 413 | F 137216 | FPS 0051 | D 1875 | Reward:μσmM 2.72 0.73 0.00 2.94 | policy_loss ['None', 'None', '-0.032', '-0.092', '-0.208'] | value_loss ['None', 'None', '0.001', '0.003', '0.005']
U 414 | F 137728 | FPS 0047 | D 1886 | Reward:μσmM 2.74 0.71 0.00 2.94 | policy_loss ['None', 'None', '-0.015', '-0.057', '-0.084'] | value_loss ['None', 'None', '0.001', '0.002', '0.007']
U 415 | F 138240 | FPS 0052 | D 1896 | Reward:μσmM 2.82 0.45 1.00 2.95 | policy_loss ['None', 'None', '-0.000', '-0.032', '-0.091'] | value_loss ['None', 'None', '0.001', '0.002', '0.004']
U 416 | F 138752 | FPS 0047 | D 1906 | Reward:μσmM 2.88 0.22 2.00 2.94 | policy_loss ['None', 'None', '-0.001', '0.007', '-0.013'] | value_loss ['None', 'None', '0.001', '0.001', '0.002']
U 417 | F 139264 | FPS 0050 | D 1917 | Reward:μσmM 2.64 0.80 0.00 2.94 | policy_loss ['None', 'None', '0.067', '0.048', '0.009'] | value_loss ['None', 'None', '0.098', '0.018', '0.015']
U 418 | F 139776 | FPS 0052 | D 1927 | Reward:μσmM 1.84 1.26 0.00 2.93 | policy_loss ['None', 'None', '0.093', '0.308', '0.316'] | value_loss ['None', 'None', '0.089', '0.563', '0.295']
U 419 | F 140288 | FPS 0054 | D 1936 | Reward:μσmM 2.49 0.94 0.00 2.91 | policy_loss ['None', 'None', '0.002', '0.022', '0.126'] | value_loss ['None', 'None', '0.003', '0.007', '0.006']
U 420 | F 140800 | FPS 0051 | D 1946 | Reward:μσmM 2.03 1.30 -1.00 2.87 | policy_loss ['None', 'None', '0.043', '0.098', '0.201'] | value_loss ['None', 'None', '0.021', '0.054', '0.313']
U 10 | Test reward:μσmM -0.30 0.46 -1.00 0.00 | Test num frames:μσmM 189.80 112.46 28.00 380.00
Status saved
U 421 | F 141312 | FPS 0050 | D 2005 | Reward:μσmM 0.09 1.51 -1.00 2.89 | policy_loss ['None', 'None', '0.184', '0.111', '0.763'] | value_loss ['None', 'None', '0.162', '0.067', '1.691']
U 422 | F 141824 | FPS 0054 | D 2015 | Reward:μσmM 1.54 0.69 1.00 2.69 | policy_loss ['None', 'None', '0.264', '0.132', '0.071'] | value_loss ['None', 'None', '0.171', '0.063', '0.012']
U 423 | F 142336 | FPS 0054 | D 2024 | Reward:μσmM 0.79 1.33 -1.00 2.14 | policy_loss ['None', 'None', '0.044', '0.272', '0.365'] | value_loss ['None', 'None', '0.002', '0.298', '0.515']
U 424 | F 142848 | FPS 0051 | D 2034 | Reward:μσmM 1.03 1.51 -1.00 2.82 | policy_loss ['None', 'None', '-0.062', '0.095', '0.176'] | value_loss ['None', 'None', '0.053', '0.077', '0.319']
U 425 | F 143360 | FPS 0053 | D 2044 | Reward:μσmM 0.86 1.61 -1.00 2.75 | policy_loss ['None', 'None', '-0.052', '0.148', '0.188'] | value_loss ['None', 'None', '0.007', '0.157', '0.213']
U 426 | F 143872 | FPS 0050 | D 2054 | Reward:μσmM 2.68 0.31 2.00 2.87 | policy_loss ['None', 'None', '-0.089', '-0.123', '-0.286'] | value_loss ['None', 'None', '0.008', '0.006', '0.031']
U 427 | F 144384 | FPS 0051 | D 2064 | Reward:μσmM 2.87 0.04 2.77 2.92 | policy_loss ['None', 'None', '-0.120', '-0.193', '-0.343'] | value_loss ['None', 'None', '0.009', '0.015', '0.043']
U 428 | F 144896 | FPS 0051 | D 2074 | Reward:μσmM 2.69 0.56 1.00 2.91 | policy_loss ['None', 'None', '-0.080', '-0.162', '-0.230'] | value_loss ['None', 'None', '0.004', '0.009', '0.027']
U 429 | F 145408 | FPS 0050 | D 2084 | Reward:μσmM 2.84 0.24 2.00 2.93 | policy_loss ['None', 'None', '-0.066', '-0.122', '-0.271'] | value_loss ['None', 'None', '0.001', '0.002', '0.014']
U 430 | F 145920 | FPS 0051 | D 2095 | Reward:μσmM 2.77 0.49 1.00 2.93 | policy_loss ['None', 'None', '-0.029', '-0.083', '-0.165'] | value_loss ['None', 'None', '0.001', '0.002', '0.017']
U 10 | Test reward:μσmM -0.10 0.30 -1.00 0.00 | Test num frames:μσmM 230.60 121.41 14.00 380.00
Status saved
U 431 | F 146432 | FPS 0046 | D 2161 | Reward:μσmM 2.88 0.22 2.00 2.95 | policy_loss ['None', 'None', '-0.025', '-0.025', '-0.149'] | value_loss ['None', 'None', '0.000', '0.001', '0.008']
U 432 | F 146944 | FPS 0052 | D 2171 | Reward:μσmM 2.78 0.66 0.00 2.95 | policy_loss ['None', 'None', '-0.003', '-0.032', '-0.104'] | value_loss ['None', 'None', '0.000', '0.001', '0.004']
U 433 | F 147456 | FPS 0049 | D 2181 | Reward:μσmM 2.75 0.69 0.00 2.95 | policy_loss ['None', 'None', '0.017', '0.020', '-0.025'] | value_loss ['None', 'None', '0.003', '0.002', '0.003']
U 434 | F 147968 | FPS 0049 | D 2192 | Reward:μσmM 2.65 0.80 0.00 2.94 | policy_loss ['None', 'None', '0.047', '0.079', '0.036'] | value_loss ['None', 'None', '0.001', '0.003', '0.002']
U 435 | F 148480 | FPS 0051 | D 2202 | Reward:μσmM 1.92 1.26 0.00 2.92 | policy_loss ['None', 'None', '0.023', '0.326', '0.200'] | value_loss ['None', 'None', '0.002', '0.330', '0.071']
U 436 | F 148992 | FPS 0052 | D 2212 | Reward:μσmM 2.70 0.29 2.00 2.90 | policy_loss ['None', 'None', '0.037', '0.028', '0.106'] | value_loss ['None', 'None', '0.002', '0.017', '0.007']
U 437 | F 149504 | FPS 0053 | D 2221 | Reward:μσmM 2.50 0.63 1.00 2.90 | policy_loss ['None', 'None', '0.060', '-0.082', '0.110'] | value_loss ['None', 'None', '0.094', '0.036', '0.014']
U 438 | F 150016 | FPS 0051 | D 2231 | Reward:μσmM 1.37 1.81 -1.00 2.93 | policy_loss ['None', 'None', '-0.073', '-0.036', '0.516'] | value_loss ['None', 'None', '0.002', '0.004', '1.084']
U 439 | F 150528 | FPS 0049 | D 2242 | Reward:μσmM 1.62 1.76 -1.00 2.93 | policy_loss ['None', 'None', '-0.018', '0.004', '0.238'] | value_loss ['None', 'None', '0.002', '0.009', '0.999']
U 440 | F 151040 | FPS 0053 | D 2251 | Reward:μσmM 2.59 0.86 0.00 2.92 | policy_loss ['None', 'None', '-0.035', '-0.234', '-0.170'] | value_loss ['None', 'None', '0.002', '0.100', '0.066']
U 10 | Test reward:μσmM -0.20 0.40 -1.00 0.00 | Test num frames:μσmM 194.30 113.62 14.00 380.00
Status saved
U 441 | F 151552 | FPS 0051 | D 2312 | Reward:μσmM 2.81 0.26 2.00 2.93 | policy_loss ['None', 'None', '0.003', '-0.088', '-0.173'] | value_loss ['None', 'None', '0.001', '0.002', '0.020']
U 442 | F 152064 | FPS 0052 | D 2321 | Reward:μσmM 2.82 0.25 2.00 2.93 | policy_loss ['None', 'None', '0.005', '-0.023', '-0.130'] | value_loss ['None', 'None', '0.002', '0.001', '0.016']
U 443 | F 152576 | FPS 0050 | D 2331 | Reward:μσmM 2.84 0.24 2.00 2.94 | policy_loss ['None', 'None', '-0.015', '0.012', '-0.140'] | value_loss ['None', 'None', '0.002', '0.001', '0.007']
U 444 | F 153088 | FPS 0047 | D 2342 | Reward:μσmM 2.76 0.51 1.00 2.94 | policy_loss ['None', 'None', '-0.018', '0.050', '-0.056'] | value_loss ['None', 'None', '0.001', '0.010', '0.006']
U 445 | F 153600 | FPS 0049 | D 2353 | Reward:μσmM 2.77 0.49 1.00 2.94 | policy_loss ['None', 'None', '0.009', '-0.067', '0.033'] | value_loss ['None', 'None', '0.001', '0.010', '0.011']
U 446 | F 154112 | FPS 0050 | D 2363 | Reward:μσmM 2.45 0.94 0.00 2.90 | policy_loss ['None', 'None', '0.098', '0.040', '0.128'] | value_loss ['None', 'None', '0.104', '0.035', '0.012']
U 447 | F 154624 | FPS 0052 | D 2373 | Reward:μσmM 2.38 0.69 1.00 2.83 | policy_loss ['None', 'None', '0.096', '0.046', '0.104'] | value_loss ['None', 'None', '0.001', '0.044', '0.015']
U 448 | F 155136 | FPS 0050 | D 2383 | Reward:μσmM 0.53 1.20 -1.00 2.80 | policy_loss ['None', 'None', '0.439', '0.247', '0.591'] | value_loss ['None', 'None', '0.316', '0.257', '1.081']
U 449 | F 155648 | FPS 0054 | D 2393 | Reward:μσmM 1.64 1.54 -1.00 2.79 | policy_loss ['None', 'None', '-0.035', '0.064', '0.210'] | value_loss ['None', 'None', '0.006', '0.008', '0.283']
U 450 | F 156160 | FPS 0053 | D 2402 | Reward:μσmM 0.90 1.66 -1.00 2.84 | policy_loss ['None', 'None', '-0.053', '0.130', '0.282'] | value_loss ['None', 'None', '0.006', '0.134', '0.299']
U 10 | Test reward:μσmM -0.90 0.30 -1.00 0.00 | Test num frames:μσmM 69.80 69.89 3.00 224.00
Status saved
U 451 | F 156672 | FPS 0047 | D 2442 | Reward:μσmM 0.66 1.68 -1.00 2.86 | policy_loss ['None', 'None', '-0.086', '0.305', '0.424'] | value_loss ['None', 'None', '0.008', '0.514', '0.651']
U 452 | F 157184 | FPS 0052 | D 2452 | Reward:μσmM 1.20 1.85 -1.00 2.91 | policy_loss ['None', 'None', '-0.126', '-0.390', '0.283'] | value_loss ['None', 'None', '0.004', '0.087', '0.705']
U 453 | F 157696 | FPS 0050 | D 2462 | Reward:μσmM 1.79 1.63 -1.00 2.90 | policy_loss ['None', 'None', '-0.029', '-0.241', '-0.168'] | value_loss ['None', 'None', '0.004', '0.021', '0.337']
U 454 | F 158208 | FPS 0052 | D 2472 | Reward:μσmM 2.35 1.19 -1.00 2.90 | policy_loss ['None', 'None', '-0.011', '-0.136', '-0.458'] | value_loss ['None', 'None', '0.001', '0.022', '0.257']
U 455 | F 158720 | FPS 0053 | D 2481 | Reward:μσmM 2.76 0.27 2.00 2.91 | policy_loss ['None', 'None', '0.002', '0.019', '-0.371'] | value_loss ['None', 'None', '0.003', '0.005', '0.091']
U 456 | F 159232 | FPS 0050 | D 2491 | Reward:μσmM 2.32 0.86 1.00 2.94 | policy_loss ['None', 'None', '0.205', '0.159', '-0.394'] | value_loss ['None', 'None', '0.281', '0.032', '0.067']
U 457 | F 159744 | FPS 0050 | D 2502 | Reward:μσmM 2.62 0.83 0.00 2.92 | policy_loss ['None', 'None', '-0.101', '0.038', '-0.173'] | value_loss ['None', 'None', '0.002', '0.012', '0.020']
U 458 | F 160256 | FPS 0052 | D 2512 | Reward:μσmM 2.61 0.79 0.00 2.94 | policy_loss ['None', 'None', '-0.044', '-0.030', '-0.054'] | value_loss ['None', 'None', '0.002', '0.131', '0.158']
U 459 | F 160768 | FPS 0052 | D 2521 | Reward:μσmM 2.70 0.75 0.00 2.93 | policy_loss ['None', 'None', '-0.031', '-0.169', '-0.142'] | value_loss ['None', 'None', '0.001', '0.033', '0.013']
U 460 | F 161280 | FPS 0050 | D 2532 | Reward:μσmM 2.75 0.69 0.00 2.95 | policy_loss ['None', 'None', '-0.021', '-0.076', '-0.170'] | value_loss ['None', 'None', '0.001', '0.002', '0.005']
U 10 | Test reward:μσmM -0.20 0.40 -1.00 0.00 | Test num frames:μσmM 192.20 128.54 2.00 380.00
Status saved
U 461 | F 161792 | FPS 0050 | D 2591 | Reward:μσmM 2.83 0.44 1.00 2.95 | policy_loss ['None', 'None', '-0.013', '-0.025', '-0.113'] | value_loss ['None', 'None', '0.001', '0.001', '0.002']
U 462 | F 162304 | FPS 0051 | D 2601 | Reward:μσmM 2.87 0.23 2.00 2.95 | policy_loss ['None', 'None', '0.019', '0.009', '-0.022'] | value_loss ['None', 'None', '0.002', '0.006', '0.002']
U 463 | F 162816 | FPS 0050 | D 2611 | Reward:μσmM 2.74 0.71 0.00 2.95 | policy_loss ['None', 'None', '0.008', '0.038', '0.014'] | value_loss ['None', 'None', '0.001', '0.006', '0.003']
U 464 | F 163328 | FPS 0051 | D 2622 | Reward:μσmM 2.71 0.53 1.00 2.95 | policy_loss ['None', 'None', '0.062', '0.031', '0.045'] | value_loss ['None', 'None', '0.048', '0.015', '0.006']
U 465 | F 163840 | FPS 0050 | D 2632 | Reward:μσmM 1.87 1.02 0.00 2.93 | policy_loss ['None', 'None', '0.294', '0.281', '0.234'] | value_loss ['None', 'None', '0.360', '0.252', '0.131']
U 466 | F 164352 | FPS 0048 | D 2642 | Reward:μσmM 1.93 1.19 0.00 2.93 | policy_loss ['None', 'None', '-0.019', '0.225', '0.223'] | value_loss ['None', 'None', '0.052', '0.235', '0.098']
U 467 | F 164864 | FPS 0055 | D 2652 | Reward:μσmM 1.60 1.31 0.00 2.84 | policy_loss ['None', 'None', '0.041', '0.215', '0.134'] | value_loss ['None', 'None', '0.004', '0.169', '0.010']
U 468 | F 165376 | FPS 0054 | D 2661 | Reward:μσmM 0.53 1.09 -1.00 2.69 | policy_loss ['None', 'None', '0.265', '0.357', '0.307'] | value_loss ['None', 'None', '0.167', '0.231', '0.329']
U 469 | F 165888 | FPS 0052 | D 2671 | Reward:μσmM 0.76 1.26 -1.00 2.79 | policy_loss ['None', 'None', '0.147', '0.135', '0.431'] | value_loss ['None', 'None', '0.073', '0.091', '0.457']
U 470 | F 166400 | FPS 0053 | D 2681 | Reward:μσmM 1.75 1.47 -1.00 2.87 | policy_loss ['None', 'None', '-0.165', '0.002', '0.158'] | value_loss ['None', 'None', '0.011', '0.166', '0.244']
U 10 | Test reward:μσmM -0.50 0.50 -1.00 0.00 | Test num frames:μσmM 136.70 118.02 6.00 380.00
Status saved
discover.py --algo ppo --env MiniGrid-ConfigWorld-v0 --lr 0.00025 --AnomalyNN test_8 --model 20240819-seed1 --test-interval 1 --task-config task3 --discover 0 --frames-per-proc 512

Namespace(task_config='task3', discover=0, algo='ppo', env='MiniGrid-ConfigWorld-v0', model='20240819-seed1', seed=1, log_interval=1, test_interval=1, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_8', epochs=32, batch_size=128, frames_per_proc=512, discount=0.99, lr=0.00025, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Optimizer loaded

Optimizer loaded

U 471 | F 166912 | FPS 0040 | D 12 | Reward:μσmM 0.03 1.54 -1.00 2.83 | policy_loss ['None', 'None', '0.114', '-0.132', '0.473'] | value_loss ['None', 'None', '0.183', '0.061', '0.954']
U 10 | Test reward:μσmM -0.10 0.30 -1.00 0.00 | Test num frames:μσmM 220.30 96.02 128.00 380.00
discover.py --algo ppo --env MiniGrid-ConfigWorld-v0 --lr 0.00025 --AnomalyNN test_8 --model 20240819-seed1 --test-interval 1 --task-config task3 --discover 0 --frames-per-proc 512

Namespace(task_config='task3', discover=0, algo='ppo', env='MiniGrid-ConfigWorld-v0', model='20240819-seed1', seed=1, log_interval=1, test_interval=1, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_8', epochs=32, batch_size=128, frames_per_proc=512, discount=0.99, lr=0.00025, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Optimizer loaded

Optimizer loaded

U 471 | F 166912 | FPS 0038 | D 13 | Reward:μσmM 0.03 1.54 -1.00 2.83 | policy_loss ['None', 'None', '0.114', '-0.134', '0.473'] | value_loss ['None', 'None', '0.183', '0.061', '0.953']
discover.py --algo ppo --env MiniGrid-ConfigWorld-v0 --lr 0.00025 --AnomalyNN test_8 --model 20240819-seed1 --test-interval 1 --task-config task3 --discover 0 --frames-per-proc 512

Namespace(task_config='task3', discover=0, algo='ppo', env='MiniGrid-ConfigWorld-v0', model='20240819-seed1', seed=1, log_interval=1, test_interval=1, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_8', epochs=32, batch_size=128, frames_per_proc=512, discount=0.99, lr=0.00025, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Optimizer loaded

Optimizer loaded

U 471 | F 166912 | FPS 0039 | D 13 | Reward:μσmM 0.03 1.54 -1.00 2.83 | policy_loss ['None', 'None', '0.114', '-0.132', '0.473'] | value_loss ['None', 'None', '0.183', '0.061', '0.953']
U 10 | Test reward:μσmM -0.40 0.49 -1.00 0.00 | Test num frames:μσmM 186.30 119.76 5.00 410.00
U 472 | F 167424 | FPS 0048 | D 63 | Reward:μσmM 0.61 1.79 -1.00 2.86 | policy_loss ['None', 'None', '-0.112', '-0.159', '0.289'] | value_loss ['None', 'None', '0.005', '0.019', '0.509']
discover.py --algo ppo --env MiniGrid-ConfigWorld-v0 --lr 0.00025 --AnomalyNN test_8 --model 20240819-seed1 --test-interval 1 --task-config task3 --discover 0 --frames-per-proc 512

Namespace(task_config='task3', discover=0, algo='ppo', env='MiniGrid-ConfigWorld-v0', model='20240819-seed1', seed=1, log_interval=1, test_interval=1, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_8', epochs=32, batch_size=128, frames_per_proc=512, discount=0.99, lr=0.00025, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Optimizer loaded

Optimizer loaded

U 471 | F 166912 | FPS 0040 | D 12 | Reward:μσmM 0.03 1.54 -1.00 2.83 | policy_loss ['None', 'None', '0.114', '-0.134', '0.473'] | value_loss ['None', 'None', '0.183', '0.061', '0.953']
U 10 | Test reward:μσmM -0.70 0.46 -1.00 0.00 | Test num frames:μσmM 50.00 52.48 1.00 128.00
U 472 | F 167424 | FPS 0050 | D 46 | Reward:μσmM 0.68 1.83 -1.00 2.86 | policy_loss ['None', 'None', '-0.093', '-0.331', '0.157'] | value_loss ['None', 'None', '0.003', '0.015', '0.453']
discover.py --algo ppo --env MiniGrid-ConfigWorld-v0 --lr 0.00025 --AnomalyNN test_8 --model 20240819-seed1 --test-interval 1 --task-config task3 --discover 0 --frames-per-proc 512

Namespace(task_config='task3', discover=0, algo='ppo', env='MiniGrid-ConfigWorld-v0', model='20240819-seed1', seed=1, log_interval=1, test_interval=1, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_8', epochs=32, batch_size=128, frames_per_proc=512, discount=0.99, lr=0.00025, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Optimizer loaded

Optimizer loaded

U 471 | F 166912 | FPS 0037 | D 13 | Reward:μσmM 0.03 1.54 -1.00 2.83 | policy_loss ['None', 'None', '0.114', '-0.134', '0.473'] | value_loss ['None', 'None', '0.183', '0.061', '0.953']
discover.py --algo ppo --env MiniGrid-ConfigWorld-v0 --lr 0.00025 --AnomalyNN test_8 --model 20240819-seed1 --test-interval 1 --task-config task3 --discover 0 --frames-per-proc 512

Namespace(task_config='task3', discover=0, algo='ppo', env='MiniGrid-ConfigWorld-v0', model='20240819-seed1', seed=1, log_interval=1, test_interval=1, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_8', epochs=32, batch_size=128, frames_per_proc=512, discount=0.99, lr=0.00025, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Optimizer loaded

Optimizer loaded

U 471 | F 166912 | FPS 0040 | D 12 | Reward:μσmM 0.03 1.54 -1.00 2.83 | policy_loss ['None', 'None', '0.114', '-0.132', '0.473'] | value_loss ['None', 'None', '0.183', '0.061', '0.953']
U 10 | Test reward:μσmM -0.50 0.50 -1.00 0.00 | Test num frames:μσmM 75.00 53.86 5.00 128.00
discover.py --algo ppo --env MiniGrid-ConfigWorld-v0 --lr 0.00025 --AnomalyNN test_8 --model 20240819-seed1 --test-interval 1 --task-config task3 --discover 0 --frames-per-proc 512

Namespace(task_config='task3', discover=0, algo='ppo', env='MiniGrid-ConfigWorld-v0', model='20240819-seed1', seed=1, log_interval=1, test_interval=1, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_8', epochs=32, batch_size=128, frames_per_proc=512, discount=0.99, lr=0.00025, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Optimizer loaded

Optimizer loaded

U 471 | F 166912 | FPS 0041 | D 12 | Reward:μσmM 0.03 1.54 -1.00 2.83 | policy_loss ['None', 'None', '0.114', '-0.134', '0.473'] | value_loss ['None', 'None', '0.183', '0.061', '0.953']
discover.py --algo ppo --env MiniGrid-ConfigWorld-v0 --lr 0.00025 --AnomalyNN test_8 --model 20240819-seed1 --test-interval 1 --task-config task3 --discover 0 --frames-per-proc 512

Namespace(task_config='task3', discover=0, algo='ppo', env='MiniGrid-ConfigWorld-v0', model='20240819-seed1', seed=1, log_interval=1, test_interval=1, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_8', epochs=32, batch_size=128, frames_per_proc=512, discount=0.99, lr=0.00025, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Optimizer loaded

Optimizer loaded

U 471 | F 166912 | FPS 0040 | D 12 | Reward:μσmM 0.03 1.54 -1.00 2.83 | policy_loss ['None', 'None', '0.114', '-0.134', '0.473'] | value_loss ['None', 'None', '0.183', '0.061', '0.953']
discover.py --algo ppo --env MiniGrid-ConfigWorld-v0 --lr 0.00025 --AnomalyNN test_8 --model 20240819-seed1 --test-interval 1 --task-config task3 --discover 0 --frames-per-proc 512

Namespace(task_config='task3', discover=0, algo='ppo', env='MiniGrid-ConfigWorld-v0', model='20240819-seed1', seed=1, log_interval=1, test_interval=1, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_8', epochs=32, batch_size=128, frames_per_proc=512, discount=0.99, lr=0.00025, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Optimizer loaded

Optimizer loaded

U 471 | F 166912 | FPS 0040 | D 12 | Reward:μσmM 0.03 1.54 -1.00 2.83 | policy_loss ['None', 'None', '0.114', '-0.134', '0.473'] | value_loss ['None', 'None', '0.183', '0.061', '0.953']
discover.py --algo ppo --env MiniGrid-ConfigWorld-v0 --lr 0.00025 --AnomalyNN test_8 --model 20240819-seed1 --test-interval 10 --task-config task3 --discover 0 --frames-per-proc 512

Namespace(task_config='task3', discover=0, algo='ppo', env='MiniGrid-ConfigWorld-v0', model='20240819-seed1', seed=1, log_interval=1, test_interval=10, save_interval=10, procs=1, frames=10000000, AnomalyNN='test_8', epochs=32, batch_size=128, frames_per_proc=512, discount=0.99, lr=0.00025, gae_lambda=0.95, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, optim_eps=1e-08, optim_alpha=0.99, clip_eps=0.2, recurrence=1, text=False, buffer_size=10000, target_update=10)

Device: cuda

Environments loaded

Training status loaded

Observations preprocessor loaded
Model loaded

ACModel(
  (image_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(12, 12), stride=(6, 6))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(9, 9), stride=(4, 4))
    (3): ReLU()
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
  (actor): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
  (critic): Sequential(
    (0): Linear(in_features=2304, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)

Optimizer loaded

Optimizer loaded

Optimizer loaded

U 471 | F 166912 | FPS 0040 | D 12 | Reward:μσmM 0.03 1.54 -1.00 2.83 | policy_loss ['None', 'None', '0.114', '-0.132', '0.473'] | value_loss ['None', 'None', '0.183', '0.061', '0.953']
U 472 | F 167424 | FPS 0050 | D 22 | Reward:μσmM 0.72 1.85 -1.00 2.91 | policy_loss ['None', 'None', '-0.135', '-0.146', '0.094'] | value_loss ['None', 'None', '0.006', '0.242', '0.576']
U 473 | F 167936 | FPS 0049 | D 33 | Reward:μσmM 1.79 1.63 -1.00 2.91 | policy_loss ['None', 'None', '-0.095', '-0.189', '-0.217'] | value_loss ['None', 'None', '0.003', '0.025', '0.233']
U 474 | F 168448 | FPS 0050 | D 43 | Reward:μσmM 1.99 1.55 -1.00 2.91 | policy_loss ['None', 'None', '-0.049', '-0.222', '-0.327'] | value_loss ['None', 'None', '0.001', '0.008', '0.369']
U 475 | F 168960 | FPS 0050 | D 53 | Reward:μσmM 2.79 0.27 2.00 2.94 | policy_loss ['None', 'None', '-0.021', '-0.122', '-0.306'] | value_loss ['None', 'None', '0.002', '0.002', '0.075']
U 476 | F 169472 | FPS 0048 | D 64 | Reward:μσmM 2.78 0.49 1.00 2.94 | policy_loss ['None', 'None', '-0.031', '-0.024', '-0.480'] | value_loss ['None', 'None', '0.001', '0.002', '0.139']
U 477 | F 169984 | FPS 0050 | D 74 | Reward:μσmM 2.79 0.48 1.00 2.94 | policy_loss ['None', 'None', '-0.024', '-0.022', '-0.291'] | value_loss ['None', 'None', '0.001', '0.002', '0.052']
U 478 | F 170496 | FPS 0049 | D 85 | Reward:μσmM 2.80 0.47 1.00 2.94 | policy_loss ['None', 'None', '0.011', '-0.016', '-0.180'] | value_loss ['None', 'None', '0.001', '0.001', '0.011']
U 479 | F 171008 | FPS 0051 | D 95 | Reward:μσmM 2.68 0.77 0.00 2.93 | policy_loss ['None', 'None', '0.036', '0.028', '-0.051'] | value_loss ['None', 'None', '0.002', '0.002', '0.003']
U 480 | F 171520 | FPS 0051 | D 104 | Reward:μσmM 2.44 0.94 0.00 2.91 | policy_loss ['None', 'None', '0.046', '0.140', '0.088'] | value_loss ['None', 'None', '0.021', '0.078', '0.023']
U 10 | Test reward:μσmM -0.20 0.40 -1.00 0.00 | Test num frames:μσmM 208.90 94.52 3.00 256.00
Status saved
U 481 | F 172032 | FPS 0050 | D 215 | Reward:μσmM 2.54 0.90 0.00 2.93 | policy_loss ['None', 'None', '0.001', '-0.012', '0.139'] | value_loss ['None', 'None', '0.003', '0.006', '0.005']
U 482 | F 172544 | FPS 0051 | D 225 | Reward:μσmM 2.65 0.58 1.00 2.92 | policy_loss ['None', 'None', '0.007', '0.036', '0.028'] | value_loss ['None', 'None', '0.003', '0.037', '0.015']
U 483 | F 173056 | FPS 0054 | D 235 | Reward:μσmM 2.72 0.30 2.00 2.91 | policy_loss ['None', 'None', '-0.015', '0.029', '0.106'] | value_loss ['None', 'None', '0.006', '0.012', '0.013']
U 484 | F 173568 | FPS 0052 | D 245 | Reward:μσmM 2.41 0.99 0.00 2.91 | policy_loss ['None', 'None', '0.023', '0.029', '0.036'] | value_loss ['None', 'None', '0.003', '0.012', '0.008']
U 485 | F 174080 | FPS 0052 | D 255 | Reward:μσmM 2.76 0.28 2.00 2.91 | policy_loss ['None', 'None', '-0.041', '-0.065', '0.025'] | value_loss ['None', 'None', '0.003', '0.051', '0.025']
U 486 | F 174592 | FPS 0054 | D 264 | Reward:μσmM 2.23 1.40 -1.00 2.93 | policy_loss ['None', 'None', '-0.037', '-0.154', '0.093'] | value_loss ['None', 'None', '0.003', '0.004', '0.501']
U 487 | F 175104 | FPS 0052 | D 274 | Reward:μσmM 2.68 0.77 0.00 2.94 | policy_loss ['None', 'None', '-0.018', '-0.055', '-0.249'] | value_loss ['None', 'None', '0.003', '0.002', '0.028']
U 488 | F 175616 | FPS 0048 | D 285 | Reward:μσmM 2.59 0.84 0.00 2.94 | policy_loss ['None', 'None', '-0.045', '0.038', '-0.004'] | value_loss ['None', 'None', '0.001', '0.110', '0.290']
U 489 | F 176128 | FPS 0049 | D 295 | Reward:μσmM 2.79 0.48 1.00 2.94 | policy_loss ['None', 'None', '-0.023', '-0.064', '-0.132'] | value_loss ['None', 'None', '0.001', '0.003', '0.008']
U 490 | F 176640 | FPS 0048 | D 306 | Reward:μσmM 2.77 0.67 0.00 2.94 | policy_loss ['None', 'None', '-0.011', '-0.029', '-0.136'] | value_loss ['None', 'None', '0.000', '0.001', '0.003']
U 10 | Test reward:μσmM -0.50 0.50 -1.00 0.00 | Test num frames:μσmM 151.20 111.95 5.00 256.00
Status saved
U 491 | F 177152 | FPS 0047 | D 388 | Reward:μσmM 2.86 0.23 2.00 2.94 | policy_loss ['None', 'None', '0.029', '0.028', '-0.036'] | value_loss ['None', 'None', '0.001', '0.001', '0.002']
U 492 | F 177664 | FPS 0050 | D 399 | Reward:μσmM 2.77 0.49 1.00 2.94 | policy_loss ['None', 'None', '0.016', '0.039', '0.041'] | value_loss ['None', 'None', '0.002', '0.001', '0.004']
U 493 | F 178176 | FPS 0053 | D 408 | Reward:μσmM 2.56 0.61 1.00 2.93 | policy_loss ['None', 'None', '0.069', '0.121', '0.206'] | value_loss ['None', 'None', '0.065', '0.042', '0.042']
U 494 | F 178688 | FPS 0051 | D 418 | Reward:μσmM 1.89 1.48 -1.00 2.92 | policy_loss ['None', 'None', '-0.045', '0.196', '0.251'] | value_loss ['None', 'None', '0.002', '0.315', '0.343']
U 495 | F 179200 | FPS 0050 | D 429 | Reward:μσmM 2.13 1.23 0.00 2.91 | policy_loss ['None', 'None', '-0.002', '0.070', '0.180'] | value_loss ['None', 'None', '0.002', '0.209', '0.061']
U 496 | F 179712 | FPS 0054 | D 438 | Reward:μσmM 1.66 1.56 -1.00 2.86 | policy_loss ['None', 'None', '0.074', '-0.047', '0.235'] | value_loss ['None', 'None', '0.004', '0.012', '0.232']
U 497 | F 180224 | FPS 0050 | D 448 | Reward:μσmM 1.26 1.77 -1.00 2.84 | policy_loss ['None', 'None', '0.026', '-0.104', '0.150'] | value_loss ['None', 'None', '0.002', '0.003', '0.310']
U 498 | F 180736 | FPS 0051 | D 458 | Reward:μσmM 1.53 1.58 -1.00 2.84 | policy_loss ['None', 'None', '0.040', '0.063', '0.052'] | value_loss ['None', 'None', '0.053', '0.059', '0.238']
U 499 | F 181248 | FPS 0051 | D 468 | Reward:μσmM 1.90 1.34 -1.00 2.87 | policy_loss ['None', 'None', '0.095', '0.015', '-0.056'] | value_loss ['None', 'None', '0.061', '0.004', '0.328']
U 500 | F 181760 | FPS 0054 | D 478 | Reward:μσmM 1.31 1.60 -1.00 2.86 | policy_loss ['None', 'None', '-0.046', '0.238', '0.193'] | value_loss ['None', 'None', '0.005', '0.459', '0.582']
U 10 | Test reward:μσmM -0.50 0.50 -1.00 0.00 | Test num frames:μσmM 160.00 112.65 1.00 256.00
Status saved
U 501 | F 182272 | FPS 0049 | D 561 | Reward:μσmM 2.55 0.64 1.00 2.86 | policy_loss ['None', 'None', '-0.095', '-0.249', '-0.004'] | value_loss ['None', 'None', '0.003', '0.069', '0.048']
U 502 | F 182784 | FPS 0047 | D 572 | Reward:μσmM 1.89 1.55 -1.00 2.90 | policy_loss ['None', 'None', '-0.039', '-0.144', '0.074'] | value_loss ['None', 'None', '0.002', '0.004', '0.378']
U 503 | F 183296 | FPS 0047 | D 583 | Reward:μσmM 2.19 1.36 -1.00 2.89 | policy_loss ['None', 'None', '-0.039', '-0.057', '-0.157'] | value_loss ['None', 'None', '0.002', '0.003', '0.245']
U 504 | F 183808 | FPS 0049 | D 594 | Reward:μσmM 2.49 1.08 -1.00 2.94 | policy_loss ['None', 'None', '-0.042', '-0.086', '-0.441'] | value_loss ['None', 'None', '0.002', '0.003', '0.124']
U 505 | F 184320 | FPS 0048 | D 605 | Reward:μσmM 2.85 0.24 2.00 2.94 | policy_loss ['None', 'None', '-0.040', '-0.017', '-0.344'] | value_loss ['None', 'None', '0.001', '0.004', '0.051']
U 506 | F 184832 | FPS 0044 | D 616 | Reward:μσmM 2.87 0.22 2.00 2.94 | policy_loss ['None', 'None', '-0.024', '-0.043', '-0.238'] | value_loss ['None', 'None', '0.000', '0.002', '0.023']
U 507 | F 185344 | FPS 0045 | D 628 | Reward:μσmM 2.78 0.66 0.00 2.95 | policy_loss ['None', 'None', '-0.011', '-0.038', '-0.110'] | value_loss ['None', 'None', '0.001', '0.001', '0.005']
U 508 | F 185856 | FPS 0046 | D 639 | Reward:μσmM 2.74 0.71 0.00 2.95 | policy_loss ['None', 'None', '0.030', '0.042', '-0.010'] | value_loss ['None', 'None', '0.001', '0.002', '0.004']
U 509 | F 186368 | FPS 0047 | D 650 | Reward:μσmM 2.70 0.75 0.00 2.93 | policy_loss ['None', 'None', '0.025', '0.021', '0.051'] | value_loss ['None', 'None', '0.002', '0.001', '0.005']
U 510 | F 186880 | FPS 0049 | D 660 | Reward:μσmM 2.37 0.94 0.00 2.92 | policy_loss ['None', 'None', '0.135', '0.214', '0.124'] | value_loss ['None', 'None', '0.109', '0.299', '0.019']
U 10 | Test reward:μσmM -0.20 0.40 -1.00 0.00 | Test num frames:μσmM 238.40 39.48 128.00 256.00
Status saved
U 511 | F 187392 | FPS 0047 | D 784 | Reward:μσmM 2.74 0.28 2.00 2.91 | policy_loss ['None', 'None', '-0.018', '0.066', '0.139'] | value_loss ['None', 'None', '0.002', '0.015', '0.005']
U 512 | F 187904 | FPS 0051 | D 794 | Reward:μσmM 1.05 1.18 0.00 2.87 | policy_loss ['None', 'None', '0.308', '0.358', '0.442'] | value_loss ['None', 'None', '0.248', '0.694', '0.399']
U 513 | F 188416 | FPS 0051 | D 804 | Reward:μσmM 1.18 1.48 -1.00 2.85 | policy_loss ['None', 'None', '-0.086', '0.400', '0.428'] | value_loss ['None', 'None', '0.003', '0.563', '0.440']
U 514 | F 188928 | FPS 0049 | D 814 | Reward:μσmM 2.26 0.73 1.00 2.83 | policy_loss ['None', 'None', '0.048', '-0.114', '0.189'] | value_loss ['None', 'None', '0.005', '0.029', '0.015']
U 515 | F 189440 | FPS 0051 | D 824 | Reward:μσmM 1.67 1.67 -1.00 2.91 | policy_loss ['None', 'None', '-0.013', '0.030', '0.198'] | value_loss ['None', 'None', '0.004', '0.323', '0.575']
U 516 | F 189952 | FPS 0048 | D 835 | Reward:μσmM 1.42 1.55 -1.00 2.89 | policy_loss ['None', 'None', '0.067', '-0.067', '0.123'] | value_loss ['None', 'None', '0.003', '0.216', '0.223']
U 517 | F 190464 | FPS 0050 | D 845 | Reward:μσmM 1.89 1.56 -1.00 2.88 | policy_loss ['None', 'None', '-0.050', '-0.138', '-0.103'] | value_loss ['None', 'None', '0.002', '0.015', '0.270']
U 518 | F 190976 | FPS 0050 | D 855 | Reward:μσmM 2.56 0.62 1.00 2.91 | policy_loss ['None', 'None', '0.018', '0.039', '-0.341'] | value_loss ['None', 'None', '0.078', '0.026', '0.073']
U 519 | F 191488 | FPS 0048 | D 866 | Reward:μσmM 2.62 0.83 0.00 2.95 | policy_loss ['None', 'None', '-0.093', '-0.228', '-0.231'] | value_loss ['None', 'None', '0.003', '0.024', '0.050']
U 520 | F 192000 | FPS 0047 | D 877 | Reward:μσmM 2.70 0.75 0.00 2.94 | policy_loss ['None', 'None', '-0.066', '-0.056', '-0.384'] | value_loss ['None', 'None', '0.001', '0.006', '0.054']
U 10 | Test reward:μσmM 0.00 0.00 0.00 0.00 | Test num frames:μσmM 256.00 0.00 256.00 256.00
Status saved
U 521 | F 192512 | FPS 0042 | D 1008 | Reward:μσmM 2.72 0.73 0.00 2.95 | policy_loss ['None', 'None', '-0.014', '-0.053', '-0.217'] | value_loss ['None', 'None', '0.001', '0.003', '0.017']
U 522 | F 193024 | FPS 0047 | D 1019 | Reward:μσmM 2.75 0.69 0.00 2.94 | policy_loss ['None', 'None', '-0.022', '-0.074', '-0.141'] | value_loss ['None', 'None', '0.000', '0.001', '0.006']
U 523 | F 193536 | FPS 0045 | D 1030 | Reward:μσmM 2.77 0.67 0.00 2.95 | policy_loss ['None', 'None', '-0.000', '-0.018', '-0.077'] | value_loss ['None', 'None', '0.000', '0.001', '0.002']
U 524 | F 194048 | FPS 0045 | D 1041 | Reward:μσmM 2.74 0.71 0.00 2.94 | policy_loss ['None', 'None', '0.025', '0.029', '-0.023'] | value_loss ['None', 'None', '0.001', '0.001', '0.001']
U 525 | F 194560 | FPS 0046 | D 1053 | Reward:μσmM 2.44 0.83 1.00 2.94 | policy_loss ['None', 'None', '0.158', '0.142', '0.163'] | value_loss ['None', 'None', '0.215', '0.122', '0.067']
U 526 | F 195072 | FPS 0047 | D 1063 | Reward:μσmM 2.76 0.29 2.00 2.92 | policy_loss ['None', 'None', '-0.036', '0.024', '0.189'] | value_loss ['None', 'None', '0.002', '0.002', '0.019']
U 527 | F 195584 | FPS 0047 | D 1074 | Reward:μσmM 2.77 0.27 2.00 2.90 | policy_loss ['None', 'None', '-0.026', '0.018', '0.059'] | value_loss ['None', 'None', '0.001', '0.007', '0.006']
U 528 | F 196096 | FPS 0050 | D 1085 | Reward:μσmM 1.02 1.84 -1.00 2.88 | policy_loss ['None', 'None', '0.012', '-0.047', '0.576'] | value_loss ['None', 'None', '0.001', '0.004', '0.947']
U 529 | F 196608 | FPS 0051 | D 1095 | Reward:μσmM 0.23 1.75 -1.00 2.92 | policy_loss ['None', 'None', '0.016', '-0.057', '0.769'] | value_loss ['None', 'None', '0.002', '0.004', '1.422']
U 530 | F 197120 | FPS 0049 | D 1105 | Reward:μσmM 1.47 1.73 -1.00 2.93 | policy_loss ['None', 'None', '0.033', '0.140', '0.116'] | value_loss ['None', 'None', '0.085', '0.119', '0.538']
U 10 | Test reward:μσmM -0.10 0.30 -1.00 0.00 | Test num frames:μσmM 231.50 73.50 11.00 256.00
Status saved
U 531 | F 197632 | FPS 0046 | D 1224 | Reward:μσmM 1.36 1.82 -1.00 2.91 | policy_loss ['None', 'None', '-0.028', '-0.011', '0.020'] | value_loss ['None', 'None', '0.002', '0.004', '0.633']
U 532 | F 198144 | FPS 0046 | D 1235 | Reward:μσmM 1.35 1.86 -1.00 2.92 | policy_loss ['None', 'None', '-0.030', '0.002', '-0.181'] | value_loss ['None', 'None', '0.001', '0.005', '0.681']
U 533 | F 198656 | FPS 0045 | D 1246 | Reward:μσmM 1.73 1.51 -1.00 2.95 | policy_loss ['None', 'None', '-0.026', '0.247', '-0.138'] | value_loss ['None', 'None', '0.001', '0.716', '0.310']
U 534 | F 199168 | FPS 0048 | D 1257 | Reward:μσmM 2.72 0.54 1.00 2.94 | policy_loss ['None', 'None', '0.016', '-0.132', '-0.144'] | value_loss ['None', 'None', '0.001', '0.029', '0.033']
U 535 | F 199680 | FPS 0045 | D 1268 | Reward:μσmM 2.76 0.51 1.00 2.93 | policy_loss ['None', 'None', '-0.008', '-0.129', '-0.151'] | value_loss ['None', 'None', '0.001', '0.011', '0.012']
U 536 | F 200192 | FPS 0047 | D 1279 | Reward:μσmM 2.84 0.24 2.00 2.93 | policy_loss ['None', 'None', '0.003', '-0.058', '-0.084'] | value_loss ['None', 'None', '0.002', '0.002', '0.006']
U 537 | F 200704 | FPS 0047 | D 1290 | Reward:μσmM 2.68 0.64 1.00 2.94 | policy_loss ['None', 'None', '0.012', '0.046', '-0.047'] | value_loss ['None', 'None', '0.053', '0.087', '0.023']
U 538 | F 201216 | FPS 0047 | D 1301 | Reward:μσmM 2.83 0.26 2.00 2.95 | policy_loss ['None', 'None', '0.009', '-0.012', '0.020'] | value_loss ['None', 'None', '0.003', '0.008', '0.005']
U 539 | F 201728 | FPS 0044 | D 1313 | Reward:μσmM 2.81 0.26 2.00 2.91 | policy_loss ['None', 'None', '0.006', '-0.031', '0.061'] | value_loss ['None', 'None', '0.002', '0.001', '0.007']
U 540 | F 202240 | FPS 0045 | D 1324 | Reward:μσmM 2.61 0.60 1.00 2.92 | policy_loss ['None', 'None', '0.074', '0.082', '0.047'] | value_loss ['None', 'None', '0.092', '0.027', '0.012']
U 10 | Test reward:μσmM -0.50 0.50 -1.00 0.00 | Test num frames:μσmM 187.00 96.49 10.00 256.00
Status saved
U 541 | F 202752 | FPS 0045 | D 1424 | Reward:μσmM 2.51 0.63 1.00 2.88 | policy_loss ['None', 'None', '0.051', '0.088', '0.144'] | value_loss ['None', 'None', '0.103', '0.046', '0.013']
U 542 | F 203264 | FPS 0047 | D 1435 | Reward:μσmM 2.60 0.31 2.00 2.80 | policy_loss ['None', 'None', '-0.016', '0.070', '0.167'] | value_loss ['None', 'None', '0.003', '0.010', '0.009']
U 543 | F 203776 | FPS 0047 | D 1446 | Reward:μσmM 2.01 1.19 0.00 2.90 | policy_loss ['None', 'None', '-0.059', '0.195', '0.203'] | value_loss ['None', 'None', '0.004', '0.677', '0.117']
U 544 | F 204288 | FPS 0049 | D 1456 | Reward:μσmM 0.84 1.85 -1.00 2.87 | policy_loss ['None', 'None', '-0.042', '0.051', '0.544'] | value_loss ['None', 'None', '0.001', '0.361', '0.928']
U 545 | F 204800 | FPS 0050 | D 1466 | Reward:μσmM 1.51 1.71 -1.00 2.88 | policy_loss ['None', 'None', '-0.025', '-0.186', '0.165'] | value_loss ['None', 'None', '0.003', '0.020', '0.277']
U 546 | F 205312 | FPS 0045 | D 1478 | Reward:μσmM 2.32 1.28 -1.00 2.91 | policy_loss ['None', 'None', '-0.068', '-0.161', '-0.357'] | value_loss ['None', 'None', '0.001', '0.006', '0.099']
U 547 | F 205824 | FPS 0046 | D 1489 | Reward:μσmM 2.36 1.11 0.00 2.94 | policy_loss ['None', 'None', '-0.001', '0.257', '-0.288'] | value_loss ['None', 'None', '0.001', '0.356', '0.037']
U 548 | F 206336 | FPS 0047 | D 1500 | Reward:μσmM 2.84 0.24 2.00 2.94 | policy_loss ['None', 'None', '-0.019', '-0.048', '-0.377'] | value_loss ['None', 'None', '0.001', '0.002', '0.056']
U 549 | F 206848 | FPS 0044 | D 1512 | Reward:μσmM 2.75 0.69 0.00 2.95 | policy_loss ['None', 'None', '-0.031', '-0.030', '-0.216'] | value_loss ['None', 'None', '0.001', '0.002', '0.016']
U 550 | F 207360 | FPS 0044 | D 1523 | Reward:μσmM 2.75 0.69 0.00 2.94 | policy_loss ['None', 'None', '-0.002', '-0.041', '-0.041'] | value_loss ['None', 'None', '0.001', '0.002', '0.002']
U 10 | Test reward:μσmM 0.00 0.00 0.00 0.00 | Test num frames:μσmM 256.00 0.00 256.00 256.00
Status saved
U 551 | F 207872 | FPS 0044 | D 1654 | Reward:μσmM 2.86 0.23 2.00 2.94 | policy_loss ['None', 'None', '-0.001', '0.009', '0.051'] | value_loss ['None', 'None', '0.001', '0.002', '0.009']
U 552 | F 208384 | FPS 0048 | D 1665 | Reward:μσmM 2.03 0.88 1.00 2.91 | policy_loss ['None', 'None', '0.278', '0.288', '0.165'] | value_loss ['None', 'None', '0.277', '0.179', '0.041']
U 553 | F 208896 | FPS 0048 | D 1676 | Reward:μσmM 0.59 1.08 -1.00 2.85 | policy_loss ['None', 'None', '0.554', '0.257', '0.452'] | value_loss ['None', 'None', '0.393', '0.173', '0.720']
U 554 | F 209408 | FPS 0049 | D 1686 | Reward:μσmM 1.73 1.25 0.00 2.81 | policy_loss ['None', 'None', '-0.082', '0.203', '0.219'] | value_loss ['None', 'None', '0.088', '0.173', '0.023']
U 555 | F 209920 | FPS 0049 | D 1696 | Reward:μσmM 1.02 1.72 -1.00 2.87 | policy_loss ['None', 'None', '-0.013', '-0.018', '0.473'] | value_loss ['None', 'None', '0.003', '0.010', '0.584']
U 556 | F 210432 | FPS 0051 | D 1707 | Reward:μσmM 1.38 1.50 -1.00 2.80 | policy_loss ['None', 'None', '0.105', '0.044', '0.229'] | value_loss ['None', 'None', '0.107', '0.012', '0.261']
U 557 | F 210944 | FPS 0049 | D 1717 | Reward:μσmM 2.38 0.69 1.00 2.83 | policy_loss ['None', 'None', '-0.024', '-0.024', '0.037'] | value_loss ['None', 'None', '0.004', '0.025', '0.014']
U 558 | F 211456 | FPS 0050 | D 1727 | Reward:μσmM 1.87 1.52 -1.00 2.86 | policy_loss ['None', 'None', '-0.101', '-0.187', '0.132'] | value_loss ['None', 'None', '0.002', '0.034', '0.235']
U 559 | F 211968 | FPS 0048 | D 1738 | Reward:μσmM 2.40 1.16 -1.00 2.91 | policy_loss ['None', 'None', '-0.081', '-0.199', '-0.226'] | value_loss ['None', 'None', '0.002', '0.012', '0.205']
U 560 | F 212480 | FPS 0045 | D 1749 | Reward:μσmM 2.19 1.45 -1.00 2.94 | policy_loss ['None', 'None', '-0.041', '-0.156', '-0.120'] | value_loss ['None', 'None', '0.001', '0.004', '0.576']
U 10 | Test reward:μσmM 0.00 0.00 0.00 0.00 | Test num frames:μσmM 256.00 0.00 256.00 256.00
Status saved
U 561 | F 212992 | FPS 0046 | D 1880 | Reward:μσmM 2.52 1.12 -1.00 2.95 | policy_loss ['None', 'None', '-0.027', '-0.083', '-0.444'] | value_loss ['None', 'None', '0.001', '0.002', '0.209']
U 562 | F 213504 | FPS 0046 | D 1892 | Reward:μσmM 2.82 0.45 1.00 2.94 | policy_loss ['None', 'None', '-0.007', '-0.043', '-0.431'] | value_loss ['None', 'None', '0.001', '0.001', '0.076']
U 563 | F 214016 | FPS 0045 | D 1903 | Reward:μσmM 2.82 0.45 1.00 2.95 | policy_loss ['None', 'None', '0.001', '0.012', '-0.218'] | value_loss ['None', 'None', '0.001', '0.002', '0.023']
U 564 | F 214528 | FPS 0049 | D 1913 | Reward:μσmM 2.74 0.71 0.00 2.95 | policy_loss ['None', 'None', '0.005', '0.037', '-0.076'] | value_loss ['None', 'None', '0.001', '0.004', '0.006']
U 565 | F 215040 | FPS 0050 | D 1924 | Reward:μσmM 2.88 0.03 2.82 2.91 | policy_loss ['None', 'None', '0.042', '0.056', '0.074'] | value_loss ['None', 'None', '0.001', '0.004', '0.004']
U 566 | F 215552 | FPS 0050 | D 1934 | Reward:μσmM 2.33 0.67 1.00 2.88 | policy_loss ['None', 'None', '0.109', '0.146', '0.190'] | value_loss ['None', 'None', '0.048', '0.063', '0.017']
U 567 | F 216064 | FPS 0051 | D 1944 | Reward:μσmM 2.67 0.30 2.00 2.86 | policy_loss ['None', 'None', '0.019', '0.066', '0.072'] | value_loss ['None', 'None', '0.002', '0.005', '0.003']
U 568 | F 216576 | FPS 0050 | D 1954 | Reward:μσmM 1.13 0.82 0.00 2.78 | policy_loss ['None', 'None', '0.506', '0.296', '0.233'] | value_loss ['None', 'None', '0.450', '0.186', '0.047']
U 569 | F 217088 | FPS 0050 | D 1964 | Reward:μσmM 0.80 0.87 0.00 2.62 | policy_loss ['None', 'None', '0.333', '0.373', '0.239'] | value_loss ['None', 'None', '0.252', '0.274', '0.070']
U 570 | F 217600 | FPS 0052 | D 1974 | Reward:μσmM 0.73 1.74 -1.00 2.80 | policy_loss ['None', 'None', '-0.086', '0.021', '0.449'] | value_loss ['None', 'None', '0.005', '0.005', '0.649']
U 10 | Test reward:μσmM -0.90 0.30 -1.00 0.00 | Test num frames:μσmM 82.90 72.79 1.00 256.00
Status saved
U 571 | F 218112 | FPS 0044 | D 2027 | Reward:μσmM 1.02 1.41 -1.00 2.85 | policy_loss ['None', 'None', '-0.111', '0.204', '0.218'] | value_loss ['None', 'None', '0.044', '0.360', '0.220']
U 572 | F 218624 | FPS 0047 | D 2038 | Reward:μσmM -0.03 1.27 -1.00 2.85 | policy_loss ['None', 'None', '0.016', '0.381', '0.430'] | value_loss ['None', 'None', '0.123', '0.577', '0.662']
U 573 | F 219136 | FPS 0051 | D 2048 | Reward:μσmM 0.32 1.51 -1.00 2.66 | policy_loss ['None', 'None', '-0.040', '0.188', '0.254'] | value_loss ['None', 'None', '0.005', '0.191', '0.399']
U 574 | F 219648 | FPS 0050 | D 2058 | Reward:μσmM 1.02 1.76 -1.00 2.82 | policy_loss ['None', 'None', '-0.058', '-0.367', '0.098'] | value_loss ['None', 'None', '0.005', '0.054', '0.226']
U 575 | F 220160 | FPS 0050 | D 2068 | Reward:μσmM 2.52 0.31 2.00 2.81 | policy_loss ['None', 'None', '0.004', '-0.234', '-0.200'] | value_loss ['None', 'None', '0.002', '0.012', '0.074']
U 576 | F 220672 | FPS 0049 | D 2079 | Reward:μσmM 2.50 0.63 1.00 2.89 | policy_loss ['None', 'None', '-0.046', '-0.056', '-0.492'] | value_loss ['None', 'None', '0.045', '0.042', '0.153']
U 577 | F 221184 | FPS 0048 | D 2090 | Reward:μσmM 2.76 0.27 2.00 2.92 | policy_loss ['None', 'None', '-0.087', '-0.139', '-0.557'] | value_loss ['None', 'None', '0.005', '0.009', '0.142']
U 578 | F 221696 | FPS 0048 | D 2100 | Reward:μσmM 2.89 0.02 2.83 2.93 | policy_loss ['None', 'None', '-0.084', '-0.181', '-0.432'] | value_loss ['None', 'None', '0.003', '0.004', '0.067']
U 579 | F 222208 | FPS 0044 | D 2112 | Reward:μσmM 2.84 0.24 2.00 2.93 | policy_loss ['None', 'None', '-0.053', '-0.086', '-0.355'] | value_loss ['None', 'None', '0.002', '0.004', '0.054']
U 580 | F 222720 | FPS 0047 | D 2122 | Reward:μσmM 2.86 0.23 2.00 2.95 | policy_loss ['None', 'None', '-0.027', '-0.055', '-0.228'] | value_loss ['None', 'None', '0.002', '0.003', '0.015']
U 10 | Test reward:μσmM -0.10 0.30 -1.00 0.00 | Test num frames:μσmM 230.80 75.60 4.00 256.00
Status saved
U 581 | F 223232 | FPS 0045 | D 2242 | Reward:μσmM 2.78 0.66 0.00 2.96 | policy_loss ['None', 'None', '-0.032', '-0.052', '-0.184'] | value_loss ['None', 'None', '0.001', '0.001', '0.006']
U 582 | F 223744 | FPS 0045 | D 2254 | Reward:μσmM 2.78 0.66 0.00 2.95 | policy_loss ['None', 'None', '-0.010', '-0.011', '-0.082'] | value_loss ['None', 'None', '0.001', '0.002', '0.003']
U 583 | F 224256 | FPS 0048 | D 2265 | Reward:μσmM 2.88 0.21 2.00 2.96 | policy_loss ['None', 'None', '0.011', '0.005', '-0.019'] | value_loss ['None', 'None', '0.001', '0.002', '0.002']
U 584 | F 224768 | FPS 0046 | D 2276 | Reward:μσmM 2.08 0.92 1.00 2.95 | policy_loss ['None', 'None', '0.255', '0.287', '0.344'] | value_loss ['None', 'None', '0.370', '0.283', '0.142']
U 585 | F 225280 | FPS 0049 | D 2286 | Reward:μσmM 2.40 0.98 0.00 2.91 | policy_loss ['None', 'None', '0.080', '0.054', '0.136'] | value_loss ['None', 'None', '0.098', '0.005', '0.013']
U 586 | F 225792 | FPS 0049 | D 2297 | Reward:μσmM 1.77 1.22 -1.00 2.91 | policy_loss ['None', 'None', '0.230', '0.043', '0.292'] | value_loss ['None', 'None', '0.243', '0.071', '0.501']
U 587 | F 226304 | FPS 0046 | D 2308 | Reward:μσmM 1.92 0.92 1.00 2.85 | policy_loss ['None', 'None', '0.149', '0.074', '0.061'] | value_loss ['None', 'None', '0.089', '0.005', '0.004']
U 588 | F 226816 | FPS 0050 | D 2318 | Reward:μσmM 1.80 1.07 0.00 2.89 | policy_loss ['None', 'None', '0.152', '0.202', '0.057'] | value_loss ['None', 'None', '0.139', '0.137', '0.018']
U 589 | F 227328 | FPS 0048 | D 2329 | Reward:μσmM 1.57 1.40 0.00 2.86 | policy_loss ['None', 'None', '-0.196', '0.156', '0.304'] | value_loss ['None', 'None', '0.022', '0.449', '0.199']
U 590 | F 227840 | FPS 0048 | D 2339 | Reward:μσmM 2.00 1.38 -1.00 2.89 | policy_loss ['None', 'None', '-0.044', '-0.098', '0.030'] | value_loss ['None', 'None', '0.008', '0.215', '0.094']
U 10 | Test reward:μσmM -0.80 0.40 -1.00 0.00 | Test num frames:μσmM 82.90 95.70 8.00 256.00
Status saved
U 591 | F 228352 | FPS 0050 | D 2391 | Reward:μσmM 2.30 1.03 0.00 2.88 | policy_loss ['None', 'None', '-0.059', '-0.171', '0.102'] | value_loss ['None', 'None', '0.012', '0.031', '0.031']
U 592 | F 228864 | FPS 0049 | D 2401 | Reward:μσmM 2.79 0.26 2.00 2.92 | policy_loss ['None', 'None', '-0.085', '-0.260', '-0.170'] | value_loss ['None', 'None', '0.001', '0.013', '0.041']
U 593 | F 229376 | FPS 0045 | D 2413 | Reward:μσmM 2.83 0.25 2.00 2.93 | policy_loss ['None', 'None', '-0.052', '-0.130', '-0.336'] | value_loss ['None', 'None', '0.002', '0.006', '0.040']
U 594 | F 229888 | FPS 0045 | D 2424 | Reward:μσmM 2.78 0.49 1.00 2.94 | policy_loss ['None', 'None', '-0.023', '-0.112', '-0.178'] | value_loss ['None', 'None', '0.001', '0.005', '0.030']
U 595 | F 230400 | FPS 0044 | D 2436 | Reward:μσmM 2.93 0.01 2.92 2.95 | policy_loss ['None', 'None', '-0.017', '-0.096', '-0.258'] | value_loss ['None', 'None', '0.000', '0.001', '0.013']
U 596 | F 230912 | FPS 0044 | D 2447 | Reward:μσmM 2.79 0.64 0.00 2.96 | policy_loss ['None', 'None', '-0.010', '-0.024', '-0.144'] | value_loss ['None', 'None', '0.001', '0.001', '0.002']
U 597 | F 231424 | FPS 0045 | D 2459 | Reward:μσmM 2.80 0.47 1.00 2.95 | policy_loss ['None', 'None', '-0.088', '0.044', '-0.008'] | value_loss ['None', 'None', '0.004', '0.012', '0.011']
U 598 | F 231936 | FPS 0048 | D 2469 | Reward:μσmM 2.54 0.70 1.00 2.93 | policy_loss ['None', 'None', '0.090', '0.193', '0.116'] | value_loss ['None', 'None', '0.074', '0.221', '0.029']
U 599 | F 232448 | FPS 0051 | D 2479 | Reward:μσmM 2.30 0.82 1.00 2.84 | policy_loss ['None', 'None', '0.195', '0.079', '0.210'] | value_loss ['None', 'None', '0.187', '0.006', '0.011']
U 600 | F 232960 | FPS 0047 | D 2490 | Reward:μσmM 2.70 0.12 2.51 2.84 | policy_loss ['None', 'None', '0.042', '0.071', '0.160'] | value_loss ['None', 'None', '0.003', '0.006', '0.004']
U 10 | Test reward:μσmM -1.00 0.00 -1.00 -1.00 | Test num frames:μσmM 51.40 39.99 11.00 161.00
Status saved
U 601 | F 233472 | FPS 0049 | D 2527 | Reward:μσmM 1.99 1.09 0.00 2.78 | policy_loss ['None', 'None', '-0.006', '0.220', '0.162'] | value_loss ['None', 'None', '0.002', '0.361', '0.027']
U 602 | F 233984 | FPS 0047 | D 2538 | Reward:μσmM 0.92 1.58 -1.00 2.85 | policy_loss ['None', 'None', '-0.119', '0.235', '0.285'] | value_loss ['None', 'None', '0.003', '0.207', '0.228']
U 603 | F 234496 | FPS 0050 | D 2548 | Reward:μσmM 2.48 0.66 1.00 2.88 | policy_loss ['None', 'None', '-0.004', '-0.259', '0.037'] | value_loss ['None', 'None', '0.002', '0.030', '0.032']
U 604 | F 235008 | FPS 0046 | D 2559 | Reward:μσmM 2.33 1.21 -1.00 2.88 | policy_loss ['None', 'None', '-0.019', '-0.205', '-0.120'] | value_loss ['None', 'None', '0.001', '0.004', '0.175']
U 605 | F 235520 | FPS 0050 | D 2569 | Reward:μσmM 2.58 0.86 0.00 2.90 | policy_loss ['None', 'None', '-0.034', '-0.054', '-0.245'] | value_loss ['None', 'None', '0.003', '0.005', '0.025']
U 606 | F 236032 | FPS 0047 | D 2580 | Reward:μσmM 2.70 0.75 0.00 2.93 | policy_loss ['None', 'None', '-0.063', '-0.089', '-0.234'] | value_loss ['None', 'None', '0.001', '0.003', '0.025']
U 607 | F 236544 | FPS 0048 | D 2591 | Reward:μσmM 2.86 0.23 2.00 2.95 | policy_loss ['None', 'None', '-0.043', '-0.048', '-0.163'] | value_loss ['None', 'None', '0.001', '0.001', '0.022']
U 608 | F 237056 | FPS 0046 | D 2602 | Reward:μσmM 2.88 0.21 2.00 2.95 | policy_loss ['None', 'None', '-0.013', '-0.037', '-0.145'] | value_loss ['None', 'None', '0.000', '0.001', '0.005']
U 609 | F 237568 | FPS 0049 | D 2612 | Reward:μσmM 2.77 0.67 0.00 2.95 | policy_loss ['None', 'None', '0.012', '-0.009', '-0.060'] | value_loss ['None', 'None', '0.001', '0.001', '0.004']
U 610 | F 238080 | FPS 0045 | D 2624 | Reward:μσmM 2.77 0.67 0.00 2.95 | policy_loss ['None', 'None', '-0.002', '0.013', '-0.006'] | value_loss ['None', 'None', '0.000', '0.001', '0.002']
U 10 | Test reward:μσmM -0.20 0.40 -1.00 0.00 | Test num frames:μσmM 208.50 95.08 10.00 256.00
Status saved
U 611 | F 238592 | FPS 0048 | D 2737 | Reward:μσmM 2.71 0.54 1.00 2.93 | policy_loss ['None', 'None', '0.041', '0.114', '0.069'] | value_loss ['None', 'None', '0.002', '0.008', '0.003']
U 612 | F 239104 | FPS 0048 | D 2747 | Reward:μσmM 2.49 0.67 1.00 2.90 | policy_loss ['None', 'None', '0.052', '0.129', '0.287'] | value_loss ['None', 'None', '0.003', '0.007', '0.064']
U 613 | F 239616 | FPS 0050 | D 2758 | Reward:μσmM -0.25 0.43 -1.00 0.00 | policy_loss ['None', 'None', 'None', '0.583', '0.481'] | value_loss ['None', 'None', 'None', '0.504', '0.368']
U 614 | F 240128 | FPS 0046 | D 2769 | Reward:μσmM 0.08 1.36 -1.00 2.33 | policy_loss ['None', 'None', '0.212', '-0.219', '0.380'] | value_loss ['None', 'None', '0.012', '0.017', '0.217']
U 615 | F 240640 | FPS 0049 | D 2779 | Reward:μσmM -0.33 0.47 -1.00 0.00 | policy_loss ['None', 'None', 'None', '0.698', '0.304'] | value_loss ['None', 'None', 'None', '0.761', '0.133']
U 616 | F 241152 | FPS 0050 | D 2790 | Reward:μσmM 0.50 0.76 -1.00 1.00 | policy_loss ['None', 'None', '0.432', '-0.076', '0.172'] | value_loss ['None', 'None', '0.226', '0.059', '0.085']
U 617 | F 241664 | FPS 0051 | D 2800 | Reward:μσmM -0.83 0.37 -1.00 0.00 | policy_loss ['None', 'None', 'None', 'None', '0.475'] | value_loss ['None', 'None', 'None', 'None', '0.397']
U 618 | F 242176 | FPS 0052 | D 2809 | Reward:μσmM 1.33 1.51 -1.00 2.71 | policy_loss ['None', 'None', '-0.123', '-0.034', '0.152'] | value_loss ['None', 'None', '0.006', '0.023', '0.109']
U 619 | F 242688 | FPS 0049 | D 2820 | Reward:μσmM 0.05 1.48 -1.00 2.76 | policy_loss ['None', 'None', '0.052', '-0.188', '0.366'] | value_loss ['None', 'None', '0.092', '0.031', '0.519']
U 620 | F 243200 | FPS 0048 | D 2830 | Reward:μσmM 1.10 1.30 -1.00 2.89 | policy_loss ['None', 'None', '0.194', '-0.238', '-0.129'] | value_loss ['None', 'None', '0.140', '0.017', '0.253']
U 10 | Test reward:μσmM -0.90 0.30 -1.00 0.00 | Test num frames:μσmM 74.00 88.38 3.00 256.00
Status saved
U 621 | F 243712 | FPS 0048 | D 2878 | Reward:μσmM 0.76 0.99 -1.00 2.83 | policy_loss ['None', 'None', '0.458', '0.492', '-0.111'] | value_loss ['None', 'None', '0.378', '0.410', '0.159']
U 622 | F 244224 | FPS 0049 | D 2888 | Reward:μσmM 1.12 0.81 0.00 2.74 | policy_loss ['None', 'None', '0.218', '0.140', '-0.187'] | value_loss ['None', 'None', '0.108', '0.031', '0.079']
U 623 | F 244736 | FPS 0045 | D 2900 | Reward:μσmM 0.95 1.20 0.00 2.87 | policy_loss ['None', 'None', '-0.144', '0.442', '0.031'] | value_loss ['None', 'None', '0.099', '0.450', '0.183']
U 624 | F 245248 | FPS 0047 | D 2911 | Reward:μσmM 0.95 1.25 -1.00 2.85 | policy_loss ['None', 'None', '-0.015', '0.149', '0.216'] | value_loss ['None', 'None', '0.107', '0.302', '0.234']
U 625 | F 245760 | FPS 0046 | D 2922 | Reward:μσmM 1.84 1.30 0.00 2.81 | policy_loss ['None', 'None', '-0.229', '-0.186', '0.041'] | value_loss ['None', 'None', '0.056', '0.341', '0.034']
U 626 | F 246272 | FPS 0049 | D 2932 | Reward:μσmM 2.49 0.94 0.00 2.92 | policy_loss ['None', 'None', '-0.262', '-0.451', '-0.203'] | value_loss ['None', 'None', '0.036', '0.097', '0.054']
U 627 | F 246784 | FPS 0048 | D 2943 | Reward:μσmM 2.79 0.27 2.00 2.94 | policy_loss ['None', 'None', '-0.161', '-0.543', '-0.378'] | value_loss ['None', 'None', '0.018', '0.124', '0.101']
U 628 | F 247296 | FPS 0047 | D 2953 | Reward:μσmM 2.81 0.26 2.00 2.93 | policy_loss ['None', 'None', '-0.118', '-0.252', '-0.426'] | value_loss ['None', 'None', '0.009', '0.017', '0.083']
U 629 | F 247808 | FPS 0047 | D 2964 | Reward:μσmM 2.77 0.67 0.00 2.95 | policy_loss ['None', 'None', '-0.084', '-0.224', '-0.485'] | value_loss ['None', 'None', '0.001', '0.002', '0.097']
U 630 | F 248320 | FPS 0047 | D 2975 | Reward:μσmM 2.80 0.63 0.00 2.95 | policy_loss ['None', 'None', '-0.022', '-0.078', '-0.397'] | value_loss ['None', 'None', '0.000', '0.000', '0.049']
U 10 | Test reward:μσmM 0.00 0.00 0.00 0.00 | Test num frames:μσmM 256.00 0.00 256.00 256.00
Status saved
U 631 | F 248832 | FPS 0045 | D 3113 | Reward:μσmM 2.78 0.66 0.00 2.95 | policy_loss ['None', 'None', '0.016', '-0.001', '-0.207'] | value_loss ['None', 'None', '0.001', '0.001', '0.008']
U 632 | F 249344 | FPS 0046 | D 3124 | Reward:μσmM 2.62 0.82 0.00 2.95 | policy_loss ['None', 'None', '0.050', '0.092', '0.018'] | value_loss ['None', 'None', '0.043', '0.059', '0.044']
U 633 | F 249856 | FPS 0050 | D 3134 | Reward:μσmM 1.95 1.11 0.00 2.95 | policy_loss ['None', 'None', '0.135', '0.163', '0.152'] | value_loss ['None', 'None', '0.071', '0.060', '0.029']
U 634 | F 250368 | FPS 0052 | D 3144 | Reward:μσmM 2.69 0.35 2.00 2.91 | policy_loss ['None', 'None', '-0.026', '0.022', '0.134'] | value_loss ['None', 'None', '0.007', '0.028', '0.012']
U 635 | F 250880 | FPS 0048 | D 3155 | Reward:μσmM 2.58 0.86 0.00 2.95 | policy_loss ['None', 'None', '-0.089', '-0.047', '0.050'] | value_loss ['None', 'None', '0.007', '0.018', '0.024']
U 636 | F 251392 | FPS 0051 | D 3165 | Reward:μσmM 2.58 0.86 0.00 2.94 | policy_loss ['None', 'None', '0.015', '-0.110', '-0.081'] | value_loss ['None', 'None', '0.002', '0.002', '0.018']
U 637 | F 251904 | FPS 0049 | D 3175 | Reward:μσmM 2.82 0.25 2.00 2.93 | policy_loss ['None', 'None', '-0.017', '-0.028', '-0.184'] | value_loss ['None', 'None', '0.002', '0.002', '0.015']
U 638 | F 252416 | FPS 0047 | D 3186 | Reward:μσmM 2.56 0.86 0.00 2.94 | policy_loss ['None', 'None', '0.039', '0.053', '-0.054'] | value_loss ['None', 'None', '0.111', '0.086', '0.043']
U 639 | F 252928 | FPS 0049 | D 3197 | Reward:μσmM 2.21 0.75 1.00 2.84 | policy_loss ['None', 'None', '0.011', '0.226', '0.122'] | value_loss ['None', 'None', '0.036', '0.012', '0.004']
U 640 | F 253440 | FPS 0050 | D 3207 | Reward:μσmM 2.62 0.83 0.00 2.91 | policy_loss ['None', 'None', '-0.092', '-0.083', '0.059'] | value_loss ['None', 'None', '0.003', '0.010', '0.005']
U 10 | Test reward:μσmM -0.60 0.49 -1.00 0.00 | Test num frames:μσmM 149.80 113.36 2.00 256.00
Status saved
U 641 | F 253952 | FPS 0050 | D 3292 | Reward:μσmM 2.36 1.11 0.00 2.92 | policy_loss ['None', 'None', '-0.049', '-0.009', '0.179'] | value_loss ['None', 'None', '0.003', '0.156', '0.160']
U 642 | F 254464 | FPS 0052 | D 3302 | Reward:μσmM 2.58 0.86 0.00 2.93 | policy_loss ['None', 'None', '-0.000', '-0.137', '-0.011'] | value_loss ['None', 'None', '0.002', '0.007', '0.024']
U 643 | F 254976 | FPS 0048 | D 3313 | Reward:μσmM 2.32 1.28 -1.00 2.93 | policy_loss ['None', 'None', '-0.035', '-0.025', '0.077'] | value_loss ['None', 'None', '0.001', '0.005', '0.294']
U 644 | F 255488 | FPS 0045 | D 3324 | Reward:μσmM 1.85 1.66 -1.00 2.94 | policy_loss ['None', 'None', '-0.012', '-0.083', '0.077'] | value_loss ['None', 'None', '0.001', '0.002', '0.423']
U 645 | F 256000 | FPS 0045 | D 3335 | Reward:μσmM 2.73 0.52 1.00 2.95 | policy_loss ['None', 'None', '0.079', '-0.010', '-0.335'] | value_loss ['None', 'None', '0.134', '0.019', '0.062']
U 646 | F 256512 | FPS 0046 | D 3346 | Reward:μσmM 2.74 0.53 1.00 2.94 | policy_loss ['None', 'None', '-0.039', '0.099', '-0.177'] | value_loss ['None', 'None', '0.002', '0.008', '0.020']
U 647 | F 257024 | FPS 0049 | D 3357 | Reward:μσmM 2.68 0.63 1.00 2.96 | policy_loss ['None', 'None', '-0.034', '0.088', '-0.056'] | value_loss ['None', 'None', '0.002', '0.031', '0.017']
U 648 | F 257536 | FPS 0047 | D 3368 | Reward:μσmM 2.72 0.73 0.00 2.94 | policy_loss ['None', 'None', '-0.020', '-0.027', '-0.027'] | value_loss ['None', 'None', '0.001', '0.003', '0.009']
U 649 | F 258048 | FPS 0047 | D 3379 | Reward:μσmM 2.93 0.01 2.91 2.95 | policy_loss ['None', 'None', '-0.018', '-0.046', '-0.090'] | value_loss ['None', 'None', '0.001', '0.002', '0.005']
U 650 | F 258560 | FPS 0049 | D 3389 | Reward:μσmM 2.88 0.21 2.00 2.95 | policy_loss ['None', 'None', '0.001', '-0.017', '-0.068'] | value_loss ['None', 'None', '0.001', '0.001', '0.001']
U 10 | Test reward:μσmM -0.50 0.50 -1.00 0.00 | Test num frames:μσmM 144.60 111.72 21.00 256.00
Status saved
U 651 | F 259072 | FPS 0048 | D 3470 | Reward:μσmM 2.18 0.94 1.00 2.94 | policy_loss ['None', 'None', '0.206', '0.243', '0.210'] | value_loss ['None', 'None', '0.265', '0.260', '0.130']
U 652 | F 259584 | FPS 0051 | D 3480 | Reward:μσmM 2.51 0.75 1.00 2.91 | policy_loss ['None', 'None', '0.081', '0.030', '0.059'] | value_loss ['None', 'None', '0.053', '0.003', '0.003']
U 653 | F 260096 | FPS 0048 | D 3490 | Reward:μσmM 2.33 1.21 -1.00 2.90 | policy_loss ['None', 'None', '0.009', '-0.013', '0.214'] | value_loss ['None', 'None', '0.002', '0.007', '0.234']
U 654 | F 260608 | FPS 0052 | D 3500 | Reward:μσmM 1.80 1.37 -1.00 2.91 | policy_loss ['None', 'None', '0.100', '0.087', '0.189'] | value_loss ['None', 'None', '0.115', '0.226', '0.249']
U 655 | F 261120 | FPS 0050 | D 3510 | Reward:μσmM 1.86 1.32 -1.00 2.89 | policy_loss ['None', 'None', '0.004', '0.092', '0.067'] | value_loss ['None', 'None', '0.055', '0.037', '0.037']
U 656 | F 261632 | FPS 0049 | D 3521 | Reward:μσmM 1.74 1.26 0.00 2.86 | policy_loss ['None', 'None', '-0.038', '0.326', '0.035'] | value_loss ['None', 'None', '0.004', '0.365', '0.013']
U 657 | F 262144 | FPS 0052 | D 3531 | Reward:μσmM 2.17 1.01 0.00 2.82 | policy_loss ['None', 'None', '0.009', '-0.033', '0.213'] | value_loss ['None', 'None', '0.002', '0.100', '0.110']
U 658 | F 262656 | FPS 0048 | D 3541 | Reward:μσmM 1.64 1.19 0.00 2.87 | policy_loss ['None', 'None', '0.050', '0.068', '0.173'] | value_loss ['None', 'None', '0.084', '0.253', '0.071']
U 659 | F 263168 | FPS 0051 | D 3551 | Reward:μσmM 0.53 1.64 -1.00 2.83 | policy_loss ['None', 'None', '0.075', '-0.060', '0.543'] | value_loss ['None', 'None', '0.117', '0.032', '0.843']
U 660 | F 263680 | FPS 0050 | D 3561 | Reward:μσmM 0.71 1.52 -1.00 2.74 | policy_loss ['None', 'None', '0.043', '0.095', '0.237'] | value_loss ['None', 'None', '0.048', '0.007', '0.413']
U 10 | Test reward:μσmM -0.90 0.30 -1.00 0.00 | Test num frames:μσmM 99.90 84.74 1.00 256.00
Status saved
U 661 | F 264192 | FPS 0048 | D 3619 | Reward:μσmM 2.69 0.32 2.00 2.91 | policy_loss ['None', 'None', '-0.104', '-0.069', '-0.235'] | value_loss ['None', 'None', '0.007', '0.031', '0.071']
U 662 | F 264704 | FPS 0050 | D 3629 | Reward:μσmM 2.37 0.94 0.00 2.87 | policy_loss ['None', 'None', '-0.041', '-0.028', '-0.304'] | value_loss ['None', 'None', '0.003', '0.205', '0.059']
U 663 | F 265216 | FPS 0051 | D 3639 | Reward:μσmM 2.62 0.83 0.00 2.92 | policy_loss ['None', 'None', '-0.078', '-0.188', '-0.387'] | value_loss ['None', 'None', '0.003', '0.016', '0.051']
U 664 | F 265728 | FPS 0046 | D 3650 | Reward:μσmM 2.86 0.23 2.00 2.94 | policy_loss ['None', 'None', '-0.103', '-0.245', '-0.375'] | value_loss ['None', 'None', '0.003', '0.008', '0.034']
U 665 | F 266240 | FPS 0046 | D 3661 | Reward:μσmM 2.88 0.22 2.00 2.95 | policy_loss ['None', 'None', '-0.041', '-0.111', '-0.288'] | value_loss ['None', 'None', '0.001', '0.004', '0.011']
U 666 | F 266752 | FPS 0047 | D 3672 | Reward:μσmM 2.89 0.21 2.00 2.95 | policy_loss ['None', 'None', '-0.013', '-0.050', '-0.155'] | value_loss ['None', 'None', '0.001', '0.002', '0.002']
U 667 | F 267264 | FPS 0045 | D 3684 | Reward:μσmM 2.50 0.90 0.00 2.96 | policy_loss ['None', 'None', '0.127', '0.554', '0.150'] | value_loss ['None', 'None', '0.182', '0.809', '0.125']
U 668 | F 267776 | FPS 0048 | D 3694 | Reward:μσmM 2.88 0.22 2.00 2.95 | policy_loss ['None', 'None', '-0.089', '0.003', '0.027'] | value_loss ['None', 'None', '0.002', '0.003', '0.002']
U 669 | F 268288 | FPS 0047 | D 3705 | Reward:μσmM 2.64 0.80 0.00 2.94 | policy_loss ['None', 'None', '0.037', '0.019', '0.047'] | value_loss ['None', 'None', '0.080', '0.055', '0.015']
U 670 | F 268800 | FPS 0048 | D 3716 | Reward:μσmM 2.68 0.77 0.00 2.94 | policy_loss ['None', 'None', '-0.039', '-0.014', '0.142'] | value_loss ['None', 'None', '0.001', '0.013', '0.016']
U 10 | Test reward:μσmM -0.70 0.46 -1.00 0.00 | Test num frames:μσmM 137.10 106.04 1.00 256.00
Status saved
U 671 | F 269312 | FPS 0049 | D 3792 | Reward:μσmM 2.79 0.27 2.00 2.92 | policy_loss ['None', 'None', '0.068', '-0.021', '-0.050'] | value_loss ['None', 'None', '0.002', '0.001', '0.002']
U 672 | F 269824 | FPS 0050 | D 3803 | Reward:μσmM 2.69 0.54 1.00 2.94 | policy_loss ['None', 'None', '0.029', '0.081', '-0.021'] | value_loss ['None', 'None', '0.047', '0.061', '0.013']
U 673 | F 270336 | FPS 0048 | D 3813 | Reward:μσmM 2.35 0.81 1.00 2.92 | policy_loss ['None', 'None', '0.182', '0.106', '0.121'] | value_loss ['None', 'None', '0.218', '0.059', '0.018']
U 674 | F 270848 | FPS 0050 | D 3823 | Reward:μσmM 2.74 0.29 2.00 2.91 | policy_loss ['None', 'None', '-0.109', '0.050', '0.147'] | value_loss ['None', 'None', '0.006', '0.009', '0.012']
U 675 | F 271360 | FPS 0050 | D 3834 | Reward:μσmM 2.39 1.16 -1.00 2.91 | policy_loss ['None', 'None', '-0.057', '-0.069', '0.100'] | value_loss ['None', 'None', '0.002', '0.006', '0.149']
U 676 | F 271872 | FPS 0048 | D 3844 | Reward:μσmM 2.58 0.82 0.00 2.92 | policy_loss ['None', 'None', '-0.052', '0.053', '-0.073'] | value_loss ['None', 'None', '0.007', '0.212', '0.066']
U 677 | F 272384 | FPS 0047 | D 3855 | Reward:μσmM 2.07 1.47 -1.00 2.94 | policy_loss ['None', 'None', '0.021', '0.028', '-0.049'] | value_loss ['None', 'None', '0.003', '0.181', '0.095']
U 678 | F 272896 | FPS 0044 | D 3867 | Reward:μσmM 2.68 0.95 -1.00 2.95 | policy_loss ['None', 'None', '-0.032', '-0.094', '-0.167'] | value_loss ['None', 'None', '0.001', '0.003', '0.124']
U 679 | F 273408 | FPS 0047 | D 3878 | Reward:μσmM 2.86 0.23 2.00 2.95 | policy_loss ['None', 'None', '-0.005', '0.001', '-0.101'] | value_loss ['None', 'None', '0.002', '0.003', '0.004']
U 680 | F 273920 | FPS 0047 | D 3889 | Reward:μσmM 2.87 0.22 2.00 2.95 | policy_loss ['None', 'None', '-0.007', '-0.022', '-0.040'] | value_loss ['None', 'None', '0.001', '0.001', '0.002']
U 10 | Test reward:μσmM -0.80 0.40 -1.00 0.00 | Test num frames:μσmM 74.40 95.25 5.00 256.00
Status saved
U 681 | F 274432 | FPS 0044 | D 3937 | Reward:μσmM 2.88 0.21 2.00 2.95 | policy_loss ['None', 'None', '-0.019', '-0.023', '-0.022'] | value_loss ['None', 'None', '0.001', '0.001', '0.004']
U 682 | F 274944 | FPS 0047 | D 3947 | Reward:μσmM 2.83 0.43 1.00 2.95 | policy_loss ['None', 'None', '-0.009', '-0.013', '-0.040'] | value_loss ['None', 'None', '0.001', '0.001', '0.001']
U 683 | F 275456 | FPS 0048 | D 3958 | Reward:μσmM 2.64 0.94 -1.00 2.95 | policy_loss ['None', 'None', '-0.003', '-0.008', '0.140'] | value_loss ['None', 'None', '0.000', '0.001', '0.458']
U 684 | F 275968 | FPS 0049 | D 3969 | Reward:μσmM 2.75 0.69 0.00 2.95 | policy_loss ['None', 'None', '0.021', '0.003', '-0.085'] | value_loss ['None', 'None', '0.001', '0.002', '0.006']
U 685 | F 276480 | FPS 0050 | D 3979 | Reward:μσmM 2.47 0.93 0.00 2.95 | policy_loss ['None', 'None', '0.091', '0.102', '0.048'] | value_loss ['None', 'None', '0.077', '0.052', '0.021']
U 686 | F 276992 | FPS 0047 | D 3990 | Reward:μσmM 2.81 0.26 2.00 2.92 | policy_loss ['None', 'None', '-0.054', '0.030', '0.137'] | value_loss ['None', 'None', '0.005', '0.005', '0.005']
U 687 | F 277504 | FPS 0051 | D 4000 | Reward:μσmM 2.12 1.26 -1.00 2.90 | policy_loss ['None', 'None', '0.073', '0.118', '0.317'] | value_loss ['None', 'None', '0.050', '0.079', '0.450']
U 688 | F 278016 | FPS 0051 | D 4010 | Reward:μσmM 0.13 1.25 -1.00 2.88 | policy_loss ['None', 'None', '-0.041', '0.459', '0.926'] | value_loss ['None', 'None', '0.002', '0.488', '1.469']
U 689 | F 278528 | FPS 0050 | D 4020 | Reward:μσmM 1.78 1.38 0.00 2.90 | policy_loss ['None', 'None', '0.019', '0.025', '0.161'] | value_loss ['None', 'None', '0.002', '0.309', '0.082']
U 690 | F 279040 | FPS 0051 | D 4030 | Reward:μσmM 1.03 1.70 -1.00 2.91 | policy_loss ['None', 'None', '0.006', '-0.238', '0.488'] | value_loss ['None', 'None', '0.003', '0.327', '0.924']
U 10 | Test reward:μσmM -0.80 0.40 -1.00 0.00 | Test num frames:μσmM 90.50 89.37 13.00 256.00
Status saved
U 691 | F 279552 | FPS 0050 | D 4086 | Reward:μσmM 0.17 1.71 -1.00 2.90 | policy_loss ['None', 'None', '-0.036', '-0.105', '0.551'] | value_loss ['None', 'None', '0.003', '0.010', '0.977']
U 692 | F 280064 | FPS 0052 | D 4096 | Reward:μσmM 2.25 1.26 -1.00 2.88 | policy_loss ['None', 'None', '0.001', '-0.022', '-0.328'] | value_loss ['None', 'None', '0.002', '0.003', '0.184']
U 693 | F 280576 | FPS 0049 | D 4106 | Reward:μσmM 1.52 1.21 -1.00 2.89 | policy_loss ['None', 'None', '0.290', '0.177', '-0.266'] | value_loss ['None', 'None', '0.283', '0.109', '0.144']
U 694 | F 281088 | FPS 0050 | D 4116 | Reward:μσmM 2.24 1.04 0.00 2.90 | policy_loss ['None', 'None', '0.040', '0.015', '-0.302'] | value_loss ['None', 'None', '0.077', '0.013', '0.037']
U 695 | F 281600 | FPS 0047 | D 4127 | Reward:μσmM 2.33 1.01 0.00 2.91 | policy_loss ['None', 'None', '-0.076', '-0.013', '-0.137'] | value_loss ['None', 'None', '0.038', '0.011', '0.010']
U 696 | F 282112 | FPS 0046 | D 4138 | Reward:μσmM 2.58 0.86 0.00 2.94 | policy_loss ['None', 'None', '-0.135', '-0.061', '-0.059'] | value_loss ['None', 'None', '0.034', '0.024', '0.009']
U 697 | F 282624 | FPS 0049 | D 4149 | Reward:μσmM 2.71 0.54 1.00 2.92 | policy_loss ['None', 'None', '-0.071', '-0.062', '-0.087'] | value_loss ['None', 'None', '0.004', '0.009', '0.011']
U 698 | F 283136 | FPS 0048 | D 4159 | Reward:μσmM 2.85 0.24 2.00 2.95 | policy_loss ['None', 'None', '-0.040', '-0.174', '-0.168'] | value_loss ['None', 'None', '0.002', '0.006', '0.009']
U 699 | F 283648 | FPS 0047 | D 4170 | Reward:μσmM 2.82 0.45 1.00 2.95 | policy_loss ['None', 'None', '-0.036', '-0.072', '-0.148'] | value_loss ['None', 'None', '0.003', '0.003', '0.003']
U 700 | F 284160 | FPS 0047 | D 4181 | Reward:μσmM 2.88 0.21 2.00 2.95 | policy_loss ['None', 'None', '-0.018', '-0.030', '-0.072'] | value_loss ['None', 'None', '0.002', '0.002', '0.002']
U 10 | Test reward:μσmM -0.70 0.46 -1.00 0.00 | Test num frames:μσmM 106.00 121.34 2.00 256.00
Status saved
U 701 | F 284672 | FPS 0047 | D 4242 | Reward:μσmM 2.79 0.64 0.00 2.95 | policy_loss ['None', 'None', '-0.015', '-0.030', '-0.033'] | value_loss ['None', 'None', '0.000', '0.001', '0.001']
U 702 | F 285184 | FPS 0048 | D 4253 | Reward:μσmM 2.83 0.43 1.00 2.95 | policy_loss ['None', 'None', '0.002', '0.003', '-0.017'] | value_loss ['None', 'None', '0.002', '0.001', '0.001']
U 703 | F 285696 | FPS 0050 | D 4263 | Reward:μσmM 2.87 0.22 2.00 2.95 | policy_loss ['None', 'None', '0.017', '-0.007', '0.035'] | value_loss ['None', 'None', '0.001', '0.009', '0.005']
U 704 | F 286208 | FPS 0049 | D 4273 | Reward:μσmM 2.16 1.31 -1.00 2.93 | policy_loss ['None', 'None', '0.078', '0.125', '0.265'] | value_loss ['None', 'None', '0.032', '0.094', '0.409']
U 705 | F 286720 | FPS 0049 | D 4284 | Reward:μσmM 2.56 0.62 1.00 2.90 | policy_loss ['None', 'None', '-0.025', '0.144', '0.138'] | value_loss ['None', 'None', '0.064', '0.055', '0.019']
U 706 | F 287232 | FPS 0050 | D 4294 | Reward:μσmM 1.75 1.35 0.00 2.82 | policy_loss ['None', 'None', '-0.000', '0.475', '0.322'] | value_loss ['None', 'None', '0.002', '0.972', '0.240']
U 707 | F 287744 | FPS 0053 | D 4304 | Reward:μσmM -0.04 1.57 -1.00 2.81 | policy_loss ['None', 'None', '0.014', '-0.025', '0.845'] | value_loss ['None', 'None', '0.001', '0.018', '1.458']
U 708 | F 288256 | FPS 0052 | D 4314 | Reward:μσmM -0.37 1.15 -1.00 2.65 | policy_loss ['None', 'None', '0.401', '0.084', '0.739'] | value_loss ['None', 'None', '0.361', '0.051', '1.444']
U 709 | F 288768 | FPS 0050 | D 4324 | Reward:μσmM 1.67 0.71 1.00 2.69 | policy_loss ['None', 'None', '0.131', '0.003', '-0.178'] | value_loss ['None', 'None', '0.078', '0.002', '0.033']
U 710 | F 289280 | FPS 0051 | D 4334 | Reward:μσmM 0.53 1.52 -1.00 2.70 | policy_loss ['None', 'None', '-0.018', '0.296', '0.445'] | value_loss ['None', 'None', '0.004', '0.328', '0.638']
U 10 | Test reward:μσmM -0.90 0.30 -1.00 0.00 | Test num frames:μσmM 109.60 76.14 23.00 256.00
Status saved
U 711 | F 289792 | FPS 0051 | D 4396 | Reward:μσmM 1.82 1.13 0.00 2.82 | policy_loss ['None', 'None', '-0.014', '0.058', '-0.017'] | value_loss ['None', 'None', '0.035', '0.014', '0.022']
U 712 | F 290304 | FPS 0052 | D 4406 | Reward:μσmM 0.77 1.78 -1.00 2.84 | policy_loss ['None', 'None', '-0.064', '-0.088', '0.370'] | value_loss ['None', 'None', '0.010', '0.003', '0.923']
U 713 | F 290816 | FPS 0052 | D 4416 | Reward:μσmM 2.66 0.30 2.00 2.88 | policy_loss ['None', 'None', '-0.085', '-0.036', '-0.337'] | value_loss ['None', 'None', '0.017', '0.014', '0.032']
U 714 | F 291328 | FPS 0051 | D 4426 | Reward:μσmM 1.99 1.55 -1.00 2.93 | policy_loss ['None', 'None', '-0.148', '-0.214', '0.016'] | value_loss ['None', 'None', '0.011', '0.015', '0.620']
U 715 | F 291840 | FPS 0047 | D 4437 | Reward:μσmM 2.70 0.75 0.00 2.94 | policy_loss ['None', 'None', '-0.129', '-0.274', '-0.450'] | value_loss ['None', 'None', '0.003', '0.011', '0.073']
U 716 | F 292352 | FPS 0047 | D 4448 | Reward:μσmM 2.87 0.22 2.00 2.94 | policy_loss ['None', 'None', '-0.066', '-0.181', '-0.413'] | value_loss ['None', 'None', '0.001', '0.003', '0.066']
U 717 | F 292864 | FPS 0045 | D 4459 | Reward:μσmM 2.88 0.21 2.00 2.95 | policy_loss ['None', 'None', '-0.025', '-0.086', '-0.343'] | value_loss ['None', 'None', '0.001', '0.001', '0.044']
U 718 | F 293376 | FPS 0048 | D 4470 | Reward:μσmM 2.94 0.01 2.91 2.95 | policy_loss ['None', 'None', '-0.001', '-0.019', '-0.166'] | value_loss ['None', 'None', '0.001', '0.001', '0.009']
U 719 | F 293888 | FPS 0050 | D 4480 | Reward:μσmM 2.93 0.01 2.90 2.95 | policy_loss ['None', 'None', '-0.007', '0.010', '-0.021'] | value_loss ['None', 'None', '0.002', '0.001', '0.001']
U 720 | F 294400 | FPS 0049 | D 4490 | Reward:μσmM 2.88 0.22 2.00 2.95 | policy_loss ['None', 'None', '-0.008', '0.015', '0.008'] | value_loss ['None', 'None', '0.001', '0.002', '0.001']
U 10 | Test reward:μσmM -0.50 0.50 -1.00 0.00 | Test num frames:μσmM 133.30 122.79 6.00 256.00
Status saved
U 721 | F 294912 | FPS 0046 | D 4563 | Reward:μσmM 2.87 0.22 2.00 2.94 | policy_loss ['None', 'None', '0.005', '0.005', '0.030'] | value_loss ['None', 'None', '0.001', '0.002', '0.002']
U 722 | F 295424 | FPS 0048 | D 4574 | Reward:μσmM 2.88 0.21 2.00 2.95 | policy_loss ['None', 'None', '-0.007', '-0.032', '-0.009'] | value_loss ['None', 'None', '0.001', '0.001', '0.002']
U 723 | F 295936 | FPS 0047 | D 4585 | Reward:μσmM 2.75 0.50 1.00 2.95 | policy_loss ['None', 'None', '0.093', '0.025', '0.015'] | value_loss ['None', 'None', '0.099', '0.005', '0.005']
U 724 | F 296448 | FPS 0047 | D 4596 | Reward:μσmM 2.90 0.04 2.81 2.94 | policy_loss ['None', 'None', '-0.011', '0.038', '0.128'] | value_loss ['None', 'None', '0.004', '0.003', '0.010']
U 725 | F 296960 | FPS 0048 | D 4606 | Reward:μσmM 2.53 1.05 -1.00 2.95 | policy_loss ['None', 'None', '-0.028', '0.055', '0.139'] | value_loss ['None', 'None', '0.001', '0.009', '0.351']
U 726 | F 297472 | FPS 0046 | D 4617 | Reward:μσmM 1.53 1.67 -1.00 2.94 | policy_loss ['None', 'None', '-0.016', '0.502', '0.403'] | value_loss ['None', 'None', '0.001', '1.350', '1.065']
U 727 | F 297984 | FPS 0050 | D 4628 | Reward:μσmM 2.53 0.99 0.00 2.94 | policy_loss ['None', 'None', '-0.002', '-0.003', '-0.142'] | value_loss ['None', 'None', '0.001', '0.220', '0.109']
U 728 | F 298496 | FPS 0048 | D 4638 | Reward:μσmM 2.68 0.77 0.00 2.94 | policy_loss ['None', 'None', '-0.002', '-0.081', '-0.139'] | value_loss ['None', 'None', '0.004', '0.023', '0.018']
U 729 | F 299008 | FPS 0048 | D 4649 | Reward:μσmM 2.85 0.24 2.00 2.92 | policy_loss ['None', 'None', '-0.014', '-0.015', '-0.058'] | value_loss ['None', 'None', '0.001', '0.002', '0.002']
U 730 | F 299520 | FPS 0047 | D 4660 | Reward:μσmM 2.85 0.24 2.00 2.94 | policy_loss ['None', 'None', '-0.002', '-0.017', '-0.038'] | value_loss ['None', 'None', '0.001', '0.001', '0.003']
U 10 | Test reward:μσmM -0.50 0.50 -1.00 0.00 | Test num frames:μσmM 137.40 119.08 2.00 256.00
Status saved
U 731 | F 300032 | FPS 0047 | D 4735 | Reward:μσmM 2.59 0.84 0.00 2.93 | policy_loss ['None', 'None', '0.012', '0.039', '0.007'] | value_loss ['None', 'None', '0.049', '0.035', '0.014']
U 732 | F 300544 | FPS 0050 | D 4745 | Reward:μσmM 2.74 0.52 1.00 2.93 | policy_loss ['None', 'None', '-0.029', '0.016', '0.035'] | value_loss ['None', 'None', '0.001', '0.009', '0.007']
U 733 | F 301056 | FPS 0050 | D 4755 | Reward:μσmM 2.26 1.36 -1.00 2.94 | policy_loss ['None', 'None', '0.011', '-0.099', '0.099'] | value_loss ['None', 'None', '0.002', '0.003', '0.403']
U 734 | F 301568 | FPS 0051 | D 4765 | Reward:μσmM 2.85 0.24 2.00 2.94 | policy_loss ['None', 'None', '-0.020', '-0.049', '-0.158'] | value_loss ['None', 'None', '0.001', '0.006', '0.012']
U 735 | F 302080 | FPS 0048 | D 4776 | Reward:μσmM 2.87 0.23 2.00 2.95 | policy_loss ['None', 'None', '-0.008', '-0.042', '-0.098'] | value_loss ['None', 'None', '0.002', '0.002', '0.005']
U 736 | F 302592 | FPS 0048 | D 4787 | Reward:μσmM 2.73 0.73 0.00 2.95 | policy_loss ['None', 'None', '-0.002', '-0.003', '-0.011'] | value_loss ['None', 'None', '0.002', '0.001', '0.004']
U 737 | F 303104 | FPS 0050 | D 4797 | Reward:μσmM 2.81 0.26 2.00 2.94 | policy_loss ['None', 'None', '0.010', '0.048', '0.022'] | value_loss ['None', 'None', '0.003', '0.006', '0.004']
U 738 | F 303616 | FPS 0050 | D 4807 | Reward:μσmM 2.45 1.12 -1.00 2.91 | policy_loss ['None', 'None', '0.017', '0.035', '0.203'] | value_loss ['None', 'None', '0.001', '0.007', '0.234']
U 739 | F 304128 | FPS 0049 | D 4818 | Reward:μσmM 1.80 1.30 -1.00 2.86 | policy_loss ['None', 'None', '0.214', '0.133', '0.174'] | value_loss ['None', 'None', '0.165', '0.087', '0.120']
U 740 | F 304640 | FPS 0051 | D 4828 | Reward:μσmM 2.12 1.22 0.00 2.88 | policy_loss ['None', 'None', '0.002', '0.147', '0.101'] | value_loss ['None', 'None', '0.003', '0.287', '0.078']
U 10 | Test reward:μσmM -0.90 0.30 -1.00 0.00 | Test num frames:μσmM 101.80 79.30 1.00 256.00
Status saved
U 741 | F 305152 | FPS 0049 | D 4886 | Reward:μσmM 2.00 1.38 -1.00 2.92 | policy_loss ['None', 'None', '0.034', '-0.038', '0.063'] | value_loss ['None', 'None', '0.097', '0.037', '0.187']
U 742 | F 305664 | FPS 0049 | D 4896 | Reward:μσmM 2.40 0.95 0.00 2.90 | policy_loss ['None', 'None', '-0.102', '0.195', '-0.090'] | value_loss ['None', 'None', '0.002', '0.199', '0.026']
U 743 | F 306176 | FPS 0052 | D 4906 | Reward:μσmM 2.17 1.32 -1.00 2.92 | policy_loss ['None', 'None', '-0.042', '-0.092', '0.071'] | value_loss ['None', 'None', '0.002', '0.113', '0.349']
U 744 | F 306688 | FPS 0053 | D 4916 | Reward:μσmM 2.32 1.28 -1.00 2.93 | policy_loss ['None', 'None', '0.005', '-0.118', '-0.109'] | value_loss ['None', 'None', '0.003', '0.007', '0.202']
U 745 | F 307200 | FPS 0050 | D 4926 | Reward:μσmM 2.64 0.77 0.00 2.94 | policy_loss ['None', 'None', '-0.053', '0.077', '-0.143'] | value_loss ['None', 'None', '0.001', '0.339', '0.118']
U 746 | F 307712 | FPS 0051 | D 4936 | Reward:μσmM 2.47 0.90 0.00 2.93 | policy_loss ['None', 'None', '0.064', '0.135', '-0.005'] | value_loss ['None', 'None', '0.108', '0.280', '0.069']
U 747 | F 308224 | FPS 0051 | D 4946 | Reward:μσmM 2.15 1.43 -1.00 2.93 | policy_loss ['None', 'None', '-0.026', '-0.012', '0.134'] | value_loss ['None', 'None', '0.003', '0.007', '0.520']
U 748 | F 308736 | FPS 0049 | D 4957 | Reward:μσmM 2.46 1.18 -1.00 2.93 | policy_loss ['None', 'None', '-0.021', '-0.070', '-0.040'] | value_loss ['None', 'None', '0.001', '0.001', '0.455']
U 749 | F 309248 | FPS 0049 | D 4967 | Reward:μσmM 2.71 0.54 1.00 2.93 | policy_loss ['None', 'None', '0.028', '-0.001', '-0.102'] | value_loss ['None', 'None', '0.004', '0.002', '0.012']
U 750 | F 309760 | FPS 0052 | D 4977 | Reward:μσmM 2.82 0.25 2.00 2.94 | policy_loss ['None', 'None', '-0.006', '0.040', '-0.063'] | value_loss ['None', 'None', '0.003', '0.002', '0.003']
U 10 | Test reward:μσmM -0.30 0.46 -1.00 0.00 | Test num frames:μσmM 187.20 105.11 23.00 256.00
Status saved
U 751 | F 310272 | FPS 0048 | D 5073 | Reward:μσmM 2.56 0.86 0.00 2.94 | policy_loss ['None', 'None', '-0.024', '0.134', '0.031'] | value_loss ['None', 'None', '0.001', '0.359', '0.065']
U 752 | F 310784 | FPS 0051 | D 5083 | Reward:μσmM 2.74 0.52 1.00 2.94 | policy_loss ['None', 'None', '-0.019', '-0.063', '0.045'] | value_loss ['None', 'None', '0.003', '0.008', '0.013']
U 753 | F 311296 | FPS 0050 | D 5094 | Reward:μσmM 2.37 1.24 -1.00 2.93 | policy_loss ['None', 'None', '-0.006', '-0.052', '0.086'] | value_loss ['None', 'None', '0.002', '0.003', '0.234']
U 754 | F 311808 | FPS 0047 | D 5105 | Reward:μσmM 1.00 1.91 -1.00 2.94 | policy_loss ['None', 'None', '-0.016', '-0.018', '0.648'] | value_loss ['None', 'None', '0.001', '0.002', '1.599']
U 755 | F 312320 | FPS 0048 | D 5115 | Reward:μσmM 1.88 1.66 -1.00 2.94 | policy_loss ['None', 'None', '-0.014', '-0.028', '0.092'] | value_loss ['None', 'None', '0.002', '0.003', '0.881']
U 756 | F 312832 | FPS 0047 | D 5126 | Reward:μσmM 2.93 0.01 2.90 2.95 | policy_loss ['None', 'None', '-0.029', '-0.040', '-0.286'] | value_loss ['None', 'None', '0.001', '0.001', '0.054']
U 757 | F 313344 | FPS 0047 | D 5137 | Reward:μσmM 2.66 0.78 0.00 2.95 | policy_loss ['None', 'None', '0.055', '0.039', '-0.221'] | value_loss ['None', 'None', '0.089', '0.036', '0.042']
U 758 | F 313856 | FPS 0045 | D 5148 | Reward:μσmM 2.81 0.45 1.00 2.95 | policy_loss ['None', 'None', '-0.055', '0.010', '-0.024'] | value_loss ['None', 'None', '0.001', '0.003', '0.005']
U 759 | F 314368 | FPS 0048 | D 5159 | Reward:μσmM 2.46 1.17 -1.00 2.94 | policy_loss ['None', 'None', '0.003', '0.050', '0.105'] | value_loss ['None', 'None', '0.001', '0.008', '0.183']
U 760 | F 314880 | FPS 0048 | D 5170 | Reward:μσmM 2.82 0.25 2.00 2.92 | policy_loss ['None', 'None', '0.038', '-0.041', '-0.023'] | value_loss ['None', 'None', '0.001', '0.001', '0.004']
U 10 | Test reward:μσmM -1.00 0.00 -1.00 -1.00 | Test num frames:μσmM 27.30 48.27 2.00 167.00
Status saved
U 761 | F 315392 | FPS 0048 | D 5194 | Reward:μσmM 1.04 1.25 -1.00 2.88 | policy_loss ['None', 'None', '0.419', '0.464', '0.517'] | value_loss ['None', 'None', '0.486', '0.385', '0.769']
U 762 | F 315904 | FPS 0049 | D 5205 | Reward:μσmM 1.23 1.20 -1.00 2.87 | policy_loss ['None', 'None', '0.199', '0.279', '0.181'] | value_loss ['None', 'None', '0.159', '0.097', '0.074']
U 763 | F 316416 | FPS 0052 | D 5214 | Reward:μσmM 1.56 0.80 1.00 2.78 | policy_loss ['None', 'None', '0.193', '0.132', '0.148'] | value_loss ['None', 'None', '0.146', '0.026', '0.009']
U 764 | F 316928 | FPS 0055 | D 5224 | Reward:μσmM 0.34 0.98 -1.00 2.36 | policy_loss ['None', 'None', '0.077', '0.562', '0.470'] | value_loss ['None', 'None', '0.023', '0.541', '0.612']
U 765 | F 317440 | FPS 0055 | D 5233 | Reward:μσmM 0.74 1.75 -1.00 2.82 | policy_loss ['None', 'None', '-0.045', '0.190', '0.519'] | value_loss ['None', 'None', '0.004', '0.364', '0.750']
U 766 | F 317952 | FPS 0051 | D 5243 | Reward:μσmM 1.71 1.36 -1.00 2.76 | policy_loss ['None', 'None', '-0.027', '-0.296', '0.087'] | value_loss ['None', 'None', '0.056', '0.043', '0.152']
U 767 | F 318464 | FPS 0050 | D 5254 | Reward:μσmM 0.94 1.82 -1.00 2.90 | policy_loss ['None', 'None', '-0.175', '-0.050', '0.331'] | value_loss ['None', 'None', '0.012', '0.319', '0.764']
U 768 | F 318976 | FPS 0053 | D 5263 | Reward:μσmM 1.07 1.93 -1.00 2.91 | policy_loss ['None', 'None', '-0.102', '-0.377', '0.058'] | value_loss ['None', 'None', '0.003', '0.089', '0.708']
U 769 | F 319488 | FPS 0049 | D 5274 | Reward:μσmM 1.60 1.73 -1.00 2.93 | policy_loss ['None', 'None', '-0.058', '-0.211', '-0.237'] | value_loss ['None', 'None', '0.002', '0.162', '0.569']
U 770 | F 320000 | FPS 0049 | D 5284 | Reward:μσmM 2.53 1.05 -1.00 2.93 | policy_loss ['None', 'None', '-0.006', '-0.209', '-0.514'] | value_loss ['None', 'None', '0.002', '0.016', '0.160']
U 10 | Test reward:μσmM 0.00 0.00 0.00 0.00 | Test num frames:μσmM 256.00 0.00 256.00 256.00
Status saved
U 771 | F 320512 | FPS 0047 | D 5413 | Reward:μσmM 2.85 0.24 2.00 2.95 | policy_loss ['None', 'None', '-0.027', '-0.126', '-0.524'] | value_loss ['None', 'None', '0.001', '0.004', '0.221']
U 772 | F 321024 | FPS 0049 | D 5423 | Reward:μσmM 2.82 0.45 1.00 2.95 | policy_loss ['None', 'None', '-0.031', '-0.092', '-0.361'] | value_loss ['None', 'None', '0.001', '0.001', '0.108']
U 773 | F 321536 | FPS 0049 | D 5434 | Reward:μσmM 2.88 0.21 2.00 2.95 | policy_loss ['None', 'None', '-0.012', '-0.027', '-0.184'] | value_loss ['None', 'None', '0.000', '0.001', '0.040']
U 774 | F 322048 | FPS 0050 | D 5444 | Reward:μσmM 2.62 0.96 -1.00 2.95 | policy_loss ['None', 'None', '0.008', '0.039', '-0.058'] | value_loss ['None', 'None', '0.003', '0.003', '0.196']
U 775 | F 322560 | FPS 0050 | D 5454 | Reward:μσmM 2.73 0.52 1.00 2.95 | policy_loss ['None', 'None', '0.069', '0.034', '-0.004'] | value_loss ['None', 'None', '0.082', '0.018', '0.013']
U 776 | F 323072 | FPS 0050 | D 5465 | Reward:μσmM 2.54 0.73 1.00 2.93 | policy_loss ['None', 'None', '0.098', '0.093', '0.164'] | value_loss ['None', 'None', '0.059', '0.026', '0.069']
U 777 | F 323584 | FPS 0053 | D 5474 | Reward:μσmM 2.54 0.90 0.00 2.91 | policy_loss ['None', 'None', '-0.026', '0.045', '0.197'] | value_loss ['None', 'None', '0.008', '0.010', '0.017']
U 778 | F 324096 | FPS 0050 | D 5484 | Reward:μσmM 2.77 0.27 2.00 2.92 | policy_loss ['None', 'None', '-0.037', '-0.010', '0.105'] | value_loss ['None', 'None', '0.002', '0.005', '0.008']
U 779 | F 324608 | FPS 0050 | D 5495 | Reward:μσmM 2.70 0.57 1.00 2.92 | policy_loss ['None', 'None', '-0.033', '0.080', '0.010'] | value_loss ['None', 'None', '0.002', '0.106', '0.011']
U 780 | F 325120 | FPS 0050 | D 5505 | Reward:μσmM 2.21 1.35 -1.00 2.93 | policy_loss ['None', 'None', '-0.027', '-0.037', '0.178'] | value_loss ['None', 'None', '0.001', '0.092', '0.502']
U 10 | Test reward:μσmM -0.30 0.46 -1.00 0.00 | Test num frames:μσmM 195.90 95.29 1.00 256.00
Status saved
U 781 | F 325632 | FPS 0050 | D 5610 | Reward:μσmM 2.84 0.24 2.00 2.95 | policy_loss ['None', 'None', '-0.019', '-0.159', '-0.187'] | value_loss ['None', 'None', '0.002', '0.006', '0.024']
U 782 | F 326144 | FPS 0049 | D 5620 | Reward:μσmM 2.92 0.02 2.88 2.95 | policy_loss ['None', 'None', '-0.023', '-0.069', '-0.203'] | value_loss ['None', 'None', '0.001', '0.004', '0.010']
U 783 | F 326656 | FPS 0051 | D 5630 | Reward:μσmM 2.76 0.69 0.00 2.95 | policy_loss ['None', 'None', '-0.008', '-0.055', '-0.069'] | value_loss ['None', 'None', '0.000', '0.001', '0.012']
U 784 | F 327168 | FPS 0050 | D 5641 | Reward:μσmM 2.33 1.27 -1.00 2.95 | policy_loss ['None', 'None', '0.025', '0.051', '0.263'] | value_loss ['None', 'None', '0.035', '0.082', '0.879']
U 785 | F 327680 | FPS 0052 | D 5650 | Reward:μσmM 2.67 0.56 1.00 2.93 | policy_loss ['None', 'None', '0.089', '0.085', '-0.068'] | value_loss ['None', 'None', '0.090', '0.023', '0.006']
U 786 | F 328192 | FPS 0050 | D 5661 | Reward:μσmM 2.18 1.31 -1.00 2.92 | policy_loss ['None', 'None', '0.018', '0.037', '0.298'] | value_loss ['None', 'None', '0.003', '0.212', '0.509']
U 787 | F 328704 | FPS 0052 | D 5671 | Reward:μσmM 2.29 1.23 -1.00 2.92 | policy_loss ['None', 'None', '0.011', '-0.020', '0.033'] | value_loss ['None', 'None', '0.004', '0.005', '0.343']
U 788 | F 329216 | FPS 0052 | D 5680 | Reward:μσmM 2.51 0.63 1.00 2.93 | policy_loss ['None', 'None', '0.093', '0.068', '-0.094'] | value_loss ['None', 'None', '0.058', '0.006', '0.012']
U 789 | F 329728 | FPS 0051 | D 5690 | Reward:μσmM 2.54 0.84 0.00 2.93 | policy_loss ['None', 'None', '-0.131', '0.066', '0.097'] | value_loss ['None', 'None', '0.003', '0.049', '0.173']
U 790 | F 330240 | FPS 0050 | D 5701 | Reward:μσmM 1.92 1.60 -1.00 2.93 | policy_loss ['None', 'None', '-0.052', '-0.104', '0.114'] | value_loss ['None', 'None', '0.001', '0.010', '0.452']
U 10 | Test reward:μσmM -0.80 0.40 -1.00 0.00 | Test num frames:μσmM 65.40 95.61 4.00 256.00
Status saved
U 791 | F 330752 | FPS 0049 | D 5744 | Reward:μσmM 2.70 0.75 0.00 2.94 | policy_loss ['None', 'None', '-0.015', '-0.153', '-0.138'] | value_loss ['None', 'None', '0.002', '0.003', '0.016']
U 792 | F 331264 | FPS 0049 | D 5754 | Reward:μσmM 2.86 0.23 2.00 2.94 | policy_loss ['None', 'None', '-0.011', '-0.054', '-0.194'] | value_loss ['None', 'None', '0.001', '0.001', '0.010']
U 793 | F 331776 | FPS 0049 | D 5765 | Reward:μσmM 2.78 0.66 0.00 2.95 | policy_loss ['None', 'None', '-0.026', '-0.048', '-0.140'] | value_loss ['None', 'None', '0.001', '0.001', '0.004']
U 794 | F 332288 | FPS 0048 | D 5775 | Reward:μσmM 2.89 0.21 2.00 2.96 | policy_loss ['None', 'None', '-0.020', '-0.022', '-0.042'] | value_loss ['None', 'None', '0.001', '0.001', '0.004']
U 795 | F 332800 | FPS 0050 | D 5785 | Reward:μσmM 2.50 0.99 0.00 2.95 | policy_loss ['None', 'None', '0.089', '0.145', '0.077'] | value_loss ['None', 'None', '0.080', '0.162', '0.062']
U 796 | F 333312 | FPS 0052 | D 5795 | Reward:μσmM 2.42 1.21 -1.00 2.94 | policy_loss ['None', 'None', '0.012', '0.059', '0.063'] | value_loss ['None', 'None', '0.004', '0.006', '0.118']
U 797 | F 333824 | FPS 0051 | D 5805 | Reward:μσmM 2.81 0.26 2.00 2.93 | policy_loss ['None', 'None', '0.038', '0.023', '0.068'] | value_loss ['None', 'None', '0.002', '0.005', '0.003']
U 798 | F 334336 | FPS 0052 | D 5815 | Reward:μσmM 2.02 1.07 0.00 2.90 | policy_loss ['None', 'None', '0.198', '0.227', '0.190'] | value_loss ['None', 'None', '0.177', '0.378', '0.088']
U 799 | F 334848 | FPS 0054 | D 5825 | Reward:μσmM 1.05 1.55 -1.00 2.70 | policy_loss ['None', 'None', '0.125', '0.149', '0.490'] | value_loss ['None', 'None', '0.065', '0.019', '0.808']
U 800 | F 335360 | FPS 0055 | D 5834 | Reward:μσmM 1.47 1.05 0.00 2.87 | policy_loss ['None', 'None', '0.127', '0.119', '0.078'] | value_loss ['None', 'None', '0.052', '0.010', '0.019']
U 10 | Test reward:μσmM -0.90 0.30 -1.00 0.00 | Test num frames:μσmM 82.80 74.04 6.00 256.00
Status saved
U 801 | F 335872 | FPS 0049 | D 5885 | Reward:μσmM 1.21 1.82 -1.00 2.85 | policy_loss ['None', 'None', '-0.139', '0.012', '0.493'] | value_loss ['None', 'None', '0.005', '0.009', '1.106']
U 802 | F 336384 | FPS 0053 | D 5894 | Reward:μσmM 1.70 1.08 0.00 2.77 | policy_loss ['None', 'None', '0.050', '0.255', '0.018'] | value_loss ['None', 'None', '0.066', '0.280', '0.021']
U 803 | F 336896 | FPS 0051 | D 5904 | Reward:μσmM 0.92 1.80 -1.00 2.86 | policy_loss ['None', 'None', '-0.105', '0.024', '0.545'] | value_loss ['None', 'None', '0.010', '0.035', '1.129']
U 804 | F 337408 | FPS 0053 | D 5914 | Reward:μσmM 2.75 0.29 2.00 2.91 | policy_loss ['None', 'None', '-0.118', '-0.183', '-0.284'] | value_loss ['None', 'None', '0.048', '0.032', '0.031']
U 805 | F 337920 | FPS 0051 | D 5924 | Reward:μσmM 1.99 1.51 -1.00 2.93 | policy_loss ['None', 'None', '-0.062', '-0.171', '-0.133'] | value_loss ['None', 'None', '0.007', '0.007', '0.498']
U 806 | F 338432 | FPS 0051 | D 5934 | Reward:μσmM 2.90 0.02 2.87 2.93 | policy_loss ['None', 'None', '-0.083', '-0.143', '-0.334'] | value_loss ['None', 'None', '0.002', '0.002', '0.051']
U 807 | F 338944 | FPS 0048 | D 5945 | Reward:μσmM 2.86 0.23 2.00 2.95 | policy_loss ['None', 'None', '-0.054', '-0.090', '-0.301'] | value_loss ['None', 'None', '0.001', '0.002', '0.046']
U 808 | F 339456 | FPS 0051 | D 5955 | Reward:μσmM 2.87 0.22 2.00 2.95 | policy_loss ['None', 'None', '-0.021', '-0.062', '-0.228'] | value_loss ['None', 'None', '0.000', '0.002', '0.023']
U 809 | F 339968 | FPS 0049 | D 5966 | Reward:μσmM 2.89 0.21 2.00 2.95 | policy_loss ['None', 'None', '-0.007', '-0.026', '-0.192'] | value_loss ['None', 'None', '0.000', '0.001', '0.010']
U 810 | F 340480 | FPS 0049 | D 5976 | Reward:μσmM 2.88 0.21 2.00 2.95 | policy_loss ['None', 'None', '0.006', '0.011', '-0.039'] | value_loss ['None', 'None', '0.001', '0.001', '0.003']
U 10 | Test reward:μσmM -0.50 0.50 -1.00 0.00 | Test num frames:μσmM 161.90 106.62 2.00 256.00
Status saved
U 811 | F 340992 | FPS 0046 | D 6064 | Reward:μσmM 2.89 0.21 2.00 2.95 | policy_loss ['None', 'None', '-0.015', '-0.002', '-0.009'] | value_loss ['None', 'None', '0.001', '0.001', '0.001']
U 812 | F 341504 | FPS 0051 | D 6074 | Reward:μσmM 2.78 0.66 0.00 2.95 | policy_loss ['None', 'None', '0.004', '-0.005', '-0.009'] | value_loss ['None', 'None', '0.000', '0.001', '0.001']
U 813 | F 342016 | FPS 0050 | D 6084 | Reward:μσmM 2.79 0.48 1.00 2.94 | policy_loss ['None', 'None', '0.043', '0.032', '0.048'] | value_loss ['None', 'None', '0.004', '0.002', '0.003']
U 814 | F 342528 | FPS 0050 | D 6095 | Reward:μσmM 2.20 1.40 -1.00 2.92 | policy_loss ['None', 'None', '0.021', '0.047', '0.165'] | value_loss ['None', 'None', '0.002', '0.003', '0.332']
U 815 | F 343040 | FPS 0053 | D 6104 | Reward:μσmM 1.38 1.34 0.00 2.91 | policy_loss ['None', 'None', '0.025', '0.234', '0.475'] | value_loss ['None', 'None', '0.002', '0.207', '0.425']
U 816 | F 343552 | FPS 0051 | D 6114 | Reward:μσmM 1.17 1.38 -1.00 2.84 | policy_loss ['None', 'None', '0.123', '0.180', '0.367'] | value_loss ['None', 'None', '0.089', '0.132', '0.502']
U 817 | F 344064 | FPS 0052 | D 6124 | Reward:μσmM 0.99 1.81 -1.00 2.82 | policy_loss ['None', 'None', '0.007', '-0.078', '0.583'] | value_loss ['None', 'None', '0.002', '0.065', '1.326']
U 818 | F 344576 | FPS 0052 | D 6134 | Reward:μσmM 0.96 1.56 -1.00 2.87 | policy_loss ['None', 'None', '0.251', '-0.087', '0.504'] | value_loss ['None', 'None', '0.306', '0.170', '0.784']
U 819 | F 345088 | FPS 0053 | D 6144 | Reward:μσmM 1.10 1.60 -1.00 2.84 | policy_loss ['None', 'None', '0.036', '0.174', '0.239'] | value_loss ['None', 'None', '0.092', '0.038', '0.299']
U 820 | F 345600 | FPS 0051 | D 6154 | Reward:μσmM 0.67 1.83 -1.00 2.89 | policy_loss ['None', 'None', '-0.055', '-0.197', '0.360'] | value_loss ['None', 'None', '0.007', '0.004', '0.928']
U 10 | Test reward:μσmM -0.40 0.49 -1.00 0.00 | Test num frames:μσmM 180.10 101.25 24.00 256.00
Status saved
U 821 | F 346112 | FPS 0051 | D 6251 | Reward:μσmM 2.54 0.90 0.00 2.91 | policy_loss ['None', 'None', '-0.131', '-0.122', '-0.363'] | value_loss ['None', 'None', '0.005', '0.010', '0.106']
U 822 | F 346624 | FPS 0053 | D 6261 | Reward:μσmM 2.10 1.36 -1.00 2.93 | policy_loss ['None', 'None', '0.061', '-0.145', '-0.364'] | value_loss ['None', 'None', '0.066', '0.007', '0.174']
U 823 | F 347136 | FPS 0051 | D 6271 | Reward:μσmM 2.42 1.21 -1.00 2.94 | policy_loss ['None', 'None', '-0.075', '-0.066', '-0.304'] | value_loss ['None', 'None', '0.003', '0.005', '0.273']
U 824 | F 347648 | FPS 0050 | D 6281 | Reward:μσmM 2.47 1.06 -1.00 2.94 | policy_loss ['None', 'None', '0.059', '-0.053', '-0.310'] | value_loss ['None', 'None', '0.071', '0.014', '0.215']
U 825 | F 348160 | FPS 0049 | D 6292 | Reward:μσmM 2.87 0.22 2.00 2.95 | policy_loss ['None', 'None', '-0.042', '-0.043', '-0.256'] | value_loss ['None', 'None', '0.003', '0.003', '0.029']
U 826 | F 348672 | FPS 0050 | D 6302 | Reward:μσmM 2.76 0.69 0.00 2.95 | policy_loss ['None', 'None', '-0.025', '-0.030', '-0.164'] | value_loss ['None', 'None', '0.004', '0.002', '0.064']
U 827 | F 349184 | FPS 0048 | D 6313 | Reward:μσmM 2.83 0.44 1.00 2.95 | policy_loss ['None', 'None', '-0.002', '-0.016', '-0.089'] | value_loss ['None', 'None', '0.001', '0.001', '0.005']
U 828 | F 349696 | FPS 0049 | D 6323 | Reward:μσmM 2.78 0.66 0.00 2.95 | policy_loss ['None', 'None', '0.003', '-0.008', '-0.013'] | value_loss ['None', 'None', '0.002', '0.002', '0.001']
U 829 | F 350208 | FPS 0049 | D 6334 | Reward:μσmM 2.74 0.50 1.00 2.94 | policy_loss ['None', 'None', '0.044', '0.075', '0.102'] | value_loss ['None', 'None', '0.055', '0.029', '0.056']
U 830 | F 350720 | FPS 0049 | D 6344 | Reward:μσmM 2.83 0.25 2.00 2.92 | policy_loss ['None', 'None', '0.031', '0.012', '0.110'] | value_loss ['None', 'None', '0.001', '0.003', '0.007']
U 10 | Test reward:μσmM -0.60 0.49 -1.00 0.00 | Test num frames:μσmM 159.40 96.74 7.00 256.00
Status saved
U 831 | F 351232 | FPS 0050 | D 6432 | Reward:μσmM 2.74 0.29 2.00 2.91 | policy_loss ['None', 'None', '0.060', '0.035', '0.092'] | value_loss ['None', 'None', '0.003', '0.003', '0.006']
U 832 | F 351744 | FPS 0051 | D 6442 | Reward:μσmM 2.19 0.87 1.00 2.92 | policy_loss ['None', 'None', '0.236', '0.215', '0.102'] | value_loss ['None', 'None', '0.352', '0.110', '0.018']
U 833 | F 352256 | FPS 0051 | D 6452 | Reward:μσmM 2.03 1.04 0.00 2.86 | policy_loss ['None', 'None', '0.026', '0.194', '0.193'] | value_loss ['None', 'None', '0.074', '0.224', '0.086']
U 834 | F 352768 | FPS 0053 | D 6462 | Reward:μσmM 0.29 1.41 -1.00 2.83 | policy_loss ['None', 'None', '-0.192', '0.616', '0.526'] | value_loss ['None', 'None', '0.011', '0.873', '0.658']
U 835 | F 353280 | FPS 0052 | D 6472 | Reward:μσmM 2.02 1.16 0.00 2.72 | policy_loss ['None', 'None', '0.061', '-0.196', '0.199'] | value_loss ['None', 'None', '0.002', '0.016', '0.017']
U 836 | F 353792 | FPS 0053 | D 6481 | Reward:μσmM 0.94 1.78 -1.00 2.90 | policy_loss ['None', 'None', '-0.057', '-0.189', '0.397'] | value_loss ['None', 'None', '0.004', '0.012', '0.450']
U 837 | F 354304 | FPS 0053 | D 6491 | Reward:μσmM 2.04 1.13 0.00 2.87 | policy_loss ['None', 'None', '-0.015', '0.253', '-0.068'] | value_loss ['None', 'None', '0.002', '0.361', '0.061']
U 838 | F 354816 | FPS 0052 | D 6501 | Reward:μσmM 2.45 0.78 1.00 2.90 | policy_loss ['None', 'None', '0.055', '-0.170', '-0.388'] | value_loss ['None', 'None', '0.095', '0.040', '0.074']
U 839 | F 355328 | FPS 0053 | D 6510 | Reward:μσmM 2.68 0.56 1.00 2.92 | policy_loss ['None', 'None', '-0.066', '-0.111', '-0.334'] | value_loss ['None', 'None', '0.004', '0.006', '0.042']
U 840 | F 355840 | FPS 0051 | D 6520 | Reward:μσmM 2.78 0.49 1.00 2.94 | policy_loss ['None', 'None', '-0.082', '-0.132', '-0.331'] | value_loss ['None', 'None', '0.001', '0.003', '0.031']
U 10 | Test reward:μσmM -0.70 0.46 -1.00 0.00 | Test num frames:μσmM 120.90 98.46 13.00 256.00
Status saved
U 841 | F 356352 | FPS 0050 | D 6594 | Reward:μσmM 2.43 0.94 0.00 2.93 | policy_loss ['None', 'None', '0.088', '0.056', '-0.106'] | value_loss ['None', 'None', '0.147', '0.106', '0.030']
U 842 | F 356864 | FPS 0049 | D 6604 | Reward:μσmM 2.50 0.92 0.00 2.94 | policy_loss ['None', 'None', '0.087', '-0.028', '-0.120'] | value_loss ['None', 'None', '0.088', '0.020', '0.011']
U 843 | F 357376 | FPS 0048 | D 6615 | Reward:μσmM 2.55 0.84 0.00 2.93 | policy_loss ['None', 'None', '-0.019', '0.133', '-0.015'] | value_loss ['None', 'None', '0.006', '0.135', '0.029']
U 844 | F 357888 | FPS 0049 | D 6625 | Reward:μσmM 2.68 0.77 0.00 2.94 | policy_loss ['None', 'None', '-0.003', '-0.089', '-0.034'] | value_loss ['None', 'None', '0.003', '0.005', '0.011']
U 845 | F 358400 | FPS 0049 | D 6635 | Reward:μσmM 2.84 0.24 2.00 2.94 | policy_loss ['None', 'None', '-0.009', '-0.021', '-0.026'] | value_loss ['None', 'None', '0.003', '0.004', '0.011']
U 846 | F 358912 | FPS 0049 | D 6646 | Reward:μσmM 2.62 0.82 0.00 2.94 | policy_loss ['None', 'None', '0.003', '0.016', '-0.045'] | value_loss ['None', 'None', '0.056', '0.038', '0.030']
U 847 | F 359424 | FPS 0051 | D 6656 | Reward:μσmM 2.64 0.67 1.00 2.93 | policy_loss ['None', 'None', '-0.004', '0.070', '0.032'] | value_loss ['None', 'None', '0.047', '0.063', '0.022']
U 848 | F 359936 | FPS 0051 | D 6666 | Reward:μσmM 2.68 0.77 0.00 2.94 | policy_loss ['None', 'None', '-0.040', '-0.113', '0.049'] | value_loss ['None', 'None', '0.001', '0.021', '0.011']
U 849 | F 360448 | FPS 0050 | D 6676 | Reward:μσmM 2.71 0.75 0.00 2.95 | policy_loss ['None', 'None', '-0.019', '-0.055', '-0.019'] | value_loss ['None', 'None', '0.001', '0.006', '0.017']
U 850 | F 360960 | FPS 0052 | D 6686 | Reward:μσmM 2.24 1.41 -1.00 2.94 | policy_loss ['None', 'None', '0.015', '-0.046', '0.109'] | value_loss ['None', 'None', '0.002', '0.002', '0.711']
U 10 | Test reward:μσmM -0.60 0.49 -1.00 0.00 | Test num frames:μσmM 158.10 87.66 9.00 256.00
Status saved
U 851 | F 361472 | FPS 0049 | D 6784 | Reward:μσmM 2.84 0.24 2.00 2.94 | policy_loss ['None', 'None', '0.012', '0.017', '-0.154'] | value_loss ['None', 'None', '0.002', '0.002', '0.009']
U 852 | F 361984 | FPS 0051 | D 6794 | Reward:μσmM 2.83 0.24 2.00 2.94 | policy_loss ['None', 'None', '0.001', '0.026', '-0.042'] | value_loss ['None', 'None', '0.008', '0.009', '0.003']
U 853 | F 362496 | FPS 0049 | D 6804 | Reward:μσmM 2.57 0.68 1.00 2.95 | policy_loss ['None', 'None', '0.018', '0.172', '0.083'] | value_loss ['None', 'None', '0.086', '0.143', '0.037']
U 854 | F 363008 | FPS 0050 | D 6814 | Reward:μσmM 2.45 0.78 1.00 2.91 | policy_loss ['None', 'None', '0.099', '0.088', '0.199'] | value_loss ['None', 'None', '0.208', '0.050', '0.010']
U 855 | F 363520 | FPS 0052 | D 6824 | Reward:μσmM 2.48 0.94 0.00 2.92 | policy_loss ['None', 'None', '0.002', '-0.048', '0.162'] | value_loss ['None', 'None', '0.004', '0.015', '0.032']
U 856 | F 364032 | FPS 0051 | D 6834 | Reward:μσmM 1.59 1.57 -1.00 2.92 | policy_loss ['None', 'None', '0.032', '0.042', '0.421'] | value_loss ['None', 'None', '0.109', '0.105', '0.550']
U 857 | F 364544 | FPS 0052 | D 6844 | Reward:μσmM 1.20 1.82 -1.00 2.92 | policy_loss ['None', 'None', '-0.082', '-0.032', '0.374'] | value_loss ['None', 'None', '0.002', '0.175', '0.940']
U 858 | F 365056 | FPS 0053 | D 6853 | Reward:μσmM 1.65 1.64 -1.00 2.87 | policy_loss ['None', 'None', '-0.002', '-0.212', '0.173'] | value_loss ['None', 'None', '0.002', '0.008', '0.338']
U 859 | F 365568 | FPS 0051 | D 6863 | Reward:μσmM 2.48 1.16 -1.00 2.94 | policy_loss ['None', 'None', '0.003', '-0.091', '-0.333'] | value_loss ['None', 'None', '0.003', '0.004', '0.209']
U 860 | F 366080 | FPS 0051 | D 6874 | Reward:μσmM 2.65 0.80 0.00 2.93 | policy_loss ['None', 'None', '-0.030', '-0.045', '-0.333'] | value_loss ['None', 'None', '0.003', '0.004', '0.063']
U 10 | Test reward:μσmM -0.50 0.50 -1.00 0.00 | Test num frames:μσmM 172.50 96.45 10.00 256.00
Status saved
U 861 | F 366592 | FPS 0049 | D 6970 | Reward:μσmM 2.85 0.24 2.00 2.94 | policy_loss ['None', 'None', '-0.051', '-0.043', '-0.299'] | value_loss ['None', 'None', '0.002', '0.003', '0.037']
U 862 | F 367104 | FPS 0050 | D 6980 | Reward:μσmM 2.75 0.50 1.00 2.95 | policy_loss ['None', 'None', '0.036', '0.007', '-0.139'] | value_loss ['None', 'None', '0.102', '0.049', '0.018']
U 863 | F 367616 | FPS 0048 | D 6991 | Reward:μσmM 2.76 0.51 1.00 2.93 | policy_loss ['None', 'None', '-0.028', '0.027', '0.014'] | value_loss ['None', 'None', '0.001', '0.004', '0.002']
U 864 | F 368128 | FPS 0048 | D 7001 | Reward:μσmM 2.43 1.02 0.00 2.94 | policy_loss ['None', 'None', '-0.016', '-0.012', '0.251'] | value_loss ['None', 'None', '0.002', '0.091', '0.344']
U 865 | F 368640 | FPS 0051 | D 7011 | Reward:μσmM 2.58 0.86 0.00 2.93 | policy_loss ['None', 'None', '0.006', '-0.092', '0.066'] | value_loss ['None', 'None', '0.002', '0.003', '0.016']
U 866 | F 369152 | FPS 0049 | D 7022 | Reward:μσmM 2.20 1.24 -1.00 2.93 | policy_loss ['None', 'None', '0.140', '0.043', '0.066'] | value_loss ['None', 'None', '0.156', '0.050', '0.454']
U 867 | F 369664 | FPS 0049 | D 7032 | Reward:μσmM 2.39 0.97 0.00 2.92 | policy_loss ['None', 'None', '0.042', '0.082', '-0.006'] | value_loss ['None', 'None', '0.042', '0.043', '0.012']
U 868 | F 370176 | FPS 0051 | D 7042 | Reward:μσmM 2.47 0.74 1.00 2.91 | policy_loss ['None', 'None', '0.050', '0.030', '-0.010'] | value_loss ['None', 'None', '0.116', '0.056', '0.014']
U 869 | F 370688 | FPS 0052 | D 7052 | Reward:μσmM 2.82 0.25 2.00 2.93 | policy_loss ['None', 'None', '-0.082', '-0.096', '-0.032'] | value_loss ['None', 'None', '0.002', '0.011', '0.006']
U 870 | F 371200 | FPS 0049 | D 7062 | Reward:μσmM 2.62 0.83 0.00 2.94 | policy_loss ['None', 'None', '0.028', '-0.113', '0.029'] | value_loss ['None', 'None', '0.002', '0.015', '0.011']
U 10 | Test reward:μσmM -0.80 0.40 -1.00 0.00 | Test num frames:μσmM 124.10 81.10 6.00 256.00
Status saved
U 871 | F 371712 | FPS 0046 | D 7144 | Reward:μσmM 2.53 0.89 0.00 2.93 | policy_loss ['None', 'None', '-0.034', '0.020', '0.021'] | value_loss ['None', 'None', '0.001', '0.155', '0.082']
U 872 | F 372224 | FPS 0052 | D 7154 | Reward:μσmM 2.77 0.51 1.00 2.94 | policy_loss ['None', 'None', '0.094', '-0.084', '-0.084'] | value_loss ['None', 'None', '0.103', '0.007', '0.005']
U 873 | F 372736 | FPS 0050 | D 7164 | Reward:μσmM 2.64 0.67 1.00 2.94 | policy_loss ['None', 'None', '-0.037', '0.103', '-0.050'] | value_loss ['None', 'None', '0.040', '0.179', '0.014']
U 874 | F 373248 | FPS 0051 | D 7174 | Reward:μσmM 2.51 0.72 1.00 2.92 | policy_loss ['None', 'None', '0.082', '0.126', '0.064'] | value_loss ['None', 'None', '0.159', '0.103', '0.022']
U 875 | F 373760 | FPS 0053 | D 7184 | Reward:μσmM 2.09 1.54 -1.00 2.89 | policy_loss ['None', 'None', '-0.026', '-0.038', '0.331'] | value_loss ['None', 'None', '0.002', '0.003', '0.584']
U 876 | F 374272 | FPS 0053 | D 7193 | Reward:μσmM 2.21 1.28 -1.00 2.93 | policy_loss ['None', 'None', '0.000', '-0.012', '0.178'] | value_loss ['None', 'None', '0.006', '0.011', '0.417']
U 877 | F 374784 | FPS 0050 | D 7203 | Reward:μσmM 2.49 0.75 1.00 2.93 | policy_loss ['None', 'None', '0.131', '0.017', '-0.178'] | value_loss ['None', 'None', '0.148', '0.066', '0.018']
U 878 | F 375296 | FPS 0055 | D 7213 | Reward:μσmM 2.58 0.86 0.00 2.91 | policy_loss ['None', 'None', '-0.053', '0.028', '-0.058'] | value_loss ['None', 'None', '0.005', '0.012', '0.004']
U 879 | F 375808 | FPS 0051 | D 7223 | Reward:μσmM 2.08 1.28 0.00 2.94 | policy_loss ['None', 'None', '-0.040', '0.193', '0.140'] | value_loss ['None', 'None', '0.003', '0.294', '0.158']
U 880 | F 376320 | FPS 0050 | D 7233 | Reward:μσmM 2.69 0.56 1.00 2.91 | policy_loss ['None', 'None', '-0.036', '-0.146', '0.003'] | value_loss ['None', 'None', '0.002', '0.017', '0.019']
U 10 | Test reward:μσmM -0.70 0.46 -1.00 0.00 | Test num frames:μσmM 131.40 103.54 2.00 256.00
Status saved
U 881 | F 376832 | FPS 0047 | D 7322 | Reward:μσmM 2.17 1.25 0.00 2.93 | policy_loss ['None', 'None', '-0.015', '0.135', '-0.005'] | value_loss ['None', 'None', '0.002', '0.297', '0.102']
U 882 | F 377344 | FPS 0054 | D 7331 | Reward:μσmM 2.36 1.19 -1.00 2.95 | policy_loss ['None', 'None', '0.019', '-0.128', '0.108'] | value_loss ['None', 'None', '0.002', '0.014', '0.404']
U 883 | F 377856 | FPS 0052 | D 7341 | Reward:μσmM 1.99 1.45 -1.00 2.91 | policy_loss ['None', 'None', '-0.000', '0.055', '0.057'] | value_loss ['None', 'None', '0.004', '0.234', '0.173']
U 884 | F 378368 | FPS 0054 | D 7351 | Reward:μσmM 2.51 0.72 1.00 2.94 | policy_loss ['None', 'None', '0.038', '0.086', '-0.127'] | value_loss ['None', 'None', '0.123', '0.148', '0.040']
U 885 | F 378880 | FPS 0051 | D 7361 | Reward:μσmM 2.60 0.60 1.00 2.93 | policy_loss ['None', 'None', '-0.021', '0.089', '-0.037'] | value_loss ['None', 'None', '0.071', '0.061', '0.018']
U 886 | F 379392 | FPS 0052 | D 7371 | Reward:μσmM 2.84 0.24 2.00 2.93 | policy_loss ['None', 'None', '-0.107', '-0.114', '-0.132'] | value_loss ['None', 'None', '0.001', '0.009', '0.007']
U 887 | F 379904 | FPS 0050 | D 7381 | Reward:μσmM 2.73 0.52 1.00 2.95 | policy_loss ['None', 'None', '0.017', '-0.109', '-0.166'] | value_loss ['None', 'None', '0.069', '0.036', '0.014']
U 888 | F 380416 | FPS 0050 | D 7391 | Reward:μσmM 2.86 0.23 2.00 2.94 | policy_loss ['None', 'None', '-0.054', '-0.049', '-0.075'] | value_loss ['None', 'None', '0.001', '0.003', '0.003']
U 889 | F 380928 | FPS 0051 | D 7401 | Reward:μσmM 2.93 0.02 2.86 2.95 | policy_loss ['None', 'None', '-0.004', '-0.079', '-0.091'] | value_loss ['None', 'None', '0.002', '0.004', '0.003']
U 890 | F 381440 | FPS 0051 | D 7411 | Reward:μσmM 2.67 0.63 1.00 2.95 | policy_loss ['None', 'None', '0.068', '0.062', '0.013'] | value_loss ['None', 'None', '0.102', '0.093', '0.029']
U 10 | Test reward:μσmM -0.70 0.46 -1.00 0.00 | Test num frames:μσmM 131.50 90.84 4.00 256.00
Status saved
U 891 | F 381952 | FPS 0046 | D 7500 | Reward:μσmM 2.43 1.09 -1.00 2.94 | policy_loss ['None', 'None', '0.092', '-0.022', '0.181'] | value_loss ['None', 'None', '0.090', '0.003', '0.502']
U 892 | F 382464 | FPS 0053 | D 7510 | Reward:μσmM 2.84 0.24 2.00 2.94 | policy_loss ['None', 'None', '-0.050', '0.017', '-0.045'] | value_loss ['None', 'None', '0.004', '0.003', '0.003']
U 893 | F 382976 | FPS 0051 | D 7520 | Reward:μσmM 2.82 0.25 2.00 2.94 | policy_loss ['None', 'None', '-0.016', '-0.002', '0.013'] | value_loss ['None', 'None', '0.003', '0.008', '0.004']
U 894 | F 383488 | FPS 0052 | D 7530 | Reward:μσmM 2.28 1.25 -1.00 2.93 | policy_loss ['None', 'None', '-0.021', '0.062', '0.093'] | value_loss ['None', 'None', '0.001', '0.205', '0.163']
U 895 | F 384000 | FPS 0052 | D 7540 | Reward:μσmM 0.80 1.91 -1.00 2.93 | policy_loss ['None', 'None', '0.014', '-0.036', '0.759'] | value_loss ['None', 'None', '0.002', '0.002', '1.937']
U 896 | F 384512 | FPS 0052 | D 7550 | Reward:μσmM 1.32 1.67 -1.00 2.91 | policy_loss ['None', 'None', '0.063', '0.020', '0.139'] | value_loss ['None', 'None', '0.009', '0.076', '0.543']
U 897 | F 385024 | FPS 0053 | D 7559 | Reward:μσmM 1.92 1.52 -1.00 2.93 | policy_loss ['None', 'None', '-0.017', '0.062', '0.313'] | value_loss ['None', 'None', '0.052', '0.060', '0.844']
U 898 | F 385536 | FPS 0055 | D 7569 | Reward:μσmM 2.74 0.28 2.00 2.89 | policy_loss ['None', 'None', '-0.003', '0.049', '-0.118'] | value_loss ['None', 'None', '0.003', '0.015', '0.009']
U 899 | F 386048 | FPS 0053 | D 7578 | Reward:μσmM 2.46 0.91 0.00 2.90 | policy_loss ['None', 'None', '-0.062', '0.077', '0.093'] | value_loss ['None', 'None', '0.001', '0.124', '0.013']
U 900 | F 386560 | FPS 0055 | D 7588 | Reward:μσmM 1.65 1.48 -1.00 2.90 | policy_loss ['None', 'None', '0.009', '0.225', '0.266'] | value_loss ['None', 'None', '0.001', '0.503', '0.357']
U 10 | Test reward:μσmM -0.80 0.40 -1.00 0.00 | Test num frames:μσmM 145.90 100.50 3.00 256.00
Status saved
U 901 | F 387072 | FPS 0049 | D 7684 | Reward:μσmM 2.29 1.23 -1.00 2.94 | policy_loss ['None', 'None', '0.017', '-0.201', '-0.026'] | value_loss ['None', 'None', '0.003', '0.012', '0.125']
U 902 | F 387584 | FPS 0052 | D 7694 | Reward:μσmM 2.64 0.58 1.00 2.94 | policy_loss ['None', 'None', '0.053', '-0.022', '-0.322'] | value_loss ['None', 'None', '0.089', '0.021', '0.044']
U 903 | F 388096 | FPS 0054 | D 7703 | Reward:μσmM 2.49 1.08 -1.00 2.94 | policy_loss ['None', 'None', '-0.065', '0.032', '-0.129'] | value_loss ['None', 'None', '0.003', '0.006', '0.173']
U 904 | F 388608 | FPS 0053 | D 7713 | Reward:μσmM 2.82 0.25 2.00 2.93 | policy_loss ['None', 'None', '-0.032', '-0.074', '-0.074'] | value_loss ['None', 'None', '0.002', '0.013', '0.010']
U 905 | F 389120 | FPS 0049 | D 7723 | Reward:μσmM 2.79 0.48 1.00 2.95 | policy_loss ['None', 'None', '-0.040', '-0.075', '-0.071'] | value_loss ['None', 'None', '0.001', '0.008', '0.005']
U 906 | F 389632 | FPS 0049 | D 7734 | Reward:μσmM 2.86 0.23 2.00 2.94 | policy_loss ['None', 'None', '-0.020', '-0.040', '-0.014'] | value_loss ['None', 'None', '0.001', '0.004', '0.003']
U 907 | F 390144 | FPS 0050 | D 7744 | Reward:μσmM 2.87 0.22 2.00 2.95 | policy_loss ['None', 'None', '0.004', '-0.025', '-0.086'] | value_loss ['None', 'None', '0.001', '0.002', '0.005']
U 908 | F 390656 | FPS 0052 | D 7754 | Reward:μσmM 2.79 0.48 1.00 2.95 | policy_loss ['None', 'None', '0.022', '-0.008', '-0.058'] | value_loss ['None', 'None', '0.002', '0.001', '0.002']
U 909 | F 391168 | FPS 0053 | D 7763 | Reward:μσmM 2.77 0.49 1.00 2.94 | policy_loss ['None', 'None', '0.012', '0.036', '0.012'] | value_loss ['None', 'None', '0.001', '0.001', '0.001']
U 910 | F 391680 | FPS 0049 | D 7774 | Reward:μσmM 1.80 0.92 1.00 2.94 | policy_loss ['None', 'None', '0.305', '0.393', '0.394'] | value_loss ['None', 'None', '0.470', '0.386', '0.195']
U 10 | Test reward:μσmM -0.70 0.46 -1.00 0.00 | Test num frames:μσmM 114.30 106.56 8.00 256.00
Status saved
U 911 | F 392192 | FPS 0047 | D 7851 | Reward:μσmM 1.56 1.40 -1.00 2.92 | policy_loss ['None', 'None', '0.156', '0.203', '0.400'] | value_loss ['None', 'None', '0.248', '0.147', '0.807']
U 912 | F 392704 | FPS 0053 | D 7861 | Reward:μσmM 1.25 1.43 -1.00 2.92 | policy_loss ['None', 'None', '0.148', '0.196', '0.343'] | value_loss ['None', 'None', '0.291', '0.400', '0.208']
U 913 | F 393216 | FPS 0052 | D 7871 | Reward:μσmM 1.49 1.20 0.00 2.87 | policy_loss ['None', 'None', '0.106', '0.097', '0.239'] | value_loss ['None', 'None', '0.110', '0.150', '0.106']
U 914 | F 393728 | FPS 0052 | D 7881 | Reward:μσmM 0.60 1.84 -1.00 2.93 | policy_loss ['None', 'None', '-0.157', '-0.173', '0.319'] | value_loss ['None', 'None', '0.010', '0.018', '0.646']
U 915 | F 394240 | FPS 0054 | D 7890 | Reward:μσmM 1.56 1.66 -1.00 2.89 | policy_loss ['None', 'None', '-0.019', '-0.086', '-0.057'] | value_loss ['None', 'None', '0.106', '0.045', '0.320']
U 916 | F 394752 | FPS 0053 | D 7900 | Reward:μσmM 2.19 1.36 -1.00 2.93 | policy_loss ['None', 'None', '-0.099', '-0.118', '-0.201'] | value_loss ['None', 'None', '0.002', '0.011', '0.140']
U 917 | F 395264 | FPS 0055 | D 7909 | Reward:μσmM 2.49 0.92 0.00 2.93 | policy_loss ['None', 'None', '-0.061', '-0.073', '-0.282'] | value_loss ['None', 'None', '0.005', '0.095', '0.203']
U 918 | F 395776 | FPS 0050 | D 7919 | Reward:μσmM 2.79 0.48 1.00 2.94 | policy_loss ['None', 'None', '-0.051', '-0.138', '-0.393'] | value_loss ['None', 'None', '0.001', '0.003', '0.059']
U 919 | F 396288 | FPS 0049 | D 7929 | Reward:μσmM 2.88 0.22 2.00 2.95 | policy_loss ['None', 'None', '-0.024', '-0.078', '-0.315'] | value_loss ['None', 'None', '0.000', '0.003', '0.032']
U 920 | F 396800 | FPS 0051 | D 7939 | Reward:μσmM 2.77 0.48 1.00 2.95 | policy_loss ['None', 'None', '0.052', '0.005', '-0.180'] | value_loss ['None', 'None', '0.097', '0.046', '0.032']
U 10 | Test reward:μσmM -0.50 0.50 -1.00 0.00 | Test num frames:μσmM 140.30 116.36 7.00 256.00
Status saved
U 921 | F 397312 | FPS 0046 | D 8031 | Reward:μσmM 2.80 0.47 1.00 2.95 | policy_loss ['None', 'None', '-0.013', '0.017', '-0.029'] | value_loss ['None', 'None', '0.001', '0.002', '0.005']
U 922 | F 397824 | FPS 0050 | D 8041 | Reward:μσmM 2.75 0.69 0.00 2.95 | policy_loss ['None', 'None', '-0.021', '-0.013', '0.005'] | value_loss ['None', 'None', '0.001', '0.002', '0.004']
U 923 | F 398336 | FPS 0051 | D 8051 | Reward:μσmM 2.85 0.24 2.00 2.95 | policy_loss ['None', 'None', '0.020', '0.017', '0.017'] | value_loss ['None', 'None', '0.006', '0.007', '0.005']
U 924 | F 398848 | FPS 0050 | D 8062 | Reward:μσmM 2.84 0.24 2.00 2.94 | policy_loss ['None', 'None', '0.000', '0.010', '0.056'] | value_loss ['None', 'None', '0.001', '0.004', '0.002']
U 925 | F 399360 | FPS 0053 | D 8071 | Reward:μσmM 2.29 1.00 0.00 2.93 | policy_loss ['None', 'None', '0.100', '0.133', '0.115'] | value_loss ['None', 'None', '0.173', '0.151', '0.053']
U 926 | F 399872 | FPS 0054 | D 8081 | Reward:μσmM 2.19 0.81 1.00 2.87 | policy_loss ['None', 'None', '0.082', '0.186', '0.182'] | value_loss ['None', 'None', '0.118', '0.072', '0.018']
U 927 | F 400384 | FPS 0056 | D 8090 | Reward:μσmM 1.08 1.90 -1.00 2.89 | policy_loss ['None', 'None', '-0.005', '-0.002', '0.213'] | value_loss ['None', 'None', '0.007', '0.012', '0.668']
U 928 | F 400896 | FPS 0054 | D 8099 | Reward:μσmM 1.78 1.47 -1.00 2.91 | policy_loss ['None', 'None', '0.064', '-0.075', '-0.031'] | value_loss ['None', 'None', '0.040', '0.011', '0.073']
U 929 | F 401408 | FPS 0051 | D 8109 | Reward:μσmM 2.32 1.28 -1.00 2.92 | policy_loss ['None', 'None', '-0.097', '-0.095', '-0.049'] | value_loss ['None', 'None', '0.001', '0.007', '0.150']
U 930 | F 401920 | FPS 0054 | D 8119 | Reward:μσmM 2.54 0.84 0.00 2.92 | policy_loss ['None', 'None', '-0.008', '0.064', '0.022'] | value_loss ['None', 'None', '0.001', '0.318', '0.106']
U 10 | Test reward:μσmM -0.40 0.49 -1.00 0.00 | Test num frames:μσmM 175.80 112.39 1.00 256.00
Status saved
U 931 | F 402432 | FPS 0046 | D 8229 | Reward:μσmM 2.28 1.36 -1.00 2.93 | policy_loss ['None', 'None', '-0.041', '-0.150', '-0.006'] | value_loss ['None', 'None', '0.001', '0.005', '0.356']
U 932 | F 402944 | FPS 0051 | D 8239 | Reward:μσmM 2.40 0.85 1.00 2.94 | policy_loss ['None', 'None', '0.147', '0.103', '-0.122'] | value_loss ['None', 'None', '0.209', '0.113', '0.046']
U 933 | F 403456 | FPS 0052 | D 8249 | Reward:μσmM 2.87 0.22 2.00 2.95 | policy_loss ['None', 'None', '-0.066', '-0.145', '-0.103'] | value_loss ['None', 'None', '0.003', '0.004', '0.006']
U 934 | F 403968 | FPS 0048 | D 8259 | Reward:μσmM 2.77 0.67 0.00 2.95 | policy_loss ['None', 'None', '-0.014', '-0.080', '-0.105'] | value_loss ['None', 'None', '0.001', '0.002', '0.011']
U 935 | F 404480 | FPS 0049 | D 8270 | Reward:μσmM 2.52 0.97 0.00 2.95 | policy_loss ['None', 'None', '0.043', '0.067', '0.153'] | value_loss ['None', 'None', '0.069', '0.140', '0.496']
U 936 | F 404992 | FPS 0052 | D 8280 | Reward:μσmM 2.79 0.48 1.00 2.95 | policy_loss ['None', 'None', '0.017', '0.052', '-0.030'] | value_loss ['None', 'None', '0.055', '0.051', '0.032']
U 937 | F 405504 | FPS 0053 | D 8289 | Reward:μσmM 2.58 0.86 0.00 2.93 | policy_loss ['None', 'None', '-0.006', '0.084', '0.077'] | value_loss ['None', 'None', '0.007', '0.011', '0.006']
U 938 | F 406016 | FPS 0053 | D 8299 | Reward:μσmM 2.69 0.56 1.00 2.92 | policy_loss ['None', 'None', '0.001', '-0.009', '0.062'] | value_loss ['None', 'None', '0.002', '0.006', '0.007']
U 939 | F 406528 | FPS 0053 | D 8309 | Reward:μσmM 2.18 1.16 0.00 2.93 | policy_loss ['None', 'None', '0.073', '0.065', '0.179'] | value_loss ['None', 'None', '0.096', '0.200', '0.156']
U 940 | F 407040 | FPS 0054 | D 8318 | Reward:μσmM 0.83 1.75 -1.00 2.90 | policy_loss ['None', 'None', '0.091', '0.103', '0.471'] | value_loss ['None', 'None', '0.078', '0.037', '0.885']
U 10 | Test reward:μσmM -0.70 0.46 -1.00 0.00 | Test num frames:μσmM 104.10 102.27 1.00 256.00
Status saved
U 941 | F 407552 | FPS 0049 | D 8390 | Reward:μσmM 1.87 1.32 -1.00 2.84 | policy_loss ['None', 'None', '0.055', '0.115', '0.130'] | value_loss ['None', 'None', '0.084', '0.006', '0.276']
U 942 | F 408064 | FPS 0054 | D 8399 | Reward:μσmM 0.95 1.59 -1.00 2.87 | policy_loss ['None', 'None', '0.099', '0.161', '0.239'] | value_loss ['None', 'None', '0.096', '0.329', '0.498']
U 943 | F 408576 | FPS 0054 | D 8409 | Reward:μσmM 1.80 1.38 -1.00 2.91 | policy_loss ['None', 'None', '-0.056', '-0.007', '0.231'] | value_loss ['None', 'None', '0.045', '0.148', '0.397']
U 944 | F 409088 | FPS 0054 | D 8418 | Reward:μσmM 2.81 0.26 2.00 2.91 | policy_loss ['None', 'None', '-0.101', '-0.378', '-0.337'] | value_loss ['None', 'None', '0.003', '0.035', '0.028']
U 945 | F 409600 | FPS 0052 | D 8428 | Reward:μσmM 2.84 0.24 2.00 2.93 | policy_loss ['None', 'None', '-0.084', '-0.203', '-0.323'] | value_loss ['None', 'None', '0.003', '0.007', '0.028']
U 946 | F 410112 | FPS 0051 | D 8438 | Reward:μσmM 2.79 0.48 1.00 2.95 | policy_loss ['None', 'None', '-0.053', '-0.115', '-0.255'] | value_loss ['None', 'None', '0.001', '0.003', '0.018']
U 947 | F 410624 | FPS 0050 | D 8448 | Reward:μσmM 2.77 0.67 0.00 2.95 | policy_loss ['None', 'None', '-0.019', '-0.057', '-0.213'] | value_loss ['None', 'None', '0.001', '0.001', '0.006']
U 948 | F 411136 | FPS 0051 | D 8458 | Reward:μσmM 2.77 0.67 0.00 2.95 | policy_loss ['None', 'None', '0.010', '-0.014', '-0.106'] | value_loss ['None', 'None', '0.000', '0.001', '0.002']
U 949 | F 411648 | FPS 0050 | D 8469 | Reward:μσmM 2.80 0.47 1.00 2.95 | policy_loss ['None', 'None', '0.033', '0.023', '-0.003'] | value_loss ['None', 'None', '0.003', '0.002', '0.003']
U 950 | F 412160 | FPS 0054 | D 8478 | Reward:μσmM 2.66 0.65 1.00 2.95 | policy_loss ['None', 'None', '0.122', '0.088', '0.059'] | value_loss ['None', 'None', '0.109', '0.085', '0.038']
U 10 | Test reward:μσmM -0.20 0.40 -1.00 0.00 | Test num frames:μσmM 212.60 87.97 7.00 256.00
Status saved
U 951 | F 412672 | FPS 0050 | D 8610 | Reward:μσmM 2.47 0.74 1.00 2.91 | policy_loss ['None', 'None', '0.067', '0.131', '0.165'] | value_loss ['None', 'None', '0.149', '0.055', '0.020']
U 952 | F 413184 | FPS 0057 | D 8619 | Reward:μσmM 2.37 0.79 1.00 2.89 | policy_loss ['None', 'None', '0.036', '0.076', '0.162'] | value_loss ['None', 'None', '0.075', '0.025', '0.007']
U 953 | F 413696 | FPS 0055 | D 8628 | Reward:μσmM 1.71 1.53 -1.00 2.89 | policy_loss ['None', 'None', '0.018', '0.001', '0.150'] | value_loss ['None', 'None', '0.058', '0.007', '0.197']
U 954 | F 414208 | FPS 0055 | D 8638 | Reward:μσmM 1.45 1.66 -1.00 2.90 | policy_loss ['None', 'None', '-0.024', '-0.123', '0.361'] | value_loss ['None', 'None', '0.006', '0.005', '0.444']
U 955 | F 414720 | FPS 0053 | D 8647 | Reward:μσmM 1.47 1.66 -1.00 2.91 | policy_loss ['None', 'None', '0.024', '-0.010', '0.224'] | value_loss ['None', 'None', '0.126', '0.041', '0.540']
U 956 | F 415232 | FPS 0055 | D 8657 | Reward:μσmM 1.61 1.60 -1.00 2.90 | policy_loss ['None', 'None', '0.037', '0.030', '0.100'] | value_loss ['None', 'None', '0.060', '0.035', '0.491']
U 957 | F 415744 | FPS 0053 | D 8666 | Reward:μσmM 2.72 0.29 2.00 2.88 | policy_loss ['None', 'None', '-0.067', '-0.021', '-0.079'] | value_loss ['None', 'None', '0.019', '0.012', '0.039']
U 958 | F 416256 | FPS 0053 | D 8676 | Reward:μσmM 2.79 0.26 2.00 2.92 | policy_loss ['None', 'None', '-0.058', '-0.111', '-0.167'] | value_loss ['None', 'None', '0.003', '0.005', '0.022']
U 959 | F 416768 | FPS 0049 | D 8686 | Reward:μσmM 2.32 1.28 -1.00 2.90 | policy_loss ['None', 'None', '-0.042', '-0.034', '-0.141'] | value_loss ['None', 'None', '0.002', '0.004', '0.068']
U 960 | F 417280 | FPS 0051 | D 8696 | Reward:μσmM 2.59 0.99 -1.00 2.95 | policy_loss ['None', 'None', '-0.035', '-0.121', '-0.178'] | value_loss ['None', 'None', '0.002', '0.004', '0.051']
U 10 | Test reward:μσmM -0.30 0.46 -1.00 0.00 | Test num frames:μσmM 205.30 99.19 6.00 256.00
Status saved
U 961 | F 417792 | FPS 0046 | D 8827 | Reward:μσmM 2.87 0.22 2.00 2.95 | policy_loss ['None', 'None', '-0.036', '-0.069', '-0.112'] | value_loss ['None', 'None', '0.001', '0.002', '0.010']
U 962 | F 418304 | FPS 0052 | D 8837 | Reward:μσmM 2.88 0.21 2.00 2.95 | policy_loss ['None', 'None', '-0.014', '-0.051', '-0.102'] | value_loss ['None', 'None', '0.001', '0.001', '0.003']
U 963 | F 418816 | FPS 0052 | D 8847 | Reward:μσmM 2.78 0.66 0.00 2.96 | policy_loss ['None', 'None', '-0.004', '-0.017', '-0.035'] | value_loss ['None', 'None', '0.001', '0.001', '0.002']
U 964 | F 419328 | FPS 0053 | D 8857 | Reward:μσmM 2.88 0.21 2.00 2.95 | policy_loss ['None', 'None', '0.003', '0.011', '-0.004'] | value_loss ['None', 'None', '0.000', '0.001', '0.001']
U 965 | F 419840 | FPS 0051 | D 8867 | Reward:μσmM 2.86 0.23 2.00 2.94 | policy_loss ['None', 'None', '0.017', '0.026', '0.046'] | value_loss ['None', 'None', '0.001', '0.003', '0.002']
U 966 | F 420352 | FPS 0052 | D 8877 | Reward:μσmM 2.24 1.02 0.00 2.92 | policy_loss ['None', 'None', '0.111', '0.317', '0.181'] | value_loss ['None', 'None', '0.139', '0.192', '0.049']
U 967 | F 420864 | FPS 0055 | D 8886 | Reward:μσmM 0.81 1.62 -1.00 2.91 | policy_loss ['None', 'None', '-0.023', '0.537', '0.657'] | value_loss ['None', 'None', '0.003', '0.602', '1.288']
U 968 | F 421376 | FPS 0051 | D 8896 | Reward:μσmM 1.10 1.55 -1.00 2.90 | policy_loss ['None', 'None', '-0.001', '0.313', '0.399'] | value_loss ['None', 'None', '0.001', '0.565', '0.416']
U 969 | F 421888 | FPS 0053 | D 8906 | Reward:μσmM 0.93 1.83 -1.00 2.94 | policy_loss ['None', 'None', '0.014', '-0.164', '0.642'] | value_loss ['None', 'None', '0.003', '0.102', '1.366']
U 970 | F 422400 | FPS 0000 | D 32433 | Reward:μσmM 1.98 1.46 -1.00 2.89 | policy_loss ['None', 'None', '0.043', '-0.246', '0.053'] | value_loss ['None', 'None', '0.004', '0.021', '0.264']
